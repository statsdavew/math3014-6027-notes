% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{MATH3014-6027 Design (and Analysis) of Experiments}
\author{Dave Woods}
\date{2022-02-28}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={MATH3014-6027 Design (and Analysis) of Experiments},
  pdfauthor={Dave Woods},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

These are draft lecture notes for the modules MATH3014 and MATH6027 Design (and Analysis) of Experiments at the University of Southampton for academic year 2021-22. They are very much work in progress.

Southampton prerequisites for this module are MATH2010 or MATH6174 and STAT6123 (or equivalent modules on linear modelling).

\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bvarepsilon}{\boldsymbol{\varepsilon}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\rT}{\mathrm{T}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\bY}{\boldsymbol{y}}
\newcommand{\btau}{\boldsymbol{\tau}}

\hypertarget{intro}{%
\chapter{Motivation, introduction and revision}\label{intro}}

\begin{definition}
\protect\hypertarget{def:exp}{}\label{def:exp}

An \textbf{experiment} is the process through which data are collected to answer a scientific question (physical science, social science, actuarial science \(\dots\)) by \textbf{deliberately} varying some features of the process under study in order to understand the impact of these changes on measureable responses.

In this course we consider only \emph{intervention} experiments, in which some aspects of the process are under the experimenters' control. We do not consider \emph{surveys} or \emph{observational} studies.

\end{definition}

\begin{definition}
\protect\hypertarget{def:design}{}\label{def:design}

\textbf{Design of experiments} is the topic in Statistics concerned with the selection of settings of controllable variables or factors in an experiment and their allocation to experimental units in order to maximise the effectiveness of the experiment at achieving its aim.

\end{definition}

People have been designing experiments for as long as they have been exploring the natural world. Collecting empirical evidence is key for scientific development, as described in terms of clinical trials by \href{https://xkcd.com/2530/}{xkcd}:

Some notable milestones in the history of the design of experiments include:

\begin{itemize}
\tightlist
\item
  prior to the 20th century:

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/Baconian_method}{Francis Bacon} (17th century; pioneer of the experimental methods)
  \item
    \href{https://en.wikipedia.org/wiki/James_Lind}{James Lind} (18th century; experiments to eliminate scurvy)
  \item
    \href{https://en.wikipedia.org/wiki/Charles_Sanders_Peirce\#Probability_and_statistics}{Charles Peirce} (19th century; advocated randomised experiments and randomisation-based inference)
  \end{itemize}
\item
  1920s: agriculture (particularly at the \href{https://www.rothamsted.ac.uk/history-and-heritage}{Rothamsted Agricultural Research Station})
\item
  1940s: clinical trials (\href{https://en.wikipedia.org/wiki/Austin_Bradford_Hill}{Austin Bradford-Hill})
\item
  1950s: (manufacturing) industry (\href{https://en.wikipedia.org/wiki/W._Edwards_Deming}{W. Edwards Deming}; \href{https://en.wikipedia.org/wiki/Genichi_Taguchi}{Genichi Taguchi})
\item
  1960s: psychology and economics (\href{https://en.wikipedia.org/wiki/Vernon_L._Smith}{Vernon Smith})
\item
  1980s: in-silico (\href{https://en.wikipedia.org/wiki/Computer_experiment}{computer experiments})
\item
  2000s: online (\href{https://en.wikipedia.org/wiki/A/B_testing}{A/B testing})
\end{itemize}

See \citet{LB2020} for further history, anecdotes and examples, especially from psychology and technology.

Figure \ref{fig:broadbalk} shows the \href{http://www.era.rothamsted.ac.uk/Broadbalk}{Broadbalk} agricultural field experiment at Rothamsted, one of the longest continuous running experiments in the world, which is testing the impact of different manures and fertilizers on the growth of winter wheat.

\begin{figure}
 
 {\centering \includegraphics[width=0.75\linewidth]{figures/broadbalk} 
 
 }
 
 \caption{The Broadbalk experiment, Rothamsted (photograph taken 2016)}\label{fig:broadbalk}
 \end{figure}

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

\begin{example}
\protect\hypertarget{exm:motivation}{}\label{exm:motivation}

Consider an experiment to compare two treatments (e.g.~drugs, diets, fertilisers, \(\dots\)). We have \(n\) subjects (people, mice, plots of land, \(\dots\)), each of which can be assigned one of the two treatments. A response (protein measurement, weight, yield, \(\dots\)) is then measured.

\end{example}

\textbf{Question:} How many subjects should be assigned to each treatment to gain the most precise\footnote{Smallest variance.} inference about the difference in response from the two treatments?

Consider a linear statistical model\footnote{In this course, we will almost always start with a statistical model which we wish to use to answer our scientific question.} for the response (see MATH2010 or MATH6174/STAT6123):

\begin{equation}
Y_j=\beta_{0}+\beta_{1}x_j+\varepsilon_j\,,\qquad j=1, \ldots, n\,,
\label{eq:slr}
\end{equation}

where \(\varepsilon_j\sim N(0,\sigma^{2})\) are independent and identically distributed errors and \(\beta_{0}, \beta_{1}\) are unknown constants (parameters).

Let\footnote{Other codings can be used: e.g. 0,1; see later in the module. It makes no difference for our current purpose.}
\begin{equation}
x_{j}=\left\{\begin{array}{ll}
-1&\textrm{if treatment 1 is applied to the $j$th subject}\\
+1&\textrm{if treatment 2 is applied to the $j$th subject}\nonumber ,
\end{array}
\right.
\end{equation}
for \(j=1,\dots,n\).\footnote{We will discuss the choice of \emph{coding} -1, +1 later.}

The difference in expected response from treatments 1 and 2 is

\begin{equation}
\begin{split}
\textrm{E}[Y_j\, |\, x_j = +1] - \textrm{E}[Y_j\, |\, x_j = -1] & = \beta_{0}+\beta_{1}-\beta_{0}+\beta_{1} \\
& = 2\beta_{1}\,.
\end{split}
\label{eq:ex-ex-response}
\end{equation}

Therefore, we require the the most precise estimator of \(\beta_{1}\) possible. That is, we wish to make the variance of our estimator of \(\beta_1\) as small as possible.

Parameters \(\beta_{0}\) and \(\beta_{1}\) can be estimated using least squares (see MATH2010 or MATH6174/STAT6123). For \(Y_1,\dots,Y_n\), we can write the model down in matrix form:

\begin{equation*}
\left[ \begin{array}{c}
Y_1\\
\vdots\\
Y_n\end{array}\right]
=\left[ \begin{array}{cc}
1&x_{1}\\
\vdots&\vdots\\
1&x_{n}\end{array}\right]
\left[ \begin{array}{c}
\beta_{0}\\
\beta_{1}\end{array}\right]
+\left[ \begin{array}{c}
\varepsilon_{1}\\
\vdots\\
\varepsilon_{n}\end{array}\right]\,.
\end{equation*}

Or, by defining some notation:

\begin{equation}
\boldsymbol{Y}=X\boldsymbol{\beta}+\boldsymbol{\varepsilon}\,
\label{eq:matrix-model}
\end{equation}

where

\begin{itemize}
\tightlist
\item
  \(\boldsymbol{Y}\) - \(n\times 1\) vector of responses;
\item
  \(X\) - \(n\times p\) model matrix;
\item
  \(\boldsymbol{\beta}\) - \(p\times 1\) vector of parameters;
\item
  \(\boldsymbol{\varepsilon}\) - \(n\times 1\) vector of errors.
\end{itemize}

The \textbf{least squares estimators}, \(\hat{\boldsymbol{\beta}}\), are chosen such that the quadratic form

\begin{equation*}
(\boldsymbol{Y}-X\boldsymbol{\beta})^{\textrm{T}}(\boldsymbol{Y}-X\boldsymbol{\beta})
\end{equation*}

is minimised (recall that \(\textrm{E}(\textbf{Y})=X\boldsymbol{\beta}\)). Therefore

\begin{equation*}
\hat{\boldsymbol{\beta}} = \textrm{argmin}_{\boldsymbol{\beta}}(\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y}+\boldsymbol{\beta}^{\textrm{T}}X^{\textrm{T}}X\boldsymbol{\beta}
-2\boldsymbol{\beta}^{\textrm{T}}X^{\textrm{T}}\boldsymbol{Y})\,.
\end{equation*}

If we differentiate with respect to \(\boldsymbol{\beta}\)\footnote{Check the \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{Matrix Cookbook} for matrix calculus, amongst much else.},

\begin{equation*}
\frac{\partial}{\partial\boldsymbol{\beta}}=2X^{\textrm{T}}X\boldsymbol{\beta}-2X^{\textrm{T}}\boldsymbol{Y}\,,\nonumber
\end{equation*}

and equate to 0, we get the estimators

\begin{equation}
\hat{\boldsymbol{\beta}}=(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y}\,.
\label{eq:lsestimators}
\end{equation}

These are the least squares estimators.

For Example \ref{exm:motivation},

\[
X=\left[\begin{array}{cc}
1&x_{1}\\
\vdots&\vdots\\
1&x_{n}\end{array}\right]\,,
\qquad
X^{\textrm{T}}X=\left[\begin{array}{cc}
n&\sum x_j\\
\sum x_j&\sum x_j^{2}\end{array}\right]\,,
\]

\[
(X^{\textrm{T}}X)^{-1}=\frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}\left[\begin{array}{cc}
\sum x_j^{2}&-\sum x_j\\
-\sum x_j&n\end{array}\right]\,,
\qquad
X^{\textrm{T}}\boldsymbol{Y}=\left[\begin{array}{c}
\sum Y_j\\
\sum x_jY_j\end{array}\right]\,.
\]
Then,
\begin{align}
\hat{\boldsymbol{\beta}}=\left[\begin{array}{c}
\hat{\beta}_{0}\\
\hat{\beta}_{1}\end{array}\right]
& =\frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}
\left[\begin{array}{cc}
\sum x_j^{2}&-\sum x_j\\
-\sum x_j&n\end{array}\right]
\left[\begin{array}{c}
\sum Y_j\\
\sum x_jY_j\end{array}\right]\nonumber \\
&= \frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}\left[\begin{array}{c}
\sum Y_j\sum x_j^{2}-\sum x_j\sum x_jY_j\\
n\sum x_jY_j-\sum x_j\sum Y_j\end{array}\right]\,.
\end{align}

We don't usually work through the algebra in such detail; the matrix form is often sufficient for theoretical and numerical calculations and software, e.g.~\texttt{R}, can be used.

The precision of \(\hat{\boldsymbol{\beta}}\) is measured via the variance-covariance matrix, given by
\begin{align}
\textrm{Var}(\hat{\boldsymbol{\beta}}) & = \textrm{Var}\{(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y}\}\\
& =(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\textrm{Var}(\boldsymbol{Y})X(X^{\textrm{T}}X)^{-1}\\
& = (X^{\textrm{T}}X)^{-1}\sigma^{2}\,,
\end{align}

where \(\boldsymbol{Y}\sim N(X\boldsymbol{\beta},I_n\sigma^{2})\), where \(I_n\) is an \(n\times n\) identity matrix.

Hence, in our example,
\begin{align*}
\textrm{Var}(\hat{\boldsymbol{\beta}}) & = \frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}\left[\begin{array}{cc}
\sum x_j^{2}&-\sum x_j\\
-\sum x_j&n\end{array}\right]\sigma^{2}\\
& = \left[\begin{array}{cc}
\textrm{Var}(\hat\beta_{0})&\textrm{Cov}(\hat\beta_{0},\hat\beta_{1})\\
\textrm{Cov}(\hat\beta_{0},\hat\beta_{1})&\textrm{Var}(\hat\beta_{1})\end{array}\right]\,.
\end{align*}

For estimating the difference between treatments, we are interested in

\begin{align*}
\textrm{Var}(\hat{\beta}_{1})& = \frac{n}{n\sum x_j^{2}-(\sum x_j)^{2}}\sigma^{2}\\
 & = \frac{n}{n^2 - (\sum x_j)^2}\sigma^{2}\,,
\end{align*}
as \(x_j=\pm 1\), therefore \(x_j^2=1\) for all \(j=1,\ldots,n\), and hence \(\sum x_j^2=n\).

To achieve the most precise estimator, we need to minimise \(\textrm{Var}(\hat{\beta}_{1})\) or equivalently minimise \(|\sum x_j|\). This goal can be achieved through the choice of \(x_{1},\dots,x_{n}\):

\begin{itemize}
\tightlist
\item
  as each \(x_j\) can only take one of two values, -1 or +1, this is equivalent to choosing the numbers of subjects assigned to treatment 1 and treatment 2;
\item
  call these \(n_{1}\) and \(n_{2}\) respectively, with \(n_{1}+n_{2}=n\)
\end{itemize}

It is obvious that \(\sum x_j = 0\) if and only if \(n_1=n_2\). Therefore, assuming \(n\) is even, the \textbf{optimal design} has

\begin{itemize}
\tightlist
\item
  \(n_{1}=\frac{n}{2}\) subjects assigned to treatment 1 and
\item
  \(n_{2}=\frac{n}{2}\) subjects assigned to treatment 2.
\end{itemize}

For \(n\) odd, we choose \(n_{1}=\frac{n+1}{2}\), \(n_{2}=\frac{n-1}{2}\), or vice versa.

\begin{definition}
\protect\hypertarget{def:simple-efficiency}{}\label{def:simple-efficiency}

We can assess different designs using their \textbf{efficiency}:
\begin{equation}
\textrm{Eff}=\frac{\textrm{Var}(\hat{\beta}_{1}\, |\, d^{*})}{\textrm{Var}(\hat{\beta}_{1}\, |\, d_{1})}
\label{eq:simple-efficiency}
\end{equation}

where \(d_{1}\) is a design we want to assess and \(d^{*}\) is the optimal design with smallest variance. Note that \(0\leq\textrm{Eff}\leq 1\).

\end{definition}

In Figure \ref{fig:simple-efficiency} below, we plot this efficiency for Example \ref{exm:motivation}, using different choices of \(n_1\). The total number of runs is fixed at \(n = 100\), and the function \texttt{eff} calculates the efficiency from Definition \ref{def:simple-efficiency} for a design with \(n_1\) subjects assigned to treatment 1. Clearly, efficiency of 1 is achieved when \(n_1 = n_2\) (equal allocation of treatments 1 and 2). If \(n_1=0\) or \(n_1 = 1\), the efficiency is zero; we cannot estimate the difference between two treatments if we only allocate subjects to one of them.



\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100} 
\NormalTok{eff }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n1) }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ ((}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ n1 }\SpecialCharTok{{-}}\NormalTok{ n) }\SpecialCharTok{/}\NormalTok{ n)}\SpecialCharTok{\^{}}\DecValTok{2} 
\FunctionTok{curve}\NormalTok{(eff, }\AttributeTok{from =} \DecValTok{0}\NormalTok{, }\AttributeTok{to =}\NormalTok{ n, }\AttributeTok{ylab =} \StringTok{"Eff"}\NormalTok{, }\AttributeTok{xlab =} \FunctionTok{expression}\NormalTok{(n[}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_math3014-6027_files/figure-latex/simple-efficiency-1.pdf}
\caption{\label{fig:simple-efficiency}Efficiencies for designs for Example \ref{exm:motivation} with different numbers, \(n_1\), of subjects assigned to treatment 1 when the total number of subjects is \(n=100\).}
\end{figure}

\hypertarget{aims-of-experimentation-and-some-examples}{%
\section{Aims of experimentation and some examples}\label{aims-of-experimentation-and-some-examples}}

Some reasons experiments are performed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Treatment comparison (Chapters 2 and 3)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  compare several treatments (and choose the best)
\item
  e.g.~clinical trial, agricultural field trial
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Factor screening (Chapters 4, 5 and 6)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  many complex systems may involve a large number of (discrete) factors (controllable features)
\item
  which of these factors have a substantive impact?
\item
  (relatively) small experiments
\item
  e.g.~industrial experiments on manufacturing processes
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Response surface exploration (Chapter 7)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  detailed description of relationship between important (continuous) variables and response
\item
  typically second order polynomial regression models
\item
  larger experiments, often built up sequentially
\item
  e.g.~alcohol yields in a pharmaceutical experiments
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Optimisation (Chapter 7)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  finding settings of variables that lead to maximum or minimum response
\item
  typically use response surface methods and sequential ``hill climbing'\,' strategy
\end{itemize}

\hypertarget{some-definitions}{%
\section{Some definitions}\label{some-definitions}}

\begin{definition}
\protect\hypertarget{def:response}{}\label{def:response}

The \textbf{response} \(Y\) is the outcome measured in an experiment; e.g.~yield from a chemical process. The response from the \(n\) observations are denoted \(Y_{1},\dots,Y_{n}\).

\end{definition}

\begin{definition}
\protect\hypertarget{def:factor-variable}{}\label{def:factor-variable}

\textbf{Factors} (discrete) or \textbf{variables} (continuous) are features which can be set or controlled in an experiment; \(m\) denotes the number of factors or variables under investigation. For discrete factors, we call the possible settings of the factor its \textbf{levels}. We denote by \(x_{ij}\) the value taken by factor or variable \(i\) in the \(j\)th run of the experiment (\(i = 1, \ldots, m\); \(j = 1, \ldots, n\)).

\end{definition}

\begin{definition}
\protect\hypertarget{def:treatment}{}\label{def:treatment}

The \textbf{treatments} or \textbf{support points} are the \emph{distinct} combinations of factor or variable values in the experiment.

\end{definition}

\begin{definition}
\protect\hypertarget{def:unit}{}\label{def:unit}

An experimental \textbf{unit} is the basic element (material, animal, person, time unit, \ldots) to which a treatment can be applied to produce a response.

\end{definition}

In Example \ref{exm:motivation} (comparing two treatments):

\begin{itemize}
\tightlist
\item
  Response \(Y\): Measured outcome, e.g.~protein level or pain score in clinical trial, yield in an agricultural field trial.
\item
  Factor \(x\): ``treatment'' applied
\item
  Levels
  \[
  \begin{array}{ll}
  \textrm{treatment 1}&x =-1\\
  \textrm{treatment 2}&x =+1
  \end{array}
  \]
\item
  Treatment or support point: Two treatments or support points
\item
  Experimental unit: Subject (person, animal, plot of land, \ldots).
\end{itemize}

\hypertarget{principles}{%
\section{Principles of experimentation}\label{principles}}

Three fundamental principles that need to be considered when designing an experiment are:

\begin{itemize}
\tightlist
\item
  replication
\item
  randomisation
\item
  stratification (blocking)
\end{itemize}

\hypertarget{replication}{%
\subsection{Replication}\label{replication}}

Each treatment is applied to a number of experimental units, with the \(j\)th treatment replicated \(r_{j}\) times. This enables the estimation of the variances of treatment effect estimators; increasing the number of replications, or replicates, decreases the variance of estimators of treatment effects.
(Note: proper replication involves independent application of the treatment to different experimental units, not just taking several measurements from the same unit).

\hypertarget{randomisation}{%
\subsection{Randomisation}\label{randomisation}}

Randomisation should be applied to the allocation of treatments to units. Randomisation protects against \textbf{bias}; the effect of
variables that are unknown and potentially uncontrolled or
subjectivity in applying treatments. It also provides a formal basis
for inference and statistical testing.

For example, in a clinical trial to compare a new drug and a control random allocation protects against

\begin{itemize}
\tightlist
\item
  ``unmeasured and uncontrollable'' features (e.g.~age, sex, health)
\item
  bias resulting from the clinician giving new drug to patients who are sicker.
\end{itemize}

Clinical trials are usually also \emph{double-blinded}, i.e.~neither the healthcare professional nor the patient knows which treatment the patient is receiving.

\hypertarget{intro-blocking}{%
\subsection{Stratification (or blocking)}\label{intro-blocking}}

We would like to use a wide variety of experimental units (e.g.~people or plots of land) to ensure \textbf{coverage} of our results, i.e.~validity of our conclusions across the population of interest. However, if the sample of units from the population is too heterogenous, then this will induce too much random variability, i.e.~increase \(\sigma^{2}\) in \(\varepsilon_{j}\sim N(0,\sigma^{2})\), and hence increase the variance of our parameter estimators.

We can reduce this extraneous variation by splitting our units into homogenous sets, or \textbf{blocks}, and including a blocking term in the model. The simplest blocked experiment is a \textbf{randomised complete block design}, where each block contains enough units for all treatments to be applied. Comparisons can then be made \emph{within} each block.

Basic principle: block what you can, randomise what you cannot.

Later we will look at blocking in more detail, and the principle of \textbf{incomplete blocks}.

\hypertarget{lin-model-rev}{%
\section{Revision on the linear model}\label{lin-model-rev}}

Recall: \(\boldsymbol{Y}=X\boldsymbol{\beta}+\boldsymbol{\varepsilon}\), with \(\boldsymbol{\varepsilon}\sim N(\boldsymbol{0},I_n\sigma^{2})\). Let the \(j\)th row of \(X\) be denoted \(\boldsymbol{x}^\textrm{T}_j\), which holds the values of the predictors, or explanatory variables, for the \(j\)th observation. Then

\begin{equation*}
Y_j=\boldsymbol{x}_j^{\textrm{T}}\boldsymbol{\beta}+\varepsilon_j\,,\quad j=1,\ldots,n\,.
\end{equation*}

For example, quite commonly, for continuous variables

\[
\boldsymbol{x}_j=(1,x_{1j},x_{2j},\dots,x_{mj})^{\textrm{T}}\,,
\]

and so
\[
\boldsymbol{x}_j^{\textrm{T}}\boldsymbol{\beta}=\beta_{0}+\beta_{1}x_{1j}+\dots+\beta_{m}x_{mj}\,.
\]

The laest squares estimators are given by

\begin{equation}
\hat{\boldsymbol{\beta}}=(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y}\,,\nonumber
\end{equation}

with

\begin{equation}
\textrm{Var}(\hat{\boldsymbol{\beta}})=(X^{\textrm{T}}X)^{-1}\sigma^{2}\,.\nonumber
\end{equation}

\hypertarget{variance-of-a-predictionfitted-value}{%
\subsection{Variance of a Prediction/Fitted Value}\label{variance-of-a-predictionfitted-value}}

A prediction of the mean response at point \(\boldsymbol{x}_0\) (which may or may not be in the design) is

\[
\hat{Y}_0 = \boldsymbol{x}_0^{\textrm{T}}\hat{\boldsymbol{\beta}}\,,
\]

with

\begin{align*}
\textrm{Var}(\hat{Y}_0) & = \textrm{Var}\left(\boldsymbol{x}_0^{\textrm{T}}\hat{\boldsymbol{\beta}}\right) \\
& = \boldsymbol{x}_0^{\textrm{T}}\textrm{Var}(\hat{\boldsymbol{\beta}})\boldsymbol{x}_0 \\
& = \boldsymbol{x}_0^{\textrm{T}}(X^{\textrm{T}}X)^{-1}\boldsymbol{x}_0\sigma^{2}\,.
\end{align*}

For a linear model, this variance depends only on the assumed regression model and the design (through \(X\)), the point at which prediction is to be made (\(\boldsymbol{x}_0\)) and the value of \(\sigma^2\); it does not depend on data \(\boldsymbol{Y}\) or parameters \(\boldsymbol{\beta}\).

Similarly, we can find the variance-covariance matrix of the fitted values:
\[
\textrm{Var}(\hat{Y})=\textrm{Var}(X\hat{\boldsymbol{\beta}})=X(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\sigma^{2}\,.
\]

\hypertarget{analysis-of-variance-and-r2-as-model-comparison}{%
\subsection{\texorpdfstring{Analysis of Variance and R\(^{2}\) as Model Comparison}{Analysis of Variance and R\^{}\{2\} as Model Comparison}}\label{analysis-of-variance-and-r2-as-model-comparison}}

To assess the goodness-of-fit of a model, we can use the residual sum of squares

\begin{align*}
\textrm{RSS} & = (\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})^{\textrm{T}} (\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})\\
& = \sum^{n}_{j=1}\left\{Y_{j}-\boldsymbol{x}_{j}^{\textrm{T}}\hat{\boldsymbol{\beta}}\right\}^{2}\\
& = \sum^{n}_{j=1}r_{j}^{2}\,,
\end{align*}

where

\[
r_{j}=Y_{j}-\boldsymbol{x}_{j}^{\textrm{T}}\hat{\boldsymbol{\beta}}\,.
\]

Often, a comparison is made to the null model

\[
Y_{j}=\beta_{0}+\varepsilon_{j}\,,
\]

i.e.~\(Y_{i}\sim N(\beta_{0},\sigma^{2})\). The residual sum of squares for the null model is given by

\[
\textrm{RSS}(\textrm{null}) = \boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y} - m\bar{Y}^{2}\,,
\]
as

\[
\hat{\beta}_{0} = \bar{Y} = \frac{1}{n}\sum_{j=1}^n Y_{j}\,.
\]

How do we compare these models?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ratio of residual sum of squares:
  \begin{align*}
  R^{2} & = 1 - \frac{\textrm{RSS}}{\textrm{RSS}(\textrm{null})} \\
  & = 1 - \frac{(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})}{\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y}-n\bar{Y}^{2}}\,.
  \end{align*}
\end{enumerate}

The quantity \(0\leq R^{2}\leq 1\) is sometimes called the \textbf{coefficient of multiple determination}:

\begin{itemize}
\tightlist
\item
  high \(R^{2}\) implies that the model describes much of the variation in the data;
\item
  \textbf{but} note that \(R^{2}\) will always increase as \(p\) (the number of explanatory variables) increases, with \(R^{2}=1\) when \(p=n\);
\item
  some software packages will report the adjusted \(R^{2}\).
\end{itemize}

\begin{align*}
R^{2}_{a} & = 1-\frac{\textrm{RSS}/(n-p)}{\textrm{RSS}(\textrm{null})/(n-1)}\\
& = 1 - \frac{(\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})^{\textrm{T}} (\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})/(n-p)}{(\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y} - n\bar{Y}^{2})/(n-1)};
\end{align*}

\begin{itemize}
\tightlist
\item
  \(R_a^2\) does not necessarily increase with \(p\) (as we divide by degrees of freedom to adjust for complexity of the model).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Analysis of variance (ANOVA): An ANOVA table is compact way of presenting the results of (sequential) comparisons of nested models. You should be familiar with an ANOVA table of the following form.
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}@{}}
\caption{\label{tab:anova} A standard ANOVA table.}\tabularnewline
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Degress of Freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(Sequential) Sum of Squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} \\
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Degress of Freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(Sequential) Sum of Squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Square
\end{minipage} \\
\midrule
\endhead
Regression & \(p-1\) & By subtraction; see \eqref{eq:SSS} & Reg SS/\((p-1)\) \\
Residual & \(n-p\) & \((\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})\)\footnote{Residual sum of squares for the full regression model.} & RSS/\((n-p)\) \\
Total & \(n-1\) & \(\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y}-n\bar{Y}^{2}\)\footnote{Residual sum of squares for the null model.} & \\
\bottomrule
\end{longtable}

In row 1 of Table \ref{tab:anova} above,
\begin{align}
\textrm{Regression SS = Total SS $-$ RSS} & = \boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y} - n\bar{Y}^{2} - (\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})\\
& = -n\bar{Y}^{2}-\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}+2\hat{\boldsymbol{\beta}}^{\textrm{T}}X^{\textrm{T}}\boldsymbol{Y} \\
& = \hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}-n\bar{Y}^{2}\,,
\label{eq:SSS}
\end{align}

with the last line following from
\begin{align*}
\hat{\boldsymbol{\beta}}^{\textrm{T}}X^{\textrm{T}}\boldsymbol{Y} & =
\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y} \\
& = \hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}
\end{align*}

This idea can be generalised to the comparison of a \emph{sequence} of nested models - see Problem Sheet 1.

Hypothesis testing is performed using the mean square:

\begin{equation}
\frac{\textrm{Regression SS}}{p-1}=\frac{\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}-n\bar{Y}^{2}}{p-1}\,.\nonumber
\end{equation}

Under \(\textrm{H}_{0}: \beta_{1}=\dots=\beta_{p-1}=0\)

\begin{align*}
\frac{\textrm{Regression SS}/(p-1)}{\textrm{RSS}/(n-p)} & = \frac{(\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}} - n\bar{Y}^{2})/(p-1)}{(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})/(n-p)}\nonumber\\
& \sim F_{p-1,n-p}\,,
\end{align*}

an \(F\) distribution with \(p-1\) and \(n-p\) degrees of freedom; defined via the ratio of two independent \(\chi^{2}\) distributions.

Also,

\begin{equation*}
\frac{\textrm{RSS}}{n-p}=\frac{(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})}{n-p}=\hat{\sigma}^{2}
\end{equation*}

is an unbiased estimator for \(\sigma^{2}\), and

\begin{equation*}
\frac{(n-p)}{\sigma^{2}}\hat{\sigma}^{2}\sim\chi^{2}_{n-p}\,.
\end{equation*}

This is a Chi-squared distribution with \(n-p\) degrees of freedom (see MATH2010 or MATH6174 notes).

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \citep[Adapted from][]{Morris2011} A classic and famous example of a simple hypothetical experiment was described by \citet{Fisher1935}:

  \begin{quote}
  A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was added first to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. For this purpose let us first lay down a simple form of experiment with a view to studying its limitations and its characteristics, both those that same essential to the experimental method, when well developed, and those that are not essential but auxiliary.

  Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgement in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is an order not determined arbitrarily by human choice, but by the actual manipulation of the physical appartatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling numbers purporting to give the actual results of such manipulation\footnote{Now, we would use routines such as \texttt{sample} in \texttt{R}.}. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received.
  \end{quote}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Define the treatments in this experiment.
  \item
    Identify the units in this experiment.
  \item
    How might a ``physical appartatus'' from a ``game of chance'' be used to perform the randomisation. Explain one example.
  \item
    Suppose eight tea cups are available for this experiment but they are not identical. Instead they come from two sets. Foru are made from heavy, thick porcelain; four from much lighter china. If each cup can only be used once, how might this fact be incorporated into the design of the experiment?
  \end{enumerate}
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  There are two treatments in the experiment - the two ingredients ``milk first'' and ``tea first''.
\item
  The experimental units are the ``cups of tea'', made up from the tea and milk used and also the cup itself.
\item
  The simplest method here might be to select four black playing cards and four red playing cards, assign one treatment to each colour, shuffle the cards, and then draw them in order. The colour drawn indicates the treatment that should be used to make the next cup of tea. This operation would give one possible randomisation.

  We could of course also use \texttt{R}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sample}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Milk first"}\NormalTok{, }\StringTok{"Tea first"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)), }\AttributeTok{size =} \DecValTok{8}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Milk first" "Tea first"  "Tea first"  "Milk first" "Milk first"
## [6] "Milk first" "Tea first"  "Tea first"
\end{verbatim}
\item
  Type of cup could be considered as a blocking factor. One way of incorporating it would be to split the experiment into two (blocks), each with four cups (two milk first, two tea first). We would still wish to randomise allocation of treatments to units within blocks.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# block 1}
\FunctionTok{sample}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Milk first"}\NormalTok{, }\StringTok{"Tea first"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\AttributeTok{size =} \DecValTok{4}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Milk first" "Milk first" "Tea first"  "Tea first"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# block 2}
\FunctionTok{sample}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Milk first"}\NormalTok{, }\StringTok{"Tea first"}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\AttributeTok{size =} \DecValTok{4}\NormalTok{, }\AttributeTok{replace =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Tea first"  "Milk first" "Milk first" "Tea first"
\end{verbatim}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Consider the linear model

  \[\boldsymbol{y}= X\boldsymbol{\beta}+ \boldsymbol{\varepsilon}\,,\]
  with \(\boldsymbol{y}\) an \(n\times 1\) vector of responses, \(X\) a \(n\times p\) model matrix and \(\boldsymbol{\varepsilon}\) a \(n\times 1\) vector of independent and identically distributed random variables with constant variance \(\sigma^2\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Derive the least squares estimator \(\hat{\boldsymbol{\beta}}\) for this multiple linear regression model, and show that this estimator is unbiased. Using the definition of (co)variance, show that
  \end{enumerate}

  \[\mbox{Var}(\hat{\boldsymbol{\beta}}) = \left(X^{\mathrm{T}}X\right)^{-1}\sigma^2\,.\]

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    If \(\boldsymbol{\varepsilon}\sim N (\boldsymbol{0},I_n\sigma^2)\), with \(I_n\) being the \(n\times n\) identity matrix, show that the maximum likelihood estimators for \(\boldsymbol{\beta}\) coincide with the least squares estimators.
  \end{enumerate}
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The method of least squares minimises the sum of squared differences between the responses and the expected values, that is, minimises the expression

  \[
   (\boldsymbol{y}-X\boldsymbol{\beta})^{\mathrm{T}}(\boldsymbol{y}-X\boldsymbol{\beta}) = \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- 2\boldsymbol{\beta}^{\mathrm{T}}X^{\mathrm{T}}\boldsymbol{y}+ \boldsymbol{\beta}^{\mathrm{T}}X^{\mathrm{T}}X\boldsymbol{\beta}\,.
   \]
  Differentiating with respect to the vector \(\boldsymbol{\beta}\), we obtain

  \[
   \frac{\partial}{\partial\boldsymbol{\beta}} = -2X^{\mathrm{T}}\boldsymbol{y}+ 2X^{\mathrm{T}}X\boldsymbol{\beta}\,.
   \]

  Set equal to \(\boldsymbol{0}\) and solve:

  \[
   X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} = X^{\mathrm{T}}\boldsymbol{y}\Rightarrow \hat{\boldsymbol{\beta}} = \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}\boldsymbol{y}\,.
   \]

  The estimator \(\hat{\boldsymbol{\beta}}\) is unbiased:

  \[
   E(\hat{\boldsymbol{\beta}}) = \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}E(\boldsymbol{y}) = \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}X\boldsymbol{\beta}= \boldsymbol{\beta}\,,
   \]

  and has variance:

  \begin{align*}
  \mbox{Var}(\hat{\boldsymbol{\beta}}) & =E\left\{ \left[\hat{\boldsymbol{\beta}} - E(\hat{\boldsymbol{\beta}})\right] \left[\hat{\boldsymbol{\beta}} - E(\hat{\boldsymbol{\beta}})\right]^{\mathrm{T}} \right\}\\
   & =  E\left\{ \left[\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right] \left[\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right]^{\mathrm{T}} \right\}\\
   & = E\left\{ \hat{\boldsymbol{\beta}}\hat{\boldsymbol{\beta}}^{\mathrm{T}} - 2\boldsymbol{\beta}\hat{\boldsymbol{\beta}}^{\mathrm{T}} + \boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}} \right\}\\
   & = E\left\{ \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}\boldsymbol{y}\boldsymbol{y}^{\mathrm{T}}X\left(X^{\mathrm{T}}X\right)^{-1}  - 2\boldsymbol{\beta}\boldsymbol{y}^{\mathrm{T}}X\left(X^{\mathrm{T}}X\right)^{-1} + \boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}}\right\}\\
   & = \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}E(\boldsymbol{y}\boldsymbol{y}^{\mathrm{T}})X\left(X^{\mathrm{T}}X\right)^{-1} - 2\boldsymbol{\beta}E(\boldsymbol{y}^{\mathrm{T}})X\left(X^{\mathrm{T}}X\right)^{-1} + \boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}}\\
   & = \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}\left[\mbox{Var}(\boldsymbol{y}) + E(\boldsymbol{y})E(\boldsymbol{y}^{\mathrm{T}})\right]X\left(X^{\mathrm{T}}X\right)^{-1} - 2\boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}}X^{\mathrm{T}}X\left(X^{\mathrm{T}}X\right)^{-1} + \boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}}\\
   & = \left(X^{\mathrm{T}}X\right)^{-1}X^{\mathrm{T}}\left[I_N\sigma^2 + X\boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}}X^{\mathrm{T}}\right]X\left(X^{\mathrm{T}}X\right)^{-1} - \boldsymbol{\beta}\boldsymbol{\beta}^{\mathrm{T}}\\
   & = \left(X^{\mathrm{T}}X\right)^{-1}\sigma^2\,. 
   \end{align*}
\item
  As \(\boldsymbol{y}\sim N\left(X\boldsymbol{\beta}, I_N\sigma^2\right)\), the likelihood is given by

  \[
   L(\boldsymbol{\beta}\,; \boldsymbol{y}) = \left(2\pi\sigma^2\right)^{-N/2}\exp\left\{-\frac{1}{2\sigma^2}(\boldsymbol{y}- X\boldsymbol{\beta})^{\mathrm{T}}(\boldsymbol{y}- X\boldsymbol{\beta})\right\}\,.
   \]

  The log-likelihood is given by

  \[
   l(\boldsymbol{\beta}\,;\boldsymbol{y}) = -\frac{1}{2\sigma^2}(\boldsymbol{y}- X\boldsymbol{\beta})^{\mathrm{T}}(\boldsymbol{y}- X\boldsymbol{\beta}) + \mbox{constant}\,.
   \]

  Up to a constant, this expression is \(-1\times\) the least squares equations; hence maximising the log-likelihood is equivalent to minimising the least squares equation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Consider the two nested linear models

  \begin{enumerate}
  \def\labelenumii{(\roman{enumii})}
  \item
    \(Y_j = \beta_0 + \beta_1x_{1j} + \beta_2x_{2j} + \ldots + \beta_{p_1}x_{p_1j} + \varepsilon_j\), or \(\boldsymbol{y}= X_1\boldsymbol{\beta}_1 + \boldsymbol{\varepsilon}\),
  \item
    \(Y_j = \beta_0 + \beta_1x_{1j} + \beta_2x_{2j} + \ldots + \beta_{p_1}x_{p_1j} + \beta_{p_1+1}x_{(p_1+1)j} + \ldots + \beta_{p_2}x_{p_2j} + \varepsilon_j\), or \(\boldsymbol{y}= X_1\boldsymbol{\beta}_1 + X_2\boldsymbol{\beta}_2+ \boldsymbol{\varepsilon}\)
  \end{enumerate}

  with \(\varepsilon_j\sim N(0, \sigma^2)\), and \(\varepsilon_{j}\), \(\varepsilon_{k}\) independent \((\boldsymbol{\varepsilon}\sim N(\boldsymbol{0},I_n\sigma^2))\).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Construct an ANOVA table to compare model (ii) with the null model \(Y_j=\beta_0 + \varepsilon_j\).
  \item
    Extend this ANOVA table to compare models (i) and (ii) by further decomposing the regression sum of squares for model (ii).
  \end{enumerate}

  \textbf{Hint:} which residual sum of squares are you interested in to compare models (i) and (ii)?

  You should end up with an ANOVA table of the form

  \begin{longtable}[]{@{}lccc@{}}
  \toprule
  Source & Degrees of freedom & Sums of squares & Mean square \\
  \midrule
  \endhead
  Model (i) & \(p_1\) & ? & ? \\
  Model (ii) & \(p_2\) & ? & ? \\
  Residual & \(n-p_1-p_2-1\) & ? & ? \\
  Total & \(n-1\) & \(\boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- n\bar{Y}^2\) & \\
  \bottomrule
  \end{longtable}

  The second row of the table gives the \textbf{extra sums of squares} for the additional terms in fitting model (ii), over and above those in model (i).

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{2}
  \tightlist
  \item
    Calculate the extra sum of squares for fitting the terms in model (i), over and above those terms only in model (ii), i.e.~those held in \(X_2\boldsymbol{\beta}_2\). Construct an ANOVA table containing both the extra sum of squares for the terms only in model (i) and the extra sum of squares for the terms only in model (ii). Comment on the table.
  \end{enumerate}
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  From lectures

  \begin{longtable}[]{@{}
    >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.12}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.36}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.30}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.22}}@{}}
  \toprule
  \begin{minipage}[b]{\linewidth}\raggedright
  Source
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Degrees of freedom
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Sums of squares
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Mean square
  \end{minipage} \\
  \midrule
  \endhead
  Regression & \(p_1+p_2\) & \(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\) & \(\left(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\right)/(p_1+p_2)\) \\
  Residual & \(n-p_1-p_2-1\) & \((\boldsymbol{y}- X\hat{\boldsymbol{\beta}})^{\mathrm{T}}(\boldsymbol{y}- X\hat{\boldsymbol{\beta}})\) & RSS\(/(n-p_1-p_2-1)\) \\
  Total & \(n-1\) & \(\boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- n\bar{Y}^2\) & \\
  \bottomrule
  \end{longtable}

  where

  \begin{align*}
  \mbox{RSS(null) - RSS(ii)} & = \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- n\bar{Y}^2 - (\boldsymbol{y}- X\hat{\boldsymbol{\beta}})^{\mathrm{T}}(\boldsymbol{y}- X\hat{\boldsymbol{\beta}})\\
  & = \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- n\bar{Y}^2 - \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}+ 2\boldsymbol{y}^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}}\\
  & = 2\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\\
  & = \hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\,.
   \end{align*}
\item
  To compare model (i) with the null model, we compute

  \begin{align*}
   \mbox{RSS(null) - RSS(i)} & = \boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- N\bar{Y}^2 - (\boldsymbol{y}- X_1\hat{\boldsymbol{\beta}}_1)^{\mathrm{T}}(\boldsymbol{y}- X_1\hat{\boldsymbol{\beta}}_1)\\
   & = \hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1 - n\bar{Y}^2\,.
   \end{align*}

  To compare models (i) and (ii), we compare them both to the null model, and look at the difference between these comparisons:

  \begin{align*}
   \mbox{[RSS(null) - RSS(ii)] - [RSS(null) - RSS(i)]} & = \mbox{RSS(i) - RSS(ii)}\\
   & = (\boldsymbol{y}- X_1\hat{\boldsymbol{\beta}}_1)^{\mathrm{T}}(\boldsymbol{y}- X_1\hat{\boldsymbol{\beta}}_1) - (\boldsymbol{y}- X\hat{\boldsymbol{\beta}})^{\mathrm{T}}(\boldsymbol{y}- X\hat{\boldsymbol{\beta}})\\
   & = \hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1\,.
   \end{align*}

  \begin{longtable}[]{@{}
    >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.12}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.36}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.30}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.22}}@{}}
  \toprule
  \begin{minipage}[b]{\linewidth}\raggedright
  Source
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Degrees of freedom
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Sums of squares
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Mean square
  \end{minipage} \\
  \midrule
  \endhead
  Regression & \(p_1+p_2\) & \(\hat{\boldsymbol{\beta}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\) & \(\left(\hat{\boldsymbol{\beta}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\right)/(p_1+p_2)\) \\
  Model (i) & \(p_1\) & \(\hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1 - n\bar{Y}^2\) & \(\left(\hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1 - n\bar{Y}^2\right)/p_1\) \\
  Extra due to Model (ii) & \(p_2\) & \(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1\) & \(\left(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1\right)/p_2\) \\
  Residual & \(n-p_1-p_2-1\) & \((\boldsymbol{y}- X\hat{\boldsymbol{\beta}})^{\mathrm{T}}(\boldsymbol{y}- X\hat{\boldsymbol{\beta}})\) & RSS\(/(n-p_1-p_2-1)\) \\
  Total & \(n-1\) & \(\boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- n\bar{Y}^2\) & \\
  \bottomrule
  \end{longtable}
\end{enumerate}

By definition, the Model (i) SS and the Extra SS for Model (ii) sum to the Regression SS.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The extra sum of squares for the terms in model (i) over and above those in model (ii) are obtained through comparison of the models

  ia. \(\boldsymbol{y}= X_2\boldsymbol{\beta}_2 + \boldsymbol{\varepsilon}\),

  iia. \(\boldsymbol{y}= X_1\boldsymbol{\beta}_1 + X_2\boldsymbol{\beta}_2+ \boldsymbol{\varepsilon}= X\boldsymbol{\beta}+ \varepsilon\)

  Extra sum of squares for model (iia):

  \begin{align*}
  \mbox{[RSS(null) - RSS(iia)] - [RSS(null) - RSS(ia)]} & = \mbox{RSS(ia) - RSS(iia)}\\
  & = (\boldsymbol{y}- X_2\hat{\boldsymbol{\beta}}_2)^{\mathrm{T}}(\boldsymbol{y}- X_2\hat{\boldsymbol{\beta}}_2) - (\boldsymbol{y}- X\hat{\boldsymbol{\beta}})^{\mathrm{T}}(\boldsymbol{y}- X\hat{\boldsymbol{\beta}})\\
  & = \hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_2^{\mathrm{T}}X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\beta}}_2\,.
   \end{align*}

  Hence, an ANOVA table for the extra sums of squares is given by

  \begin{longtable}[]{@{}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.12}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.36}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.30}}
    >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.22}}@{}}
  \toprule
  \begin{minipage}[b]{\linewidth}\centering
  Source
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Degrees of freedom
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Sums of squares
  \end{minipage} & \begin{minipage}[b]{\linewidth}\centering
  Mean square
  \end{minipage} \\
  \midrule
  \endhead
  Regression & \(p_1+p_2\) & \(\hat{\boldsymbol{\beta}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\) & \(\left(\hat{\boldsymbol{\beta}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - n\bar{Y}^2\right)/(p_1+p_2)\) \\
  Extra Model (i) & \(p_1\) & \(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_2^{\mathrm{T}}X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\beta}}_2\) & \(\left(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_2^{\mathrm{T}}X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\beta}}_2\right)/p_1\) \\
  Extra Model (ii) & \(p_2\) & \(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1\) & \(\left(\hat{\boldsymbol{\beta}}^{\mathrm{T}}X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} - \hat{\boldsymbol{\beta}}_1^{\mathrm{T}}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}}_1\right)/p_2\) \\
  Residual & \(n-p_1-p_2-1\) & \((\boldsymbol{y}- X\hat{\boldsymbol{\beta}})^{\mathrm{T}}(\boldsymbol{y}- X\hat{\boldsymbol{\beta}})\) & RSS\(/(n-p_1-p_2-1)\) \\
  Total & \(n-1\) & \(\boldsymbol{y}^{\mathrm{T}}\boldsymbol{y}- n\bar{Y}^2\) & \\
  \bottomrule
  \end{longtable}
\end{enumerate}

Note that for these \emph{adjusted} sums of squares, in general the extra sum of squares for model (i) and (ii) do not sum to the regression sum of squares. This will only be the case if the columns of \(X_1\) and \(X_2\) are mutually orthogonal, i.e.~\(X_1^{\mathrm{T}}X_2 = \boldsymbol{0}\).

\hypertarget{crd}{%
\chapter{Completely randomised designs}\label{crd}}

The simplest form of experiment we will consider compares \(t\) different \textbf{unstructured} treatments. By unstructured, we mean the treatments form a discrete collection, not related through the settings of other experimental features (compare with factorial experiments in Chapter \ref{factorial}). We also make the assumption that there are no restrictions in the randomisation of treatments to experimental units (compare with Chapter \ref{blocking} on blocking). A designs for such an experiment is therefore called a \textbf{completely randomised design} (CRD).

\begin{example}
\protect\hypertarget{exm:one-way}{}\label{exm:one-way}

Pulp experiment \citep[ch.~2]{WH2009}

In a paper pulping mill, an experiment was run to examine differences between the reflectance (brightness; ratio of amount of light leaving a target to the amount of light striking the target) of sheets of pulp made by \(t=4\) operators. The data are given in Table \ref{tab:pulp-expt-data} below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pulp }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{operator =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\DecValTok{5}\NormalTok{),}
                   \AttributeTok{repetition =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)), }
                   \AttributeTok{reflectance =} \FunctionTok{c}\NormalTok{(}\FloatTok{59.8}\NormalTok{, }\FloatTok{59.8}\NormalTok{, }\FloatTok{60.7}\NormalTok{, }\FloatTok{61.0}\NormalTok{, }\FloatTok{60.0}\NormalTok{, }\FloatTok{60.2}\NormalTok{, }\FloatTok{60.7}\NormalTok{, }\FloatTok{60.8}\NormalTok{, }
                                    \FloatTok{60.8}\NormalTok{, }\FloatTok{60.4}\NormalTok{, }\FloatTok{60.5}\NormalTok{, }\FloatTok{60.6}\NormalTok{, }\FloatTok{60.8}\NormalTok{, }\FloatTok{59.9}\NormalTok{, }\FloatTok{60.9}\NormalTok{, }\FloatTok{60.5}\NormalTok{, }\FloatTok{59.8}\NormalTok{, }\FloatTok{60.0}\NormalTok{, }\FloatTok{60.3}\NormalTok{, }\FloatTok{60.5}\NormalTok{)}
\NormalTok{                     )}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(pulp, }\AttributeTok{names\_from =}\NormalTok{ operator, }\AttributeTok{values\_from =}\NormalTok{ reflectance)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{],}
 \AttributeTok{col.names =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Operator"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{),}
 \AttributeTok{caption =} \StringTok{"Pulp experiment: reflectance values (unitless) from four different operators."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:pulp-expt-data}Pulp experiment: reflectance values (unitless) from four different operators.}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
Operator 1 & Operator 2 & Operator 3 & Operator 4\\
\hline
59.8 & 59.8 & 60.7 & 61.0\\
\hline
60.0 & 60.2 & 60.7 & 60.8\\
\hline
60.8 & 60.4 & 60.5 & 60.6\\
\hline
60.8 & 59.9 & 60.9 & 60.5\\
\hline
59.8 & 60.0 & 60.3 & 60.5\\
\hline
\end{tabular}
\end{table}

The experiment has one factor (operator) with four levels (sometimes called a one-way layout). The CRD employed has equal replication of each treatment (operator).

We can informally compare the responses from these four treatments graphically.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(reflectance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ operator, }\AttributeTok{data =}\NormalTok{ pulp)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_math3014-6027_files/figure-latex/pulp-boxplot-1} 

}

\caption{Pulp experiment: distributions of reflectance from the four operators.}\label{fig:pulp-boxplot}
\end{figure}

Figure \ref{fig:pulp-boxplot} shows that, relative to the variation, there may be a difference in the mean response between treatments 1 and 2, and 3 and 4. In this chapter, we will see how to make this comparison formally using linear models, and to assess how the choice of design impacts our results.

\end{example}

Throughout this chapter we will assume the \(i\)th treatment is applied to \(n_i\) experimental unit, with total number of runs \(n = \sum_{i=1}^t n_i\) in the experiment.

\hypertarget{a-unit-treatment-linear-model}{%
\section{A unit-treatment linear model}\label{a-unit-treatment-linear-model}}

An appropriate, and common, model to describe data from such experiments when the response is continuous is given by

\begin{equation}
y_{ij} = \mu + \tau_i + \varepsilon_{ij}\,, \quad i = 1, \ldots, t; j = 1, \ldots, n_i\,, 
\label{eq:utm}
\end{equation}

where \(y_{ij}\) is the response from the \(j\)th application of treatment \(i\), \(\mu\) is a constant parameter, \(\tau_i\) is the effect of the \(i\)th treatment, and \(\varepsilon_{ij}\) is the random individual effect from each experimental unit with \(E(\varepsilon_{ij})=0\) and \(\mathrm{Var}(\varepsilon_{ij}) = \sigma^2\). All random errors are assumed independent and here we also assume \(\varepsilon_{ij} \sim N(0, \sigma^2)\).

Model \eqref{eq:utm} assumes that each treatment can be randomly allocated to one of the \(n\) experimental units, and that the response observed is independent of the allocation of all the other treatments (the stable unit treatment value assumption or SUTVA).

Why is this model appropriate and commonly used? The expected response from the application of the \(i\)th treatment is

\[
E(y_{ij}) = \mu + \tau_i\,.
\]
The parameter \(\mu\) can be thought of as representing the impact of many different features particular to \textbf{this} experiment but common to all units, and \(\tau_i\) is the deviation due to applying treatment \(i\). From the applicable of two different hypothetical experiments, A and B, the expected response from treatment \(i\) may be different due to a different overall mean. From experiment A:

\[
E(y_{ij}) = \mu_{\mathrm{A}} + \tau_i\,.
\]
From experiment B:
\[
E(y_{ij}) = \mu_{\mathrm{B}} + \tau_i\,.
\]
But the \textbf{difference} between treatments \(k\) and \(l\) (\(k, l = 1,\ldots, t\))

\begin{align*}
E(y_{kj}) - E(y_{lj}) & = \mu_A + \tau_k - \mu_A - \tau_l \\
& = \tau_k - \tau_l\,,
\end{align*}

is constant across different experiments. This concept of \textbf{comparison} underpins most design of experiments, and will be applied throughout this module.

\hypertarget{the-partitioned-linear-model}{%
\section{The partitioned linear model}\label{the-partitioned-linear-model}}

In matrix form, we can write model \eqref{eq:utm} as

\[
\boldsymbol{y}= X_1\mu + X_2\boldsymbol{\tau}+ \boldsymbol{\varepsilon}\,,
\]
where \(X_1 = \boldsymbol{1}_n\), the \(n\)-vector with every entry equal to one,

\[
X_2 = \bigoplus_{i = 1}^t \boldsymbol{1}_{n_i} = \begin{bmatrix}
\boldsymbol{1}_{n_1} & \boldsymbol{0}_{n_1} & \cdots &  \boldsymbol{0}_{n_1} \\
\boldsymbol{0}_{n_2} & \boldsymbol{1}_{n_2} & \cdots &  \boldsymbol{0}_{n_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{n_t} & \boldsymbol{0}_{n_t} & \cdots &  \boldsymbol{1}_{n_t} \\
\end{bmatrix}\,,
\]
with \(\bigoplus\) denoting ``direct sum'', \(\boldsymbol{0}_{n_i}\) is the \(n_i\)-vector with every entry equal to zero, \(\boldsymbol{\tau}= [\tau_1, \ldots, \tau_t]^{\mathrm{T}}\) and \(\boldsymbol{\varepsilon}= [\varepsilon_{11}, \ldots, \varepsilon_{tn_t}]^{\mathrm{T}}\).

Why are we partitioning the model? Going back to our discussion of the role of \(\mu\) and \(\tau_i\), it is clear that we not interested in estimating \(\mu\), which represents an experiment-specific contribution to the expected mean. Our only interest is in estimating the (differences between the) \(\tau_i\). Hence, we can treat \(\mu\) as a nuisance parameter.

If we define \(X = [X_1\, \vert\, X_2]\) and \(\boldsymbol{\beta}^{\mathrm{T}} = [\mu \vert \boldsymbol{\tau}^{\mathrm{T}}]\), we can write the usual least squares equations

\begin{equation}
X^{\mathrm{T}}X\hat{\boldsymbol{\beta}} = X^{\mathrm{T}}\boldsymbol{y}
\label{eq:crd-ls}
\end{equation}
as a system of two matrix equations

\begin{align*}
X_1^{\mathrm{T}}X_1\hat{\mu} + X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_1^{\mathrm{T}}\boldsymbol{y}\\
X_2^{\mathrm{T}}X_1\hat{\mu} + X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_2^{\mathrm{T}}\boldsymbol{y}\,. \\
\end{align*}

Assuming \((X_1^{\mathrm{T}}X_1)^{-1}\) exists, which it does in this case, we can pre-multiply the first of these equations by \(X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}\) and subtract it from the second equation to obtain

\begin{align*}
X_2^{\mathrm{T}}[I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}]X_1\hat{\mu} 
& + X_2^{\mathrm{T}}[I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}]X_2\hat{\boldsymbol{\tau}} \\
& = X_2^{\mathrm{T}}[I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}]\boldsymbol{y}\,.
\end{align*}

Writing \(H_1 = X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\), we obtain

\begin{equation}
X_2^{\mathrm{T}}[I_n - H_1]X_1\hat{\mu} + X_2^{\mathrm{T}}[I_n - H_1]X_2\hat{\boldsymbol{\tau}} = X_2^{\mathrm{T}}[I_n - H_1]\boldsymbol{y}\,.
\label{eq:almost-reduced}
\end{equation}
The matrix \(H_1\) is a ``hat'' matrix for a linear model containing only the term \(\mu\), and hence \(H_1X_1 = X_1\) (see MATH2010 or STAT6123). Hence the first term in \eqref{eq:almost-reduced} is zero, and we obtain the \textbf{reduced normal equations} for \(\boldsymbol{\tau}\):

\begin{equation}
X_2^{\mathrm{T}}[I_n - H_1]X_2\hat{\boldsymbol{\tau}} = X_2^{\mathrm{T}}[I_n - H_1]\boldsymbol{y}\,.
\label{eq:crd-red-normal}
\end{equation}

Note that the solutions from \eqref{eq:crd-red-normal} are not different from the solution to \(\hat{\boldsymbol{\tau}}\) that would be obtained from solving \eqref{eq:crd-ls}; equation \eqref{eq:crd-red-normal} is simply a re-expression, where we have eliminated the nuisance parameter \(\mu\). This fact means that we rarely need to solve \eqref{eq:crd-red-normal} explicitly.

Recalling that for a hat matrix, \(I_n - H_1\) is idempotent and symmetric (see MATH2010 or MATH6174), if we define

\[
X_{2|1} = (I_n - H_1)X_2\,,
\]
then we can rewrite equation \eqref{eq:crd-red-normal} as

\begin{equation}
X_{2|1}^{\mathrm{T}}X_{2|1}\hat{\boldsymbol{\tau}} = X_{2|1}^{\mathrm{T}}\boldsymbol{y}\,, 
\label{eq:rne}
\end{equation}

which are the normal equations for a linear model with expectation \(E(\boldsymbol{y}) = X_{2|1}\boldsymbol{\tau}\).

\hypertarget{reduced-normal-equations-for-the-crd}{%
\section{Reduced normal equations for the CRD}\label{reduced-normal-equations-for-the-crd}}

For the CRD discussed in this chapter, \(X_1^{\mathrm{T}}X_1 = n\), the total number of runs in the experiment\footnote{In later chapters we will see examples where \(X_1\) has \(>1\) columns, and hence \(X_1^{\mathrm{T}}X_1\) is a matrix.}. Hence \((X_1^{\mathrm{T}}X_1)^{-1} = 1/n\) and \(H_1 = \frac{1}{n}J_n\), with \(J_n\) the \(n\times n\) matrix with all entries equal to 1.

The adjusted model matrix then has the form

\begin{align}
X_{2|1} & = (I_n - H_1)X_2 \nonumber\\
& = X_2 - \frac{1}{n}J_nX_2 \nonumber\\
& = X_2 - \frac{1}{n}[n_1\boldsymbol{1}_n \vert \cdots \vert n_t\boldsymbol{1}_n]\,. \label{eq:crd-x21} 
\end{align}

That is, every column of \(X_2\) has been adjusted by the subtraction of the column mean from each entry\footnote{Often called ``column centred''}. Also notice that each row of \(X_{2|1}\) has a row-sum equal to zero (\(= 1 - \sum_{i=1}^tn_t/n\)). Hence, \(X_{2|1}\) is not of full column rank, and so the reduced normal equations do not have a unique solution\footnote{If we recalled the material on ``dummy'' variables from MATH2010 or STAT6123, we would already have realised this.}.

Although \eqref{eq:rne}, and hence, \eqref{eq:crd-ls}, have no unique solution\footnote{That is, for any two solutions \(\tilde{\boldsymbol{\beta}}_1\) and \(\tilde{\boldsymbol{\beta}}_2\), \(X\tilde{\boldsymbol{\beta}}_1 = \tilde{\boldsymbol{\beta}}_1\).}, it can be shown that both \(\widehat{X_{2|1}\boldsymbol{\tau}}\) and \(\widehat{X\boldsymbol{\beta}}\) have unique solutions. Hence fitted values \(\hat{\boldsymbol{y}} = \widehat{X\boldsymbol{\beta}}\) and the residual sum of squares

\[
RSS = \left(\boldsymbol{y}- \widehat{X\boldsymbol{\beta}}\right)^{\mathrm{T}}\left(\boldsymbol{y}- \widehat{X\boldsymbol{\beta}}\right)
\]
are both uniquely defined for any solution to \eqref{eq:crd-ls}. That is, every solution to the normal equations leads to the same fitted values and residual sum of squares.

In MATH2010 and STAT6123 we fitted models with categorical variables by defining a set of dummy variables and estimating a reduced model. Here, we will take a slightly different approach and study which combinations of parameters from \eqref{eq:utm} are estimable, and in particular which linear combinations of the treatment parameters \(\tau_i\) we can estimate.

Let's study equation \eqref{eq:rne} in more detail. We have

\begin{align*}
X^{\mathrm{T}}_{2|1}X_{2|1} & = X_2^{\mathrm{T}}(I_n - H_1)X_2 \\
 & = X_2^{\mathrm{T}}X_2 - X_2^{\mathrm{T}}H_1X_2 \\
 & = \mathrm{diag}(\boldsymbol{n}) - \frac{1}{n}X_2^{\mathrm{T}}J_nX_2 \\
 & = \mathrm{diag}(\boldsymbol{n}) - \frac{1}{n}\boldsymbol{n}\boldsymbol{n}^{\mathrm{T}}\,,
\end{align*}

where \(\boldsymbol{n}^{\mathrm{T}} = (n_1, \ldots, n_t)\). Hence, the reduced normal equations become

\begin{align}
\left[\mathrm{diag}(\boldsymbol{n}) - \frac{1}{n}\boldsymbol{n}\boldsymbol{n}^{\mathrm{T}}\right]\hat{\boldsymbol{\tau}} & = X_2^{\mathrm{T}}\boldsymbol{y}- \frac{1}{n}X_2^{\mathrm{T}}J_n\boldsymbol{y}\\
& = X_2^{\mathrm{T}}\boldsymbol{y}- \boldsymbol{n}\bar{y}_{..}\,,
\label{eq:crd-rne}
\end{align}

where \(\bar{y}_{..} = \frac{1}{n}\sum_{i = 1}^t\sum_{j = 1}^{n_i} y_{ij}\), i.e.~the overall average of the observations from the experiment.

From \eqref{eq:crd-rne} we obtain a system of \(t\) equations, each having the form

\begin{equation}
\hat{\tau}_i - \hat{\tau}_w = \bar{y}_{i.} - \bar{y}_{..}\,,
\label{eq:crd-irne}
\end{equation}

where \(\hat{\tau}_w = \frac{1}{n}\sum_{i=1}^tn_i\hat{\tau}_i\) and \(\bar{y}_{i.} = \frac{1}{n_i}\sum_{j = 1}^{n_i}y_{ij}\) \((i = 1, \ldots, t)\).

These \(t\) equations are not independent; when multiplied by the \(n_i\), they sum to zero due to the linear dependency between the columns of \(X_{2|1}\). Hence, there is no unique solution to \(\hat{\boldsymbol{\tau}}\) from equation \eqref{eq:crd-rne}. However, we can estimate certain linear combinations of the \(\tau_i\), called \emph{contrasts}.

\hypertarget{contrasts}{%
\section{Contrasts}\label{contrasts}}

\begin{definition}
\protect\hypertarget{def:contrast}{}\label{def:contrast}

A treatment \textbf{contrast} is a linear combination \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\) with coefficient vector \(\boldsymbol{c}^{\mathrm{T}} = (c_1,\ldots, c_t)\) such that \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{1} = 0\); that is, \(\sum_{i = 1}^t c_i = 0\).

\end{definition}

For example, assume we have \(t = 3\) treatments, then the following vectors \(\boldsymbol{c}\) all define contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (1, -1, 0)\),
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (1, 0, -1)\),
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (0, 1, -1)\).
\end{enumerate}

In fact, they define all \({3\choose 2} = 3\) pairwise comparisons between treatments. The following are also contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (2, -1, -1)\),
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (0.5, -1, 0.5)\),
\end{enumerate}

each comparing the sum, or average, of expected responses from two treatments to the expected response from the remaining treatment.

The following are not contrasts, as \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{1} \ne 0\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (2, -1, 0)\),
\item
  \(\boldsymbol{c}^{\mathrm{T}} = (1, 0, 0)\),
\end{enumerate}

with the final example once again demonstrating that we cannot estimate the individual \(\tau_i\).

\hypertarget{contrast-crd}{%
\section{Treatment contrast estimators in the CRD}\label{contrast-crd}}

We estimate a treatment contrast \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\) in the CRD via linear combinations of equations \eqref{eq:crd-irne}:

\begin{align*}
& \sum_{i=1}^t c_i\hat{\tau}_i - \sum_{i=1}^tc_i\hat{\tau}_w = \sum_{i=1}^tc_i\bar{y}_{i.} - \sum_{i=1}^tc_i\bar{y}_{..} \\
\Rightarrow & \sum_{i=1}^t c_i\hat{\tau}_i = \sum_{i=1}^tc_i\bar{y}_{i.}\,,
\end{align*}

as \(\sum_{i=1}^tc_i\hat{\tau}_w = \sum_{i=1}^tc_i\bar{y}_{..} = 0\), as \(\sum_{i=1}^tc_i = 0\). Hence, the unique estimator of the contrast \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\) has the form

\[
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} = \sum_{i=1}^tc_i\bar{y}_{i.}\,.
\]
That is, we estimate the contrast in the treatment effects by the corresponding contrast in the treatment means.

The variance of this estimator is straightforward to obtain:

\begin{align*}
\mathrm{var}\left(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\right) 
& = \sum_{i=1}^tc_i^2\mathrm{var}(\bar{y}_{i.}) \\
& = \sigma^2\sum_{i=1}^tc_i^2/n_i\,,
\end{align*}

as, under our model assumptions, each \(\bar{y}_{i.}\) is an average of independent observations with variance \(\sigma^2\). Similarly, from model \eqref{eq:utm} we can derive the distribution of \(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\) as

\[
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} \sim N\left(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}, \sigma^2\sum_{i=1}^tc_i^2/n_i\right)\,.
\]
Confidence intervals and hypothesis tests for \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\) can be constructed/conducted using this distribution, e.g.

\begin{itemize}
\tightlist
\item
  a \(100(1-\frac{\alpha}{2})\)\% confidence interval:
  \[
   \boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau} \in \sum_{i=1}^tc_i\bar{y}_{i.} \pm t_{n-t, 1-\frac{\alpha}{2}}s\sqrt{\sum_{i=1}^tc_i^2/n_i}\,,
   \]
\end{itemize}

where \(t_{n-t, 1-\frac{\alpha}{2}}\) is the \(1-\frac{\alpha}{2}\) quantile of a \(t\)-distribution with \(n-t\) degrees of freedom and

\begin{equation} 
s^2 = \frac{1}{n-t}\sum_{i=1}^t\sum_{j=1}^{n_i}(y_{ij} - \bar{y}_{i.})^2
\label{eq:crd-s2}
\end{equation}

is the estimate of \(\sigma^2\).

\begin{itemize}
\tightlist
\item
  the hypothesis \(H_0: \boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau} = 0\) against the two-sided alternative \(H_1: \boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau} \ne 0\) is rejected using a test of with confidence level \(1-\alpha/2\) if
\end{itemize}

\[
 \frac{|\sum_{i=1}^tc_i\bar{y}_{i.}|}{s\sqrt{\sum_{i=1}^tc_i^2/n_i}} > t_{n-t, 1-\frac{\alpha}{2}}\,.
 \]

\hypertarget{r-crd}{%
\section{Analysing CRDs in R}\label{r-crd}}

Let's return to Example \ref{exm:one-way}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(pulp, }\AttributeTok{names\_from =}\NormalTok{ operator, }\AttributeTok{values\_from =}\NormalTok{ reflectance)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{],}
 \AttributeTok{col.names =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Operator"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{),}
 \AttributeTok{caption =} \StringTok{"Pulp experiment: reflectance values (unitless) from four different operators."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:one-way-analysis}Pulp experiment: reflectance values (unitless) from four different operators.}
\centering
\begin{tabular}[t]{r|r|r|r}
\hline
Operator 1 & Operator 2 & Operator 3 & Operator 4\\
\hline
59.8 & 59.8 & 60.7 & 61.0\\
\hline
60.0 & 60.2 & 60.7 & 60.8\\
\hline
60.8 & 60.4 & 60.5 & 60.6\\
\hline
60.8 & 59.9 & 60.9 & 60.5\\
\hline
59.8 & 60.0 & 60.3 & 60.5\\
\hline
\end{tabular}
\end{table}

Clearly, we could directly calculate, and then compare, mean responses for each operator. However, there are (at least) two other ways we can proceed which use the fact we are fitting a linear model. These will be useful when we consider more complex models.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using \texttt{pairwise.t.test}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{with}\NormalTok{(pulp, }
 \FunctionTok{pairwise.t.test}\NormalTok{(reflectance, operator, }\AttributeTok{p.adjust.method =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  reflectance and operator 
## 
##   1     2     3    
## 2 0.396 -     -    
## 3 0.084 0.015 -    
## 4 0.049 0.008 0.775
## 
## P value adjustment method: none
\end{verbatim}

  This function performs hypothesis tests for all pairwise treatment comparisons (with a default confidence level of 0.95). Here we can see that operators 1 and 4, 2 and 3, and 2 and 4 have statistically significant differences.
\item
  Using \texttt{lm} and the \texttt{emmeans} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pulp.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(reflectance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ operator, }\AttributeTok{data =}\NormalTok{ pulp)}
\FunctionTok{anova}\NormalTok{(pulp.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: reflectance
##           Df Sum Sq Mean Sq F value Pr(>F)  
## operator   3   1.34   0.447     4.2  0.023 *
## Residuals 16   1.70   0.106                 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pulp.emm }\OtherTok{\textless{}{-}}\NormalTok{ emmeans}\SpecialCharTok{::}\FunctionTok{emmeans}\NormalTok{(pulp.lm, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ operator)}
\FunctionTok{pairs}\NormalTok{(pulp.emm, }\AttributeTok{adjust =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate    SE df t.ratio p.value
##  1 - 2        0.18 0.206 16   0.873  0.3960
##  1 - 3       -0.38 0.206 16  -1.843  0.0840
##  1 - 4       -0.44 0.206 16  -2.134  0.0490
##  2 - 3       -0.56 0.206 16  -2.716  0.0150
##  2 - 4       -0.62 0.206 16  -3.007  0.0080
##  3 - 4       -0.06 0.206 16  -0.291  0.7750
\end{verbatim}

  Here, we have first fitted the linear model object. The \texttt{lm} function, by default, will have set up dummy variables with the first treatment (operator) as a baseline (see MATH2010 or STAT6123). We then take the intermediate step of calculating the ANOVA table for this experiment, and use an F-test to compare the model accounting for operator differences to the null model; there are differences between operators at the 5\% significance level,

  The choice of dummy variables in the linear model is unimportant; any set could be used\footnote{Recall that although \(\mu\) and \(\boldsymbol{\tau}\) are not uniquely estimable, fitted values \(\hat{y}_i = \hat{\mu} + \hat{\tau}_i\) \textbf{are}, and hence it does not matter which dummy variables we use in \texttt{lm}.}, as in the next line we use the \texttt{emmeans} function (from the package of the same name) to specify that we are interested in estimating contrasts in the factor \texttt{operator} (which specifies our treatments in this experiment). Finally, the \texttt{pairs} command performs hypothesis tests for all pairwise comparisons between the four treatments. The results are the same as those obtained from using \texttt{pairwise.t.test}.
\end{enumerate}

Our preferred approach is using method 2 (\texttt{lm} and \texttt{emmeans}), for four main reasons:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The function \texttt{contrasts} in the \texttt{emmeans} package can be used to estimate arbitrary treatment contrasts (see \texttt{help("contrast-methods")}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# same as \textasciigrave{}pairs\textasciigrave{} above}
\NormalTok{emmeans}\SpecialCharTok{::}\FunctionTok{contrast}\NormalTok{(pulp.emm, }\StringTok{"pairwise"}\NormalTok{, }\AttributeTok{adjust =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate    SE df t.ratio p.value
##  1 - 2        0.18 0.206 16   0.873  0.3960
##  1 - 3       -0.38 0.206 16  -1.843  0.0840
##  1 - 4       -0.44 0.206 16  -2.134  0.0490
##  2 - 3       -0.56 0.206 16  -2.716  0.0150
##  2 - 4       -0.62 0.206 16  -3.007  0.0080
##  3 - 4       -0.06 0.206 16  -0.291  0.7750
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimating single contrast c = (1, {-}.5, {-}.5)}
\CommentTok{\# comparing operator 1 with operators 2 and 3}
\NormalTok{contrast1v23.emmc }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(levs) }
  \FunctionTok{data.frame}\NormalTok{(}\StringTok{\textquotesingle{}t1 v avg t2 t3\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5}\NormalTok{, }\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{emmeans}\SpecialCharTok{::}\FunctionTok{contrast}\NormalTok{(pulp.emm, }\StringTok{\textquotesingle{}contrast1v23\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast       estimate    SE df t.ratio p.value
##  t1.v.avg.t2.t3     -0.1 0.178 16  -0.560  0.5830
\end{verbatim}
\item
  It more easily generalises to the more complicated models we will see in Chapter \ref{blocking}.
\item
  It explicitly acknowledges that we have fitted a linear model, and so encourages us to check the model assumptions (see \protect\hyperlink{nap-black-ex}{Exercise 3}).
\item
  It is straightfoward to apply adjustments for \protect\hyperlink{multiple-comp}{multiple comparisons}.
\end{enumerate}

\hypertarget{multiple-comp}{%
\section{Multiple comparisons}\label{multiple-comp}}

When we perform hypothesis testing, we choose the critical region (i.e.~the rule that decides if we reject \(H_0\)) to control the probability of a type I error; that is, we control the probability of incorrectly rejecting \(H_0\). If we need to test multiple hypotheses, e.g.~to test all pairwise differences, we need to consider the overall probability of incorrectly rejecting \textbf{one or more} null hypothesis. This is called the \textbf{experiment-wise} or \textbf{family-wise} error rate.

For Example \ref{exm:one-way}, there are \({4 \choose 2} = 6\) pairwise comparisons. Under the assumption that all tests are independent\footnote{They aren't, but it simplifies the maths!}, assuming each individual test has type I error 0.05, the experiment-wise type I error rate is given by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha)}\SpecialCharTok{\^{}}\DecValTok{6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.265
\end{verbatim}

An experiment-wise error rate of 0.265 is substantially greater than 0.05. Hence, we would expect to make many more type I errors than may be desirable. \href{https://xkcd.com/882}{xkcd} has a fun example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}} \FloatTok{0.05}
\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ alpha)}\SpecialCharTok{\^{}}\DecValTok{20}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.642
\end{verbatim}

Therefore it is usually desirable to maintain some control of the experiment-wise type I error rate. We will consider two methods.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The \textbf{Bonferroni method}. An upper bound on the experiment-wise type I error rate for testing \(k\) hypotheses can be shown to be

  \begin{align*}
  P(\mbox{wrongly reject at least one of } H_{0}^1, \ldots, H_{0}^k) = &    P\left(\bigcup_{i=1}^{k}\{\mbox{wrongly reject } H_{0}^i\}\right) \\
  & \leq \sum_{i=1}^{k}\underbrace{P(\mbox{wrongly reject } H_{0}^i)}_{\leq \alpha} \\ 
  & \leq k\alpha\,.
  \end{align*}

  Hence a \emph{conservative}\footnote{So the experiment-wise type I error will actually be less than the prescribed \(\alpha\)} adjustment for multiple comparisons is to test each hypothesis at size \(\alpha / k\), i.e.~for the CRD compare to the quantile \(t_{n-t, 1-\frac{\alpha}{2k}}\) (or multiply each individual p-value by \(k\)).

  For Example \ref{exm:one-way}, we can test all pairwise comparisons, each at size \(\alpha/k\) using the \texttt{adjustment} argument in \texttt{pairs}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(pulp.emm, }\AttributeTok{adjust =} \StringTok{\textquotesingle{}bonferroni\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate    SE df t.ratio p.value
##  1 - 2        0.18 0.206 16   0.873  1.0000
##  1 - 3       -0.38 0.206 16  -1.843  0.5030
##  1 - 4       -0.44 0.206 16  -2.134  0.2920
##  2 - 3       -0.56 0.206 16  -2.716  0.0920
##  2 - 4       -0.62 0.206 16  -3.007  0.0500
##  3 - 4       -0.06 0.206 16  -0.291  1.0000
## 
## P value adjustment: bonferroni method for 6 tests
\end{verbatim}

  Now, only one comparison is significant at an experiment-wise type I error rate of \(\alpha = 0.05\) (operators 2 and 4).
\item
  \textbf{Tukey's method}. An alternative approach that gives an exact experiment-wise error rate of \(100\alpha\)\% compares the \(t\) statistic to a critical value from the studentised range distribution\footnote{Given two independent samples \(u_1, \ldots, u_l\) and \(v_1,\ldots,v_m\) from the same distribution, the studentised range distribution is the distribution of \(\frac{R}{\sqrt{2}S}\), where \(R = u_{max}-u_{min}\) is the range of the first sample, and \(S = \sqrt{\frac{1}{m-1}\sum_{i=1}^m(v_i - \bar{v})^2}\) be the sample standard deviation of the second sample.}, given by \(\frac{1}{\sqrt{2}}q_{t, n-t, 1-\alpha}\) with \(q_{t, n-t, 1-\alpha}\) the \(1-\alpha\) quantile from the studentised range distribution (available in \texttt{R} as \texttt{qtukey}).

  For Example \ref{exm:one-way}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(pulp.emm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate    SE df t.ratio p.value
##  1 - 2        0.18 0.206 16   0.873  0.8190
##  1 - 3       -0.38 0.206 16  -1.843  0.2900
##  1 - 4       -0.44 0.206 16  -2.134  0.1840
##  2 - 3       -0.56 0.206 16  -2.716  0.0660
##  2 - 4       -0.62 0.206 16  -3.007  0.0380
##  3 - 4       -0.06 0.206 16  -0.291  0.9910
## 
## P value adjustment: tukey method for comparing a family of 4 estimates
\end{verbatim}
\end{enumerate}

The default adjustment in the \texttt{pairs} function is the Tukey method. Comparing the p-values for each comparison using unadjusted t-tests, the Boneferroni and Tukey methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pairs.u }\OtherTok{\textless{}{-}} \FunctionTok{pairs}\NormalTok{(pulp.emm, }\AttributeTok{adjust =} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\NormalTok{pairs.b }\OtherTok{\textless{}{-}} \FunctionTok{pairs}\NormalTok{(pulp.emm, }\AttributeTok{adjust =} \StringTok{\textquotesingle{}bonferroni\textquotesingle{}}\NormalTok{)}
\NormalTok{pairs.t }\OtherTok{\textless{}{-}} \FunctionTok{pairs}\NormalTok{(pulp.emm)}
\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{transform}\NormalTok{(pairs.b)[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{], }\AttributeTok{Bonf.p.value =} \FunctionTok{transform}\NormalTok{(pairs.b)[, }\DecValTok{6}\NormalTok{], }\AttributeTok{Tukey.p.value =} \FunctionTok{transform}\NormalTok{(pairs.t)[, }\DecValTok{6}\NormalTok{], }\AttributeTok{unadjust.p.value =} \FunctionTok{transform}\NormalTok{(pairs.u)[, }\DecValTok{6}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   contrast estimate    SE df t.ratio Bonf.p.value Tukey.p.value
## 1    1 - 2     0.18 0.206 16   0.873       1.0000        0.8185
## 2    1 - 3    -0.38 0.206 16  -1.843       0.5034        0.2903
## 3    1 - 4    -0.44 0.206 16  -2.134       0.2918        0.1845
## 4    2 - 3    -0.56 0.206 16  -2.716       0.0915        0.0658
## 5    2 - 4    -0.62 0.206 16  -3.007       0.0501        0.0377
## 6    3 - 4    -0.06 0.206 16  -0.291       1.0000        0.9911
##   unadjust.p.value
## 1          0.39551
## 2          0.08389
## 3          0.04864
## 4          0.01525
## 5          0.00835
## 6          0.77476
\end{verbatim}

Although the decision on which hypotheses to reject (comparson 2 - 4) is the same here for both methods, the p-values from the Bonferroni method are all larger, reflecting its more conservative nature.

\hypertarget{impact-of-design-choices-on-estimation}{%
\section{Impact of design choices on estimation}\label{impact-of-design-choices-on-estimation}}

Recall from Section \ref{contrast-crd} that the width of a confidence interval for a contrast \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\) is given by \(2t_{n-t, 1-\frac{\alpha}{2}}s\sqrt{\sum_{i=1}^tc_i^2/n_i}\). The expectation of the square of this quantity is given by

\[
4t^2_{n-t, 1-\frac{\alpha}{2}}\sigma^2\sum_{i=1}^tc_i^2/n_i\,,
\]
as \(E(s^2) = \sigma^2\). It is intuitive that a good design should have small values of the square root of this quantity (divided by \(2\sigma\)),

\[
t_{n-t, 1-\frac{\alpha}{2}}\sqrt{\sum_{i=1}^tc_i^2/n_i}\,,
\]
which can be achieved either by increasing \(n\), and hence reducing the size of the \(t\)-quantile, or for choice of the \(n_i\) for a fixed \(n\), i.e.~through choice of replication of each treatment.

\hypertarget{crd-opt-all}{%
\subsection{Optimal treatment allocation}\label{crd-opt-all}}

It is quite common that although the total number, \(n\), of runs in the experiment may be fixed, the number \(n_1, n_2, \ldots, n_t\) applied to the different treatments is under the experimenter's control. Choosing \(n_1, n_2\) subject to \(n_1+n_2 = n\) was the first \textbf{optimal design} problem we encountered in Chapter \ref{intro}.

Assume interest lies in estimating the set of \(p\) contrasts \(\boldsymbol{c}_1^{\mathrm{T}}\boldsymbol{\tau}, \ldots, \boldsymbol{c}_p^{\mathrm{T}}\boldsymbol{\tau}\), with \(\boldsymbol{c}_l^{\mathrm{T}} = (c_{l1}, \ldots, c_{lt})\). One useful measure of the overall quality of the estimators of these \(p\) contrasts is the average variance, given by

\[
\sigma^2\sum_{l=1}^p\sum_{i=1}^tc_{li}^2/n_i\,.
\]
So we will minimise this variance by allocating larger values of \(n_i\) to the treatments with correspondingly larger values of the contrast coefficients \(c_{li}\). Therefore an approach to optimal allocation is to choose \(\boldsymbol{n} = (n_1, \ldots, n_t)^{\mathrm{T}}\) so as to

\begin{equation}
\mbox{minimise} \quad \phi(\boldsymbol{n}) = \sum_{l=1}^p\sum_{i=1}^tc_{li}^2/n_i\,\qquad \mbox{subject to} \quad \sum_{i=1}^tn_i = n\,.
\label{eq:opt-all}
\end{equation}

This is a discrete optimisation problem (the \(n_i\) are integers). It is usually easier to solve the relaxed problem, where we allow continuous \(0\le n_i \le n\), and round the resulting solution to obtain integers. There is no guarantee that such a rounded allocation will actually be the optimal integer-valued solution, but it is usually fairly close.

To solve the continuous version of \eqref{eq:opt-all} we will use the method of Lagrange mutliplers, where we define the function

\[
h(\boldsymbol{n}, \lambda) = \phi(\boldsymbol{n}) + \lambda\left(\sum_{i=1}^tn_i - n\right)\,,
\]
introducing the new scalar variable \(\lambda\), and solve the set of \(t+1\) equations:

\begin{align*}
\frac{\partial h}{\partial n_1} & = 0 \\
\vdots & \\
\frac{\partial h}{\partial n_t} & = 0 \\
\frac{\partial h}{\partial \lambda} & = 0\,.
\end{align*}

In this case, we have

\begin{equation}
\frac{\partial h}{\partial n_i} = -\sum_{l=1}^pc_{li}^2/n_i^2 + \lambda = 0\,,\quad i=1,\ldots t,
\label{eq:lm-ni}
\end{equation}
and
\[
\frac{\partial h}{\partial \lambda} = \sum_{i=1}^t n_i - n = 0\,.
\]
This last equation ensures \(\sum_{i=1}^tn_i = n\). From the \(t\) equations described by \eqref{eq:lm-ni}, we get

\[
n_i \propto \sqrt{\sum_{l=1}^pc_{li}^2}
\]
We don't need to explicitly solve for \(\lambda\) to find the normalising constant for each \(n_i\). As we know \(\sum_{i=1}^tn_i = n\), we obtain,

\begin{equation}
n_i = \frac{\sqrt{\sum_{l=1}^pc_{li}^2}}{\sum_{i=1}^t\sqrt{\sum_{l=1}^pc_{li}^2}}n\,.
\label{eq:opt-ni}
\end{equation}

Let's return to Example \ref{exm:one-way} and calculate the optimal allocations under two different sets of contrasts. First, we define an \texttt{R} function for calculating \eqref{eq:opt-ni}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opt\_ni }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(C, n) \{}
\NormalTok{  CtC }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(C) }\SpecialCharTok{\%*\%}\NormalTok{ C}
\NormalTok{  n }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(CtC)) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(CtC)))}
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}

Checking that the function \texttt{opt-ni} matches \eqref{eq:opt-ni} is left as an exercise.

Consider two sets of contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  All pairwise comparisons between the four treatments
  \begin{align*}
  c_1 & = (-1, 1, 0, 0) \\
  c_2 & = (-1, 0, 1, 0) \\
  c_3 & = (-1, 0, 0, 1) \\
  c_4 & = (0, -1, 1, 0) \\
  c_5 & = (0, -1, 0, 1) \\
  c_6 & = (0, 0, -1, 1)\,.
  \end{align*}

  Calculating \eqref{eq:opt-ni}, we obtain

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}
\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{nrow =} \DecValTok{6}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T}
\NormalTok{)}
\FunctionTok{opt\_ni}\NormalTok{(C, }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5 5 5 5
\end{verbatim}

  Hence confirming that equal replication of the treatments is optimal for minimising the average variance of estimators of the pairwise treatment differences.
\item
  If operator 4 is new to the mill, it may be desired to test their output to the average output from the other three operators, using a contrast with coefficients \(c = (1/3, 1/3, 1/3, -1)\). The allocation to minimise the variance of the corresponding estimator is given by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{3}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
  \AttributeTok{nrow =} \DecValTok{1}
\NormalTok{)}
\FunctionTok{opt\_ni}\NormalTok{(C, }\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  3.33  3.33  3.33 10.00
\end{verbatim}

  So the optimal allocation splits 10 units between operators 1-3, and allocates 10 units to operator 4. There is no exact integer rounding possible, so we will use \(n_1 = 4\), \(n_2=n_3 = 3\), \(n_4 = 10\) and calculate the efficiency by comparing the variance of this allocation to that from the equally allocated design.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{crd\_var }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(C, n) \{}
\NormalTok{  CtC }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(C) }\SpecialCharTok{\%*\%}\NormalTok{ C}
  \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(CtC) }\SpecialCharTok{/}\NormalTok{ n)}
\NormalTok{\} }
\NormalTok{n\_equal }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{n\_opt }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\FunctionTok{crd\_var}\NormalTok{(C, n\_opt) }\SpecialCharTok{/} \FunctionTok{crd\_var}\NormalTok{(C, n\_equal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.757
\end{verbatim}

  So the efficiency of the equally allocated design for estimating this contrast is 75.69 \%.
\end{enumerate}

\hypertarget{crd-size}{%
\subsection{Overall size of the experiment}\label{crd-size}}

We can also consider the complementary question: suppose the proportion of runs that is to be allocated to each treatment has been fixed in advance, what size of experiment should be performed to meet the objectives? That is, given a fixed proportion, \(w_i\), of resource to be allocated to the \(i\)th treatment, so that \(n_i = nw_i\) units will be allocated to that treatment, what value of \(n\) should be chosen?

One way of thinking about this question is to consider the ratio

\begin{align*}
\frac{|\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}|}{\sqrt{\mbox{Var}(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}})}} & = \frac{|\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}|}{\sqrt{\frac{\sigma^2}{n} \sum_{i=1}^tc_i^2/w_i}} \\
& = \sqrt{n}\frac{|\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}| / \sigma}{\sqrt{\sum_{i=1}^tc_i^2/w_i}}\,,
\end{align*}

which is analogous to the test statistic for \(H_0: \boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau} = 0\). For a given value of the signal-to-noise ratio \(d = |\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}| / \sigma\), we can choose \(n\) to result in a specified value of \(T = |\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}| / \sqrt{\mbox{Var}(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}})}\):

\[
n = T^2\frac{\sum_{i=1}^t c_i^2/w_i}{d^2}\,.
\]
Returning to Example \ref{exm:one-way}, assume are testing a single pairwise comparison and that we require \(T = 3\), so that the null hypothesis would be comfortably rejected at the 5\% level (cf 1.96 for a standard z-test). For equal allocation of the units to each treatment (\(w_1 = \cdots = w_4 = 1/4\)) and a variety of different values of the signal-to-noise ratio \(d\), we obtained the following optimal experiment sizes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opt\_n }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(cv, prop, snr, target) target }\SpecialCharTok{\^{}} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{c}\NormalTok{(}\FunctionTok{t}\NormalTok{(cv) }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{( }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ prop) }\SpecialCharTok{\%*\%}\NormalTok{ cv) }\SpecialCharTok{/}\NormalTok{ snr }\SpecialCharTok{\^{}} \DecValTok{2}
\NormalTok{cv }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{w }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{snr }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\FunctionTok{cbind}\NormalTok{(}\StringTok{\textquotesingle{}Signal{-}to{-}noise\textquotesingle{}} \OtherTok{=}\NormalTok{ snr, }\StringTok{\textquotesingle{}n\textquotesingle{}} \OtherTok{=} \FunctionTok{opt\_n}\NormalTok{(cv, w, snr, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Signal-to-noise     n
## [1,]             0.5 288.0
## [2,]             1.0  72.0
## [3,]             1.5  32.0
## [4,]             2.0  18.0
## [5,]             2.5  11.5
## [6,]             3.0   8.0
\end{verbatim}

So, for example, to achieve \(T = 3\) with a signal-to-noise ratio of \(d=0.5\) requires \(n=288\) runs. As would be expected, the number of runs required to achieve this value of \(T\) decreases as the signal-to-noise ration increases. For \(d=3\), only a very small experiment with \(n=8\) runs is needed.

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    For Example \ref{exm:one-way}, calculate the mean response for each operator and show that the treatment differences and results from hypothesis tests using the results in Section \ref{contrast-crd} are the same as those found in Section \ref{r-crd} using \texttt{pairwise.t.test}, and \texttt{emmeans}.
  \item
    Also check the results in Section \ref{multiple-comp} by (i) adjusting individual p-values (for Bonferroni) and (ii) using the \texttt{qtukey} command.
  \end{enumerate}
\end{enumerate}

Solution

As a reminder, the data from the experiment is as follows.

\begin{tabular}{r|r|r|r}
\hline
Operator 1 & Operator 2 & Operator 3 & Operator 4\\
\hline
59.8 & 59.8 & 60.7 & 61.0\\
\hline
60.0 & 60.2 & 60.7 & 60.8\\
\hline
60.8 & 60.4 & 60.5 & 60.6\\
\hline
60.8 & 59.9 & 60.9 & 60.5\\
\hline
59.8 & 60.0 & 60.3 & 60.5\\
\hline
\end{tabular}

The mean response, and variance, from each treatment is given by

\begin{tabular}{l|r|r|r}
\hline
operator & n\_i & mean & variance\\
\hline
1 & 5 & 60.2 & 0.268\\
\hline
2 & 5 & 60.1 & 0.058\\
\hline
3 & 5 & 60.6 & 0.052\\
\hline
4 & 5 & 60.7 & 0.047\\
\hline
\end{tabular}

The sample variance, \(s^2 = 0.106\), from \eqref{eq:crd-s2}. As \(\sum_{i=1}^tc_i^2/n_i = \frac{2}{5}\) for contrast vectors \(\boldsymbol{c}\) corresponding to pairwise differences, the standard error of each pairwise difference is given by \(\sqrt{\frac{2s^2}{5}} = 0.206\). Hence, we can create a table of pairwise differences, standard errors and test statistics.

\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
contrast & estimate & SE & df & t.ratio & unadjust.p.value & Bonferroni & Tukey\\
\hline
1 - 2 & 0.18 & 0.206 & 16 & 0.873 & 0.396 & 1.000 & 0.819\\
\hline
1 - 3 & -0.38 & 0.206 & 16 & -1.843 & 0.084 & 0.503 & 0.290\\
\hline
1 - 4 & -0.44 & 0.206 & 16 & -2.134 & 0.049 & 0.292 & 0.184\\
\hline
2 - 3 & -0.56 & 0.206 & 16 & -2.716 & 0.015 & 0.092 & 0.066\\
\hline
2 - 4 & -0.62 & 0.206 & 16 & -3.007 & 0.008 & 0.050 & 0.038\\
\hline
3 - 4 & -0.06 & 0.206 & 16 & -0.291 & 0.775 & 1.000 & 0.991\\
\hline
\end{tabular}

Unadjusted p-values are obtained from the t-distribution, as twice the tail probabilities (\texttt{2\ *\ (1\ -\ pt(abs(t.ratio),\ 16))}). For Bonferroni, we simply multiply these p-values by \({t \choose 2} = 6\), and then take the minimum of this value and 1. For the Tukey method, we use \texttt{1\ -\ ptukey(abs(t.ratio)\ *\ sqrt(2),\ 4,\ 16)} (see \texttt{?ptukey}).

Alternatively, to test each hypothesis at the 5\% level, we can compare each t.ratio to (i) \texttt{qt(0.975,\ 16)\ =\ 2.12} (unadjusted); (ii) \texttt{qt(1\ -\ 0.025/6,\ 16)\ =\ 3.008} (Bonferroni); or (iii) \texttt{qtukey(0.95,\ 4,\ 16)\ /\ sqrt(2)\ =\ 2.861}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  ~\citep[Adapted from][]{WH2009} The bioactivity of four different drugs \(A\), \(B\), \(C\) and \(D\) for treating a particular illness was compared in a study and the following ANOVA table was given for the data:

  \begin{longtable}[]{@{}lccc@{}}
  \toprule
  Source & Degrees of freedom & Sums of squares & Mean square \\
  \midrule
  \endhead
  Treatment & 3 & 64.42 & 21.47 \\
  Residual & 26 & 62.12 & 2.39 \\
  Total & 29 & 126.54 & \\
  \bottomrule
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \item
    What considerations should be made when assigning drugs to patients, and why?
  \item
    Use an \(F\)-test to test at the 0.01 level the null hypothesis that the four drugs have the same bioactivity.
  \item
    The average response from each treatment is as follows: \(\bar{y}_{A.}=66.10\) (\(n_A=7\) patients), \(\bar{y}_{B.}=65.75\) (\(n_B=8\)), \(\bar{y}_{C.} = 62.63\) (\(n_C=9\)), and \(\bar{y}_{D.}=63.85\) (\(n_D=6\)). Conduct hypothesis tests for all pairwise comparisons using the Bonferroni and Tukey methods for an experiment-wise error rate of 0.05.
  \item
    In fact, \(A\) and \(B\) are brand-name drugs and \(C\) and \(D\) are generic drugs. Test the null hypothesis at the 5\% level that brand-name and generic drugs have the same bioactivity.
  \end{enumerate}
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  Each patient should be randomly allocated to one of the drugs. This is to protect against possible bias from lurking variables, e.g.~demographic variables or subjective bias from the study administrator (blinding the study can also help to protect against this).
\item
  Test statistic = (Treatment mean square)/(Residual mean square) = 21.47/2.39 = 8.98. Under \(H_0\): no difference in bioactivity between the drugs, the test statistic follows an \(F_{3,26}\) distribution, which has a 1\% critical value of \texttt{qf(0.99,\ 3,\ 26)\ =\ 4.637}. Hence, we can reject \(H_0\).
\item
  For each difference, the test statistic has the form

  \[
   \frac{|\bar{y}_{i.}-\bar{y}_{j.}|}{s\sqrt{\frac{1}{n_i}+\frac{1}{n_j}}}\,,
   \]

  for \(i, j = A, B, C, D;\, i\ne j\). The treatment means and repetitions are given in the question (note that not all \(n_i\) are equal). From the ANOVA table, we get \(s^2 = 62.12/26 = 2.389\). The following table summarises the differences between drugs:

  \begin{longtable}[]{@{}lllllll@{}}
  \toprule
  & \(A-B\) & \(A-C\) & \(A-D\) & \(B-C\) & \(B-D\) & \(C-D\) \\
  \midrule
  \endhead
  Abs. difference & 0.35 & 3.47 & 2.25 & 3.12 & 1.9 & 1.22 \\
  Test statistic & 0.44 & 4.45 & 2.62 & 4.15 & 2.28 & 1.50 \\
  \bottomrule
  \end{longtable}

  The Bonferroni critical value is \(t_{26, 1-0.05/12} = 3.507\). The Tukey critical value is \(q_{4,26, 0.95}/\sqrt{2} = 2.743\) (available \texttt{R} as \texttt{qtukey(0.95,\ 4,\ 26)\ /\ sqrt(2)}). Hence under both methods, bioactivity of drugs \(A\) and \(C\), and \(B\) and \(C\), are significantly different.
\item
  A suitable contrast has \(\boldsymbol{c} = (0.5, 0.5, -0.5, -0.5)\), with \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau} = (\tau_A + \tau_B) / 2 - (\tau_C + \tau_D) / 2\) (the difference in average treatment effects).

  An estimate for this contrast is given by \((\bar{y}_{A.} + \bar{y}_{B.}) / 2 - (\bar{y}_{C.} + \bar{y}_{D.}) / 2\), with variance

  \[\mbox{Var}\left(\frac{1}{2}(\bar{y}_{A.}+\bar{y}_{B.}) - \frac{1}{2}(\bar{y}_{C.}+\bar{Y}_{D.})\right) = \frac{\sigma^2}{4}\left(\frac{1}{n_A} + \frac{1}{n_B} + \frac{1}{n_C} + \frac{1}{n_D}\right)\,.\]

  Hence, a test statistic for \(H_0:\, \frac{1}{2}(\tau_A+\tau_B) - \frac{1}{2}(\tau_C+\tau_D)=0\) is given by

  \[
  \frac{\frac{1}{2}(\bar{y}_{A.}+\bar{y}_{B.}) - \frac{1}{2}(\bar{y}_{C.}+\bar{y}_{D.})}{\sqrt{\frac{s^2}{4}\left(\frac{1}{n_A} + \frac{1}{n_B} + \frac{1}{n_C} + \frac{1}{n_D}\right)}} = \frac{2.685}{\frac{\sqrt{2.389}}{2}\sqrt{\frac{1}{7} + \frac{1}{8} + \frac{1}{9} + \frac{1}{6}}} = 4.70\,.
   \]

  The critical value is \(t_{26, 1-0.05/2} = 2.056\). Hence, we can reject \(H_0\) and conclude there is a difference between brand-name and generic drugs.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  The below table gives data from a completely randomised design to compare six different batches of hydrochloric acid on the yield of a dye (naphthalene black 12B).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{napblack }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{batch =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)),}
               \AttributeTok{repetition =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }
               \AttributeTok{yield =} \FunctionTok{c}\NormalTok{(}\DecValTok{145}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{140}\NormalTok{, }\DecValTok{155}\NormalTok{, }\DecValTok{90}\NormalTok{, }\DecValTok{160}\NormalTok{, }\DecValTok{95}\NormalTok{,}
                               \DecValTok{195}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{205}\NormalTok{, }\DecValTok{110}\NormalTok{, }\DecValTok{160}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{195}\NormalTok{, }\DecValTok{65}\NormalTok{, }\DecValTok{145}\NormalTok{,}
                               \DecValTok{195}\NormalTok{, }\DecValTok{230}\NormalTok{, }\DecValTok{115}\NormalTok{, }\DecValTok{235}\NormalTok{, }\DecValTok{225}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{55}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{45}\NormalTok{)}
\NormalTok{                 )}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
\NormalTok{tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(napblack, }\AttributeTok{names\_from =}\NormalTok{ batch, }\AttributeTok{values\_from =}\NormalTok{ yield)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{],}
 \AttributeTok{col.names =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Batch"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{),}
 \AttributeTok{caption =} \StringTok{"Naphthalene black experiment: yields (grams of standard colour) from six different batches of hydrochloric acid."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{table}

   \caption{\label{tab:nap-black}Naphthalene black experiment: yields (grams of standard colour) from six different batches of hydrochloric acid.}
   \centering
   \begin{tabular}[t]{r|r|r|r|r|r}
   \hline
   Batch 1 & Batch 2 & Batch 3 & Batch 4 & Batch 5 & Batch 6\\
   \hline
   145 & 140 & 195 & 45 & 195 & 120\\
   \hline
   40 & 155 & 150 & 40 & 230 & 55\\
   \hline
   40 & 90 & 205 & 195 & 115 & 50\\
   \hline
   120 & 160 & 110 & 65 & 235 & 80\\
   \hline
   180 & 95 & 160 & 145 & 225 & 45\\
   \hline
   \end{tabular}
   \end{table}

  Conduct a full analysis of this experiment, including

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    exploratory data analysis;
  \item
    fitting a linear model, and conducting an F-test to compare to a model that explains variation using the six batches to the null model;
  \item
    usual linear model diagnostics;
  \item
    multiple comparisons of all pairwise differences between treatments.
  \end{enumerate}
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Two of the simplest ways of examining the data are to calculate basic descriptive statistics, e.g.~the mean and standard deviation of the yield in each batch, and to plot the data in the different batches using a simple graphical display, e.g.~a stripchart of the yields in each batch. Notice that in both \texttt{aggregate} and \texttt{stripchart} we use the formula \texttt{yield $\sim$ batch}. This formula splits the data into groups defined by batch.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{aggregate}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ batch, }\AttributeTok{data =}\NormalTok{ napblack, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{c}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(x), }
                                                          \AttributeTok{st.dev =} \FunctionTok{sd}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   batch yield.mean yield.st.dev
## 1     1      105.0         63.0
## 2     2      128.0         33.3
## 3     3      164.0         38.0
## 4     4       98.0         68.7
## 5     5      200.0         50.0
## 6     6       70.0         31.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ batch, }\AttributeTok{data =}\NormalTok{ napblack)}
\end{Highlighting}
\end{Shaded}

  \begin{figure}

   {\centering \includegraphics{bookdown_math3014-6027_files/figure-latex/napblack-summary-1} 

   }

   \caption{Naphthalene black experiment: distributions of dye yields from the six batches.}\label{fig:napblack-summary}
   \end{figure}

  Notice that even within any particular batch, the number of grams of standard dyestuff colour determined by the dye trial varies from observation to observation. This \emph{within-group} variation is considered to be random or residual variation. This cannot be explained by any differences between batches. However, a second source of variation in the overall data set can be explained by variation between the batches, i.e.~between the different batch means themselves. We can see from the box plots (Figure \ref{fig:napblack-summary}) and the mean yields in each batch that observations from batch number five appear to have given higher yields (in grams of standard colour) than those from the other batches.
\item
  When we fit linear models and compare them using analysis of variance (ANOVA), it enables us to decide whether the differences that seem to be evident in these simple plots and descriptive statistics are statistically significant or whether this kind of variation could have arisen by chance, even though there are no real differences between the batches.

  An ANOVA table may be used to compare a linear model including differences between the batches to the null model. The linear model we will fit is a simple unit-treatment model:

  \begin{equation}
   Y_{ij} =  \mu +  \tau_i +  \varepsilon_{ij} \,,\qquad i=1,\ldots,6;~j=1,\ldots,5\,,
   \label{eq:linmod}
   \end{equation}

  where \(Y_{ij}\) is the response obtained from the \(j\)th repetition of the \(i\)th batch, \(\mu\) is a constant term, \(\tau_i\) is the expected effect due to the observation being in the \(k\)th batch \((k=1,\ldots,5)\) and \(\varepsilon_{ij}\) are the random errors.

  A test of the hypothesis that the group means are all equal is equivalent to a test that the \(\tau_i\) are all equal to 0 \((H_0:\, \tau_1 = \tau_2 = \cdots = \tau_6 = 0)\). We can use \texttt{lm} to fit model \eqref{eq:linmod}, and \texttt{anova} to test the hypothesis. Before we fit the linear model, we need to make sure \texttt{batch} has type \texttt{factor}\footnote{Factors are variables in \texttt{R} which take on a limited number of different values (e.g.~categorical variables). We need to define a categorical variable, like \texttt{batch} as a \texttt{factor} to ensure they are treated correctly by functions such as \texttt{lm}.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{napblack}\SpecialCharTok{$}\NormalTok{batch }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(napblack}\SpecialCharTok{$}\NormalTok{batch)}
\NormalTok{napblack.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(yield }\SpecialCharTok{\textasciitilde{}}\NormalTok{ batch, }\AttributeTok{data =}\NormalTok{ napblack)}
\FunctionTok{anova}\NormalTok{(napblack.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##           Df Sum Sq Mean Sq F value Pr(>F)   
## batch      5  56358   11272     4.6 0.0044 **
## Residuals 24  58830    2451                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

  The p-value of 0.004 indicates significant differences between at least two of the batch
  means. Therefore \(H_0\) is rejected and a suitable multiple comparison test should be carried
  out.
\item
  To perform our analysis, we have fitted a linear model. Therefore, we should use some plots of the residuals \(y_{ij} - \hat{y}_{ij}\) to check the model assumptions, particularly that the errors are independently and identically normally distributed. The function \texttt{rstandard} which produces residuals which have been standardised to have variance equal to 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{standres }\OtherTok{\textless{}{-}} \FunctionTok{rstandard}\NormalTok{(napblack.lm)}
\NormalTok{fitted }\OtherTok{\textless{}{-}} \FunctionTok{fitted}\NormalTok{(napblack.lm)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{pty =} \StringTok{"s"}\NormalTok{)}
\FunctionTok{with}\NormalTok{(napblack, \{}
  \FunctionTok{plot}\NormalTok{(batch, standres, }\AttributeTok{xlab =} \StringTok{"Batch"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Standarised residuals"}\NormalTok{)}
  \FunctionTok{plot}\NormalTok{(fitted, standres, }\AttributeTok{xlab =} \StringTok{"Fitted value"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Standarised residuals"}\NormalTok{)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

  \begin{figure}

   {\centering \includegraphics[width=1\linewidth]{bookdown_math3014-6027_files/figure-latex/residuals-1} 

   }

   \caption{Residuals against batch (left) and fitted values (right) for the linear model fit to the naphthalene black data.}\label{fig:residuals}
   \end{figure}

  The plots (Figure \ref{fig:residuals}) show no large standardised residuals (\(>2\) in absolute value\footnote{We would anticipate 95\% of the standardised residuals to lie in {[}-1.96, 1.96{]}, as they will follow a standard normal distribution if the model assumptions are correct.}). While there is some evidence of unequal variation across batches, there is no obvious pattern with respect to fitted values (e.g.~no ``funnelling'').

  We can also plot the standardised residuals against the quantiles of a standard normal distribution to assess the assumption of normality.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{pty =} \StringTok{"s"}\NormalTok{)}
\FunctionTok{qqnorm}\NormalTok{(standres, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{figure}

   {\centering \includegraphics[width=1\linewidth]{bookdown_math3014-6027_files/figure-latex/normalplot-1} 

   }

   \caption{Normal probability plot for the standardised residuals for the linear model fit to the naphthalene black data.}\label{fig:normalplot}
   \end{figure}

  The points lie quite well on a straight line (see Figure \ref{fig:normalplot}), suggesting the assumption of normality is valid. Overall, the residual plots look reasonable; some investigation of transformations to correct for non-constant variance could be investigated (see MATH2010/STAT6123).
\item
  When a significant difference between the treatments has been indicated, the next stage is to try to determine which treatments differ. In some cases a specific difference is of interest, a control versus a new treatment for instance, in which case that difference could now be
  inspected. However, usually no specific differences are to be considered a priori, and \textit{any}
  difference is of practical importance. A multiple comparison procedure is required to
  investigate all possible differences, which takes account of the number of possible differences
  available amongst the treatments (15 differences between the six batches here).

  We will use Tukey's method for controlling the experiment-wise type I error rate, fixed here at 5\%, as implemented by \texttt{emmeans}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{napblack.emm }\OtherTok{\textless{}{-}}\NormalTok{ emmeans}\SpecialCharTok{::}\FunctionTok{emmeans}\NormalTok{(napblack.lm, }\StringTok{\textquotesingle{}batch\textquotesingle{}}\NormalTok{)}
\FunctionTok{pairs}\NormalTok{(napblack.emm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate   SE df t.ratio p.value
##  1 - 2         -23 31.3 24  -0.730  0.9760
##  1 - 3         -59 31.3 24  -1.880  0.4350
##  1 - 4           7 31.3 24   0.220  1.0000
##  1 - 5         -95 31.3 24  -3.030  0.0570
##  1 - 6          35 31.3 24   1.120  0.8690
##  2 - 3         -36 31.3 24  -1.150  0.8560
##  2 - 4          30 31.3 24   0.960  0.9270
##  2 - 5         -72 31.3 24  -2.300  0.2330
##  2 - 6          58 31.3 24   1.850  0.4540
##  3 - 4          66 31.3 24   2.110  0.3170
##  3 - 5         -36 31.3 24  -1.150  0.8560
##  3 - 6          94 31.3 24   3.000  0.0610
##  4 - 5        -102 31.3 24  -3.260  0.0350
##  4 - 6          28 31.3 24   0.890  0.9440
##  5 - 6         130 31.3 24   4.150  0.0040
## 
## P value adjustment: tukey method for comparing a family of 6 estimates
\end{verbatim}

  We have two significant differences, between batches 4-5 and 5-6.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{subset}\NormalTok{(}\FunctionTok{transform}\NormalTok{(}\FunctionTok{pairs}\NormalTok{(napblack.emm)), p.value }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    contrast estimate   SE df t.ratio p.value
## 13    4 - 5     -102 31.3 24   -3.26 0.03482
## 15    5 - 6      130 31.3 24    4.15 0.00429
\end{verbatim}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \citep[Adapted from][]{Morris2011} Consider a completely randomised design with \(t = 5\) treatments and \(n=50\) units. The contrasts
\end{enumerate}

\[
\tau_2 - \tau_1, \quad \tau_3 - \tau_2, \quad \tau_4 - \tau_3, \tau_5 - \tau_4
\]

are of primary interest to the experimenter.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Find an allocation of the 50 units to the 5 treatments, i.e.~find \(n_1, \ldots, n_5\), that minimises the average variance of the corresponding contrast estimators.
\item
  Fixing the proportions of experimental effort applied to each treatment to those found in part (a), i.e.~to \(w_i = n_i/50\), find the value of \(n\) required to make the ratio \(T = |\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}|/\sqrt{\mbox{var}\left(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\right)} = 2\) assuming a signal-to-noise ratio of 1.
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  We can use the function \texttt{opt\_ni} given in Section \ref{crd-opt-all}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50}
\NormalTok{C }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}
  \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
  \DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
  \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
  \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}
\NormalTok{  ), }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T}
\NormalTok{)}
\FunctionTok{opt\_ni}\NormalTok{(C, n) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  8.01 11.33 11.33 11.33  8.01
\end{verbatim}

  Rounding, we obtain a solution of the form \(n_1 = n_5 =8\), \(n_2 = n_4 = 11\) and \(n_3 = 12\). Any of \(n_2, n_3, n_4\) may be rounded up to 12 to form a design with the same variance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nv }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\FunctionTok{crd\_var}\NormalTok{(C, nv }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\FunctionTok{crd\_var}\NormalTok{(C, nv }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\FunctionTok{crd\_var}\NormalTok{(C, nv }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.78
## [1] 0.78
## [1] 0.78
\end{verbatim}
\item
  The optimal ratios for each treatment from part (a) are \(w_1 = w_5 = 0.16\) and \(w_2 = w_3 = w_4 = 0.227\). Fixing these, we can use code from Section \ref{crd-size} to find the required value of \(n\) for each contrast.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nv }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{) nv[i] }\OtherTok{\textless{}{-}} \FunctionTok{opt\_n}\NormalTok{(C[i, ], }\FunctionTok{opt\_ni}\NormalTok{(C, n) }\SpecialCharTok{/}\NormalTok{ n, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{) }\CommentTok{\# snr = 1, target = 2}
\NormalTok{nv}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 42.6 35.3 35.3 42.6
\end{verbatim}

  Hence, we need \(n = 43\) for to achieve \(T = 2\) for the first and last contrasts, and \(n = 36\) for the second and third. The differences are due to the different proportions \(w_i\) assumed for each treatment. To achieve \(T=2\) for all contrasts, we pick the larger number, \(n = 43\).
\end{enumerate}

\hypertarget{blocking}{%
\chapter{Blocking}\label{blocking}}

The completely randomised design (CRD) works well when there is sufficient homogeneous experimental units to perform the whole experiment under the same, or very similar, conditions and there are no restrictions on the randomisation of treatments to units. The only systematic (non-random) differences in the observed responses result from differences between the treatments. While such designs are commonly and successfully used, especially in smaller experiments, their application can often be unrealistic or impractical in many settings.

A common way in which the CRD fails is a lack of sufficiently similar experimental units. If there are systemtic differences between different batches, or \textbf{blocks} of units, these differences should be taken into account in both the allocation of treatments to units and the modelling of the resultant data. Otherwise, block-to-block differences may bias treatment comparisons and/or inflate our estimate of the background variability and hence reduce our ability to detect important treatment effects.

\begin{example}
\protect\hypertarget{exm:blocks-bars}{}\label{exm:blocks-bars}

Steel bar experiment \citep[ch.~4]{Morris2011}

\citet{KSN2005} described an experiment to assess the strength of steel reinforcement bars from \(t=4\) coatings\footnote{Thr four coatings were all made from Engineering Thermoplastic Polyurethane (ETPU); coating one was solely made from ETPU, coatings 2-4 had additional glass fibre, carbon fibre or aramid fibre added, respectively.} (treatments). In total \(n=32\) different bars (units) were available, but the testing process meant sets of four bars were tested together. To account for potential test-specific features (e.g.~environmental or operational), these different test sets were assumed to form \(b=8\) blocks of size \(k=4\). The data are shown in Table \ref{tab:bar-expt-data} below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{coating =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\DecValTok{8}\NormalTok{),}
                   \AttributeTok{block =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{)), }
                   \AttributeTok{strength =} \FunctionTok{c}\NormalTok{(}\DecValTok{136}\NormalTok{, }\DecValTok{147}\NormalTok{, }\DecValTok{138}\NormalTok{, }\DecValTok{149}\NormalTok{, }\DecValTok{136}\NormalTok{, }\DecValTok{143}\NormalTok{, }\DecValTok{122}\NormalTok{, }\DecValTok{153}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{142}\NormalTok{, }\DecValTok{131}\NormalTok{, }\DecValTok{136}\NormalTok{,}
                                   \DecValTok{155}\NormalTok{, }\DecValTok{148}\NormalTok{, }\DecValTok{130}\NormalTok{, }\DecValTok{129}\NormalTok{, }\DecValTok{145}\NormalTok{, }\DecValTok{149}\NormalTok{, }\DecValTok{136}\NormalTok{, }\DecValTok{139}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{149}\NormalTok{, }\DecValTok{147}\NormalTok{, }\DecValTok{144}\NormalTok{,}
                                   \DecValTok{147}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{125}\NormalTok{, }\DecValTok{140}\NormalTok{, }\DecValTok{148}\NormalTok{, }\DecValTok{149}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{145}\NormalTok{)}
\NormalTok{                     )}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(bar, }\AttributeTok{names\_from =}\NormalTok{ coating, }\AttributeTok{values\_from =}\NormalTok{ strength),}
 \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Block"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\StringTok{"Coating"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
 \AttributeTok{caption =} \StringTok{"Steel bar experiment: tensile strength values (kliograms per square inch, ksi) from steel bars with four different coatings."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:bar-expt-data}Steel bar experiment: tensile strength values (kliograms per square inch, ksi) from steel bars with four different coatings.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
Block & Coating 1 & Coating 2 & Coating 3 & Coating 4\\
\hline
1 & 136 & 147 & 138 & 149\\
\hline
2 & 136 & 143 & 122 & 153\\
\hline
3 & 150 & 142 & 131 & 136\\
\hline
4 & 155 & 148 & 130 & 129\\
\hline
5 & 145 & 149 & 136 & 139\\
\hline
6 & 150 & 149 & 147 & 144\\
\hline
7 & 147 & 150 & 125 & 140\\
\hline
8 & 148 & 149 & 118 & 145\\
\hline
\end{tabular}
\end{table}

Here, each block has size 4, which is equal to the number of treatments in the experiment, and each treatment is applied in each block. This is an example of a \textbf{randomised complete block design}.

We can study the data graphically, plotting by treatment and by block.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(strength }\SpecialCharTok{\textasciitilde{}}\NormalTok{ block, }\AttributeTok{data =}\NormalTok{ bar)}
\FunctionTok{boxplot}\NormalTok{(strength }\SpecialCharTok{\textasciitilde{}}\NormalTok{ coating, }\AttributeTok{data =}\NormalTok{ bar)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_math3014-6027_files/figure-latex/bar-expt-boxplots-1} \includegraphics{bookdown_math3014-6027_files/figure-latex/bar-expt-boxplots-2} 

}

\caption{Steel bar experiment: distributions of tensile strength (ksi) from the eight blocks (top) and the four coatings (bottom).}\label{fig:bar-expt-boxplots}
\end{figure}

The box plots within each plot in Figure \ref{fig:bar-expt-boxplots} are comparable, as every treatment has occured with every block the same number of times (once). For example, when we compare the box plots for treatments 1 and 3, we know each of then display one observation from each block. Therefore, differences between treatments will not be influenced by large differences between blocks. This \textbf{balance} makes our analysis more straighforward. By eye, it appears here there may be differences between both coating 3 and the other three coatings.

\end{example}

\begin{example}
\protect\hypertarget{exm:blocks-tyres}{}\label{exm:blocks-tyres}

Tyre experiment \citep[ch.~3]{WH2009}

\citet{Davies1954}, p.200, examined the effect of \(t=4\) different rubber compounds (treatments) on the lifetime of a tyre. Each tyre is only large enough to split into \(k=3\) segments whilst still containing a representative amount of each compound. When tested, each tyre is subjected to the same road conditions, and hence is treated as a block. A design with \(b=4\) blocks was used, as displayed in Table \ref{tab:tyre-expt-data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tyre }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{compound =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)),}
                   \AttributeTok{block =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)), }
                   \AttributeTok{wear =} \FunctionTok{c}\NormalTok{(}\DecValTok{238}\NormalTok{, }\DecValTok{238}\NormalTok{, }\DecValTok{279}\NormalTok{, }\DecValTok{196}\NormalTok{, }\DecValTok{213}\NormalTok{, }\DecValTok{308}\NormalTok{, }\DecValTok{254}\NormalTok{, }\DecValTok{334}\NormalTok{, }\DecValTok{367}\NormalTok{, }\DecValTok{312}\NormalTok{, }\DecValTok{421}\NormalTok{, }\DecValTok{412}\NormalTok{)}
\NormalTok{                     )}
\FunctionTok{options}\NormalTok{(}\AttributeTok{knitr.kable.NA =} \StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(tyre, }\AttributeTok{names\_from =}\NormalTok{ compound, }\AttributeTok{values\_from =}\NormalTok{ wear),}
 \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Block"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\StringTok{"Compound"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)),}
 \AttributeTok{caption =} \StringTok{"Tyre experiment: relative wear measurements (unitless) from tires made with four different rubber compounds."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:tyre-expt-data}Tyre experiment: relative wear measurements (unitless) from tires made with four different rubber compounds.}
\centering
\begin{tabular}[t]{l|r|r|r|r}
\hline
Block & Compound 1 & Compound 2 & Compound 3 & Compound 4\\
\hline
1 & 238 & 238 & 279 & \\
\hline
2 & 196 & 213 &  & 308\\
\hline
3 & 254 &  & 334 & 367\\
\hline
4 &  & 312 & 421 & 412\\
\hline
\end{tabular}
\end{table}

Here, each block has size \(k=3\), which is smaller than the number of treatments (\(t=4\)). Hence, each block cannot contain an application of each treatment. This is an example of an \textbf{incomplete block design}.

Graphical exploration of the data is a little more problematic in this example. As each treatment does not occur in each block, box plots such as Figure \ref{fig:tyre-expt-boxplots} are not as informative. Do compounds three and four have higher average wear because they were the only compounds to both occur in blocks 3 and 4? Or do blocks 3 and 4 have a higher mean because they contain both compounds 3 and 4? The design cannot help us entirely disentangle the impact of blocks and treatments\footnote{This is our first example of (partial) confounding, which we will see again in Chapters \ref{block-factorial} and \ref{fractional-factorial}}. In our modelling, we will assume variation should first be described by blocks (which are generally fixed aspects of the experiment) and then treatments (which are more directly under the experimenter's control).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(wear }\SpecialCharTok{\textasciitilde{}}\NormalTok{ block, }\AttributeTok{data =}\NormalTok{ tyre)}
\FunctionTok{boxplot}\NormalTok{(wear }\SpecialCharTok{\textasciitilde{}}\NormalTok{ compound, }\AttributeTok{data =}\NormalTok{ tyre)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{bookdown_math3014-6027_files/figure-latex/tyre-expt-boxplots-1} \includegraphics{bookdown_math3014-6027_files/figure-latex/tyre-expt-boxplots-2} 

}

\caption{Tyre experiment: distributions of wear from the four blocks (top) and the four compounds (bottom).}\label{fig:tyre-expt-boxplots}
\end{figure}

\end{example}

\hypertarget{unit-block-treatment-model}{%
\section{Unit-block-treatment model}\label{unit-block-treatment-model}}

If \(n_{ij}\) is the number of times treatment \(j\) occurs in block \(i\), a common statistical model to describe data from a blocked experiment has the form

\begin{equation}
y_{ijl} = \mu + \beta_i + \tau_j + \varepsilon_{ijl}\,, \qquad i = 1,\ldots, b; j = 1, \ldots, t; l = 1,\ldots,n_{ij}\,,
\label{eq:block-model}
\end{equation}
where \(y_{ijl}\) is the response from the \(l\)th application of the \(j\)th treatment in the \(i\)th block, \(\mu\) is a constant parameter, \(\beta_i\) is the effect of the \(i\)th block, \(\tau_j\) is the effect of treatment \(j\), and \(\varepsilon_{ijl}\sim N(0, \sigma^2)\) are once again random individual effects from each experimental unit, assumed independent. The total number of runs in the experiment is given by \(n = \sum_{i=1}^b\sum_{j=1}^t n_{ij}\).

For Example \ref{exm:blocks-bars}, there are \(t=4\) experiments, \(b = 8\) blocks and each treatment occurs once in each block, so \(n_{ij} = 1\) for all \(i, j\). In Example \ref{exm:blocks-tyres}, there are again \(t=4\) treatments but now only \(b=4\) blocks and not every treatment occurs in every block. In fact, we have \(n_{11} = n_{12} = n_{13} = 1\), \(n_{14} = 0\), \(n_{21} = n_{22} =n_{24} = 1\), \(n_{23} = 0\), \(n_{31} = n_{33} =n_{34} = 1\), \(n_{32} = 0\), \(n_{41} = 0\) and \(n_{42} = n_{43} =n_{44} = 1\).

Writing model \eqref{eq:block-model} is matrix form as a partitioned linear model, we obtain

\begin{equation}
\boldsymbol{y}= \mu\boldsymbol{1}_n + X_1\boldsymbol{\beta} + X_2\boldsymbol{\tau} + \boldsymbol{\varepsilon}\,,
\label{eq:block-plm}
\end{equation}

with \(\boldsymbol{y}\) the \(n\)-vector of responses, \(X_1\) and \(X_2\) \(n\times b\) and \(n\times t\) model matrices for blocks and treatments, respectively, \(\boldsymbol{\beta} = (\beta_1,\ldots, \beta_b)^{\mathrm{T}}\), \(\boldsymbol{\tau} = (\tau_1,\ldots, \tau_t)^{\mathrm{T}}\) and \(\boldsymbol{\varepsilon}\) the \(n\)-vector of errors.

In equation\eqref{eq:block-plm}, assuming without loss of generality that runs of the experiment are ordered by block, the matrix \(X_1\) has the form

\[
X_1 = \bigoplus_{i = 1}^b \boldsymbol{1}_{k_i} = \begin{bmatrix}
\boldsymbol{1}_{k_1} & \boldsymbol{0}_{k_1} & \cdots &  \boldsymbol{0}_{k_1} \\
\boldsymbol{0}_{k_2} & \boldsymbol{1}_{k_2} & \cdots &  \boldsymbol{0}_{k_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b} & \boldsymbol{0}_{k_b} & \cdots &  \boldsymbol{1}_{k_b} \\
\end{bmatrix}\,,
\]
where \(k_i = \sum_{j=1}^t n_{ij}\), the number of units in the \(i\)th block. The structure of matrix \(X_2\) is harder to describe so distinctly, but each row includes a single non-zero entry, equal to one, indicating which treatment was applied in that run of the experiment. The first \(k_1\) rows correspond to block 1, the second \(k_2\) to block 2, and so on. We will see special cases later.

\hypertarget{normal-equations}{%
\section{Normal equations}\label{normal-equations}}

Writing as a partitioned model \(\boldsymbol{y}= W\boldsymbol{\alpha} + \boldsymbol{\varepsilon}\), with \(W = [\boldsymbol{1} | X_1 | X_2]\) and \(\boldsymbol{\alpha}^{\mathrm{T}} = [\mu | \boldsymbol{\beta}^{\mathrm{T}} | \boldsymbol{\tau}^{\mathrm{T}}]\), the least squares normal equations

\begin{equation}
W^{\mathrm{T}}W \hat{\boldsymbol{\alpha}} = W^{\mathrm{T}}\boldsymbol{y}
\label{eq:bne}
\end{equation}

can be written as a set of three matrix equations:

\begin{align}
n\hat{\mu} + \boldsymbol{1}_n^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + \boldsymbol{1}_n^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = \boldsymbol{1}_n^{\mathrm{T}}\boldsymbol{y}\,, \label{eq:blocks-normal-1}\\
X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_1^{\mathrm{T}}\boldsymbol{y}\,, \label{eq:blocks-normal-2}\\
X_2^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_2^{\mathrm{T}}\boldsymbol{y}\,. \label{eq:blocks-normal-3}\\
\end{align}

Above, the matrices \(X_1^{\mathrm{T}}X_1 = \mathrm{diag}(k_1,\ldots,k_b)\) and \(X_2^{\mathrm{T}}X_2 = \mathrm{diag}(n_1,\ldots,n_t)\) have simple forms as diagonal matrices with entries equal to the size of each block and the number of replications of each treatment, respectively.

The \(t\times b\) matrix \(N = X_2^{\mathcal{T}}X_1\) is particularly important in block designs, and is called the \textbf{incidence} matrix. Each of the \(i\)th row of \(N\) indicates in which blocks the \(i\)th treatment occurs.

We can eliminate the explicit dependence on \(\mu\) and \(\boldsymbol{\beta}\) to find reduced normal equations for \(\boldsymbol{\tau}\) by multiplying the middle equation by \(X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}\):

\begin{multline}
X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} \\
 = X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} \\
 = X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{y}\\
\end{multline}

and subtracting from the final equation:

\begin{multline}
X_2^{\mathrm{T}}\left(\boldsymbol{1}_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\right)\hat{\mu} + \left(X_2^{\mathrm{T}}X_1 - X_2^{\mathrm{T}}X_1\right)\boldsymbol{\beta} \\ + X_2^{\mathrm{T}}\left(I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\right)X_2\boldsymbol{\tau}\\
= X_2^{\mathrm{T}}\left(I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\right)\boldsymbol{y}\,.
\end{multline}
Clearly, a zero matrix is multiplying the block effects \(\boldsymbol{\beta}\). Also,

\[
X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n = \boldsymbol{1}_n\,,
\]
as

\[
X_1(X_1^{\mathrm{T}}X_1)^{-1} = \bigoplus_{i = 1}^b \frac{1}{k_i}\boldsymbol{1}_{k_i} = \begin{bmatrix}
\frac{1}{k_1}\boldsymbol{1}_{k_1} & \boldsymbol{0}_{k_1} & \cdots &  \boldsymbol{0}_{k_1} \\
\boldsymbol{0}_{k_2} & \frac{1}{k_2}\boldsymbol{1}_{k_2} & \cdots &  \boldsymbol{0}_{k_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b} & \boldsymbol{0}_{k_b} & \cdots &  \frac{1}{k_b}\boldsymbol{1}_{k_b} \\
\end{bmatrix}\,,
\]
and hence

\[
X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}} = \bigoplus_{i = 1}^b \frac{1}{k_i}J_{k_i} = \begin{bmatrix}
\frac{1}{k_1}J_{k_1} & \boldsymbol{0}_{k_1\times k_2} & \cdots &  \boldsymbol{0}_{k_1\times k_b} \\
\boldsymbol{0}_{k_2\times k_1} & \frac{1}{k_2}J_{k_2} & \cdots &  \boldsymbol{0}_{k_2\times k_b} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b\times k_1} & \boldsymbol{0}_{k_b\times k_2} & \cdots &  \frac{1}{k_b}J_{k_b} \\
\end{bmatrix}\,.
\]

Writing \(H_1 = X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\), we then get the reduced normal equations for \(\boldsymbol{\tau}\):

\begin{equation}
X_2^{\mathrm{T}}\left(I_n - H_1\right)X_2\boldsymbol{\tau}= X_2^{\mathrm{T}}\left(I_n - H_1\right)\boldsymbol{y}\,.
\label{eq:block-rne}
\end{equation}

We can demonstrate the form of these matrices through our two examples.

For Example \ref{exm:blocks-bars}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{X1 }\OtherTok{\textless{}{-}} \FunctionTok{kronecker}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{8}\NormalTok{), one)}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{)}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(}\StringTok{"rbind"}\NormalTok{, }\FunctionTok{replicate}\NormalTok{(}\DecValTok{8}\NormalTok{, X2, }\AttributeTok{simplify =} \ConstantTok{FALSE}\NormalTok{))}
\CommentTok{\#incidence matrix}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ X1}
\NormalTok{X1tX1 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X1) }\SpecialCharTok{\%*\%}\NormalTok{ X1 }\CommentTok{\# diagonal}
\NormalTok{X2tX2 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ X2 }\CommentTok{\# diagonal}
\NormalTok{H1 }\OtherTok{\textless{}{-}}\NormalTok{ X1 }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X1) }\SpecialCharTok{\%*\%}\NormalTok{ X1) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X1)}
\NormalTok{ones }\OtherTok{\textless{}{-}}\NormalTok{ H1 }\SpecialCharTok{\%*\%} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{32}\NormalTok{) }\CommentTok{\# H1 times vector of 1s is also a vector of 1s}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ X2 }\SpecialCharTok{{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ H1 }\SpecialCharTok{\%*\%}\NormalTok{ X2 }\CommentTok{\# X2t(I {-} H1)X2}
\FunctionTok{qr}\NormalTok{(A)}\SpecialCharTok{$}\NormalTok{rank }\CommentTok{\# rank 3}
\NormalTok{X2tH1 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ H1 }\CommentTok{\# adjustment to y}
\NormalTok{W }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(ones, X1, X2) }\CommentTok{\# overall model matrix}
\FunctionTok{qr}\NormalTok{(W)}\SpecialCharTok{$}\NormalTok{rank }\CommentTok{\# rank 11 (t+b {-} 1)}
\end{Highlighting}
\end{Shaded}

For Example \ref{exm:blocks-tyres}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{X1 }\OtherTok{\textless{}{-}} \FunctionTok{kronecker}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{), one)}
\NormalTok{X2 }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
        \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
          \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{12}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T}
\NormalTok{)}
\CommentTok{\#incidence matrix}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ X1}
\NormalTok{X1tX1 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X1) }\SpecialCharTok{\%*\%}\NormalTok{ X1 }\CommentTok{\# diagonal}
\NormalTok{X2tX2 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ X2 }\CommentTok{\# diagonal}
\NormalTok{H1 }\OtherTok{\textless{}{-}}\NormalTok{ X1 }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X1) }\SpecialCharTok{\%*\%}\NormalTok{ X1) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X1)}
\NormalTok{ones }\OtherTok{\textless{}{-}}\NormalTok{ H1 }\SpecialCharTok{\%*\%} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{) }\CommentTok{\# H1 times vector of 1s is also a vector of 1s}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ X2 }\SpecialCharTok{{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ H1 }\SpecialCharTok{\%*\%}\NormalTok{ X2 }\CommentTok{\# X2t(I {-} H1)X2}
\FunctionTok{qr}\NormalTok{(A)}\SpecialCharTok{$}\NormalTok{rank }\CommentTok{\# rank 3}
\NormalTok{X2tH1 }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(X2) }\SpecialCharTok{\%*\%}\NormalTok{ H1 }\CommentTok{\# adjustment to y}
\NormalTok{W }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(ones, X1, X2) }\CommentTok{\# overall model matrix}
\FunctionTok{qr}\NormalTok{(W)}\SpecialCharTok{$}\NormalTok{rank }\CommentTok{\# rank 7 (t+b {-} 1)}
\end{Highlighting}
\end{Shaded}

Notice that if we write \(X_{2|1} = (I_n - H_1)X_2\), then the reduced normal equations become

\[
X_{2|1}^{\mathrm{T}}X_{2|1}\boldsymbol{\tau} = X_{2|1}^{\mathrm{T}}\boldsymbol{y}\,,
\]
which have the same form as the CRD in Chapter \ref{crd} albeit with a different \(X_{2|1}\) matrix as we are adjusting for more complex nuisance parameters.

In general, the solution of these equations will depend on the exact form of the design. For the randomised complete block design, the solution turns out to be straighforward (see Section @ref(\#rcdb) below). By default, to fit model \eqref{eq:block-plm}, the \texttt{lm} function in \texttt{R} applies the constraint \(\tau_t = \beta_b = 0\), and removes the corresponding columns from \(X_1\) and \(X_2\), to leave a \(W\) matrix with full column rank. Clearly, this solution is not unique but, as with CRDs, we will identify uniquely estimatable combinations of the model parameters (and use \texttt{emmeans} to extract these estimates from an \texttt{lm} object).

\hypertarget{analysis-of-variance}{%
\section{Analysis of variance}\label{analysis-of-variance}}

As was the case with the CRD, it can be shown that any solution to the normal equations \eqref{eq:bne} will produce a unique solution to \(\widehat{W\alpha}\), and hence a unique analysis of variance decomposition can be obtained.

For a block experiment, the ANOVA table is comparing the full model \eqref{eq:block-plm}, the model containing the block effects

\begin{equation}
\boldsymbol{y}= \mu\boldsymbol{1} + X_1\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\label{eq:anova-bm}
\end{equation}

and the null model

\begin{equation}
\boldsymbol{y}= \mu\boldsymbol{1} + \boldsymbol{\varepsilon}\,,
\label{eq:anova-nm}
\end{equation}

and has the form:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.12}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.36}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.30}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.22}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Degrees of freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sums of squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean square
\end{minipage} \\
\midrule
\endhead
Blocks & \(b-1\) & RSS \eqref{eq:anova-nm} - RSS \eqref{eq:anova-bm} & \\
Treatments & \(t-1\) & RSS \eqref{eq:anova-bm} - RSS \eqref{eq:block-plm} & {[}RSS \eqref{eq:anova-bm} - RSS \eqref{eq:block-plm}{]} / \((t-1)\) \\
Residual & \(n - b - t + 1\) & RSS \eqref{eq:block-plm} & RSS \eqref{eq:block-plm} / \((n - b - t + 1)\) \\
Total & \(n - 1\) & RSS \eqref{eq:anova-nm} & \\
\bottomrule
\end{longtable}

We test the hypothesis \(H_0: \tau_1 = \cdots = \tau_t = 0\) at the \(100\alpha\)\% significance level by comparing the ratio of treatment and residual mean squares to the \(1-\alpha\) quantile of an \(F\) distribution with \(t-1\) and \(n-b-t+1\) degrees of freedom.

For Example \ref{exm:blocks-bars}, we obtain the following ANOVA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(strength }\SpecialCharTok{\textasciitilde{}}\NormalTok{ block }\SpecialCharTok{+}\NormalTok{ coating, }\AttributeTok{data =}\NormalTok{ bar)}
\FunctionTok{anova}\NormalTok{(bar.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: strength
##           Df Sum Sq Mean Sq F value Pr(>F)   
## block      7    215      31    0.55 0.7903   
## coating    3   1310     437    7.75 0.0011 **
## Residuals 21   1184      56                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Clearly, the null hypothesis of no treatment effect is rejected. The \texttt{anova} function also compares the block mean square to the residual mean square to perform a test of the hypothesis \(H_0: \beta_1 = \cdots = \beta_b = 0\). This is not a hypothesis that should usually be tested. The blocks are a nuisance factor and are generally a feature of the experimental process that has not been subject to randomisation; we are not interested in testing for block-to-block differences.\footnote{\texttt{R} and \texttt{anova} don't, of course, know that this is a block design or that a blocking factor is being tested.}

For Example \ref{exm:blocks-tyres}, we get the ANOVA table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tyre.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(wear }\SpecialCharTok{\textasciitilde{}}\NormalTok{ block }\SpecialCharTok{+}\NormalTok{ compound, }\AttributeTok{data =}\NormalTok{ tyre)}
\FunctionTok{anova}\NormalTok{(tyre.lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: wear
##           Df Sum Sq Mean Sq F value  Pr(>F)    
## block      3  39123   13041    37.2 0.00076 ***
## compound   3  20729    6910    19.7 0.00335 ** 
## Residuals  5   1751     350                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Again, the null hypothesis is rejected, and hence we should investigate which tyre compounds differ in their mean response.

The residual mean square for model \eqref{eq:block-plm} also provides an unbiased estimate, \(s^2\), of \(\sigma^2\), the variability of the \(\varepsilon_{ijl}\), \emph{assuming the unit-block-treatment model is correct}. For Example \ref{exm:blocks-bars},
\(s^2 = 56.387\) and for Example \ref{exm:blocks-tyres}, \(s^2 = 350.183\).

\hypertarget{rcdb}{%
\section{Randomised complete block designs}\label{rcdb}}

A randomised complete block design (RCBD) has each treatment replicated exactly once in each block, that is \(n_{ij} = 1\) for \(i=1,\ldots, b; j = 1, \ldots, t\). Therefore each block has common size \(k_1=\cdots =k_b = t\). The \(t\) treatments are randomised to the \(t\) units in each block. We can drop the index \(l\) from our unit-block-treatment model, as every treatment is replicated just once:

\begin{equation*}
y_{ij} = \mu + \beta_i + \tau_j + \varepsilon_{ij}\,, \qquad i = 1,\ldots, b; j = 1, \ldots, t\,.
\end{equation*}

For an RCBD, the matrix \(X_{2|1}\) has the form

\begin{align}
X_{2|1} & = (I_n - H_1)X_2 \nonumber \\
& = X_2 - H_1X_2 \nonumber \\
& = X_2 - \frac{1}{t}J_{n \times t} \label{eq:rcbd-x21}\,,
\end{align}

following from the fact that

\begin{align}
H_1X_2 & = X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2 \\
& = \frac{1}{t}X_1X_1^{\mathrm{T}}X_2 \\
& = \frac{1}{t}X_1N^{\mathrm{T}} \\
& = \frac{1}{t}X_1J_{b\times t} \\
& = \frac{1}{t}J_{n\times t}\,,
\end{align}

as for a RCBD \(X_1^{\mathrm{T}}X_1 = \mathrm{diag}(k_1,\ldots, k_b) = tI_b\) and \(X_2^{\mathrm{T}}X_1 = N = J_{t\times b}\).

Comparing \eqref{eq:rcbd-x21} to the form of \(X_{2|1}\) for a CRD, equation \eqref{eq:crd-x21}, we see that for the RCBD, \(X_{2|1}\) has the same form as a CRD with \(b\) replicates of each treatment (that is, \(n_i = b\) for \(i=1,\ldots, t\)). This is a powerful result, as it tell us

\begin{itemize}
\item
  The reduced normal equations for the RCBD take the same form as for the CRD,

  \[
    \hat{\tau}_j - \hat{\tau}_w = \bar{y}_{.j} - \bar{y}_{..}\,,
  \]

  with \(\hat{\tau}_w = \frac{1}{t}\sum_{j=1}^t\hat{\tau}_j\), \(\bar{y}_{.j} = \frac{1}{b}\sum_{i=1}^b y_{ij}\) and \(\bar{y}_{..} = \frac{1}{n}\sum_{i=1}^b\sum_{j=1}^t y_{ij}\). Hence, as with a CRD, we can estimate any contrast \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\), having \(\sum_{j=1}^tc_j = 1\), with estimator

  \[
  \widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} = \sum_{j=1}^tc_j\bar{y}_{.j}\,.
  \]

  Hence, the \textbf{point estimate} for a contrast \(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}\) is exactly the same as would be obtained by ignoring blocks and treating the experiment as a CRD with \(n = bt\) and \(n_i = b\), for \(i=1,\ldots, t\).
\item
  Inference for a contrast takes exactly the same form as for a CRD (Section \ref{contrast-crd}), with in particular:

  \[
  \mathrm{var}\left(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\right) = \frac{\sigma^2}{b}\sum_{j=1}^tc_j^2\,,
  \]

  and

  \[
    \widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} \sim N\left(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}, \frac{\sigma^2}{b}\sum_{j=1}^t c_j^2\right)\,.
  \]
\end{itemize}

Although these equations have the same form as for a CRD, note that \(\sigma^2\) is representing different quantities in each case.

\begin{itemize}
\item
  In a CRD, \(\sigma^2\) is the uncontrolled variation in the response \emph{among all experimental units}.
\item
  In a RCBD, \(\sigma^2\) is the uncontrolled variation in the response \emph{among all units within a common block}.
\end{itemize}

Block-to-block differences are modelled via inclusion of the block effects \(\beta_i\) in the model, and hence if blocking is effective, we would expect \(\sigma^2\) from a RCBD to be substantially smaller than from a corresponding CRD with \(n_i = b\).

Example \ref{exm:blocks-bars} is a RCBD. We can estimate the contrasts

\[
\tau_{1} - \tau_{2} \\
\tau_{1} - \tau_{3} \\
\tau_{1} - \tau_{4} \\
\]

between coatings\footnote{These contrasts measure the difference between the coating only made from ETPU and the three coatings with added fibres.} using \texttt{emmeans}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar.emm }\OtherTok{\textless{}{-}}\NormalTok{ emmeans}\SpecialCharTok{::}\FunctionTok{emmeans}\NormalTok{(bar.lm, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ coating)}
\NormalTok{contrastv1.emmc }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(levs)}
  \FunctionTok{data.frame}\NormalTok{(}\StringTok{\textquotesingle{}t1 v t2\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\StringTok{\textquotesingle{}t1 v t3\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }
  \StringTok{\textquotesingle{}t1 v t4\textquotesingle{}} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{emmeans}\SpecialCharTok{::}\FunctionTok{contrast}\NormalTok{(bar.emm, }\StringTok{\textquotesingle{}contrastv1\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate   SE df t.ratio p.value
##  t1.v.t2     -1.25 3.75 21  -0.330  0.7420
##  t1.v.t3     15.00 3.75 21   4.000  0.0010
##  t1.v.t4      4.00 3.75 21   1.070  0.2990
## 
## Results are averaged over the levels of: block
\end{verbatim}

It is important to once again adjust for mulitple comparisons. Here we can use a Bonferroni adjustment, and multiply each p-value by the number of tests (3). We obtain p-values of 1 (coating 1 versus 2), 0.002 (1 versus 3) and 0.896 (2 versus 3). Hence, there is a significant difference between coatings 1 and 3, with \(H_0: \tau_1 = \tau_3\) rejected at the 1\% significant level.

We can demonstrate the equivalence of the contrast point estimates between a RCBD and a CRD by fitting a unit-treatment model that ignores blocks:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bar\_crd.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(strength }\SpecialCharTok{\textasciitilde{}}\NormalTok{ coating, }\AttributeTok{data =}\NormalTok{ bar)}
\NormalTok{bar\_crd.emm }\OtherTok{\textless{}{-}}\NormalTok{ emmeans}\SpecialCharTok{::}\FunctionTok{emmeans}\NormalTok{(bar\_crd.lm, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ coating)}
\NormalTok{emmeans}\SpecialCharTok{::}\FunctionTok{contrast}\NormalTok{(bar\_crd.emm, }\StringTok{\textquotesingle{}contrastv1\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate   SE df t.ratio p.value
##  t1.v.t2     -1.25 3.53 28  -0.350  0.7260
##  t1.v.t3     15.00 3.53 28   4.240  <.0001
##  t1.v.t4      4.00 3.53 28   1.130  0.2670
\end{verbatim}

As expected the point estimates of the three contrasts are identical. In this case, the standard error of each contrast is actually smaller assuming a CRD without blocks, suggesting block-to-block differences were actually small here (further evidence is provided by the small block sums of squares in the ANOVA table). Here the estimate of \(\sigma\) from the RCBD is \(s_{RCBD} = 7.509\) and from the CRD is \(s_{CRD} = 7.07\), so for this example the unit-to-unit variation within and between blocks is not so different, and actually estimated to be slightly smaller in the CRD\footnote{Of course, the CRD has seven more degrees of freedom for estimating \(\sigma^2\) as block effects do not require estimation.}.

\hypertarget{blocks-orthogonal}{%
\section{Orthogonal blocking}\label{blocks-orthogonal}}

The equality of the point estimates from the RCBD and the CRD is a consequence of the block and treatment parameters in model \eqref{eq:block-model} being \textbf{orthogonal}. That is, the least squares estimators for \(\boldsymbol{\beta}\) and \(\boldsymbol{\tau}\) are independent in the sense that the estimators obtained from model \eqref{eq:block-plm} are the same as those obtained from the sub-models

\[
\boldsymbol{y}= \mu\boldsymbol{1}_n + X_1\boldsymbol{\beta} + \boldsymbol{\varepsilon}\,,
\]

and

\[
\boldsymbol{y}= \mu\boldsymbol{1}_n + X_2\boldsymbol{\tau} + \boldsymbol{\varepsilon}\,.
\]

That is, the presence or absence of the block parameters does not affect the estimator of the treatment parameters (and vice versa).

A condition for \(\boldsymbol{\beta}\) and \(\boldsymbol{\tau}\) to be estimated orthogonally can be derived from normal equations \eqref{eq:blocks-normal-1} - \eqref{eq:blocks-normal-2}. Firstly. we premultiply \eqref{eq:blocks-normal-1} by \(\frac{1}{n}X_1^{\mathrm{T}}\boldsymbol{1}_n\) and substract it from \eqref{eq:blocks-normal-2}:

\begin{align}
 & \left(X_1^{\mathrm{T}}\boldsymbol{1}_n - X_1^{\mathrm{T}}\boldsymbol{1}_n\right)\hat{\mu} + \left(X_1^{\mathrm{T}}X_1 - \frac{1}{n}X_1^{\mathrm{T}}\boldsymbol{1}_n\boldsymbol{1}_n^{\mathrm{T}}X_1\right)\hat{\boldsymbol{\beta}} + \left(X_1^{\mathrm{T}}X_2 - \frac{1}{n}X_1^{\mathrm{T}}\boldsymbol{1}_n\boldsymbol{1}_n^{\mathrm{T}}X_2\right)\hat{\boldsymbol{\tau}} \nonumber \\
 & = X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1\hat{\boldsymbol{\beta}} + X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_2\hat{\boldsymbol{\tau}} \nonumber \\
 & =  X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right) \label{eq:blocks-orth-ne1}\,.
\end{align}

Secondly, we premultiply \eqref{eq:blocks-normal-1} by \(\frac{1}{n}X_2^{\mathrm{T}}\boldsymbol{1}_n\) and substract it from \eqref{eq:blocks-normal-3}:

\begin{equation}
X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_2\hat{\boldsymbol{\tau}} = X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)\,.
\label{eq:blocks-orth-ne2}
\end{equation}

For equations \eqref{eq:blocks-orth-ne1} and \eqref{eq:blocks-orth-ne2} to be independent, we require

\[
X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1 = \boldsymbol{0}_{t\times b}\,.
\]
Hence, we obtain the following condition on the incidence matrix \(N = X_2^{\mathrm{T}}X_1\) for a block design to be orthogonal:

\begin{align}
N & = \frac{1}{n}X_2^{\mathrm{T}}X_1 \\
& = \frac{1}{n}\boldsymbol{n}\boldsymbol{k}^{\mathrm{T}}\,,
\end{align}

where \(\boldsymbol{n}^{\mathrm{T}} = (n_1,\ldots, n_t)\) is the vector of treatment replications and \(\boldsymbol{k}^{\mathrm{T}} = (k_1,\ldots, k_b)\) is the vector of block sizes.

The most common orthogonal block design for unstructured treatments is the RCBD, which has \(n = bt\), \(\boldsymbol{n} = b\boldsymbol{1}_t\), \(\boldsymbol{k} = t\boldsymbol{1}_b\), and

\begin{align}
N & = J_{t \times b}
& = \frac{1}{bt}\boldsymbol{n}\boldsymbol{k}^{\mathrm{T}}\,.
\end{align}

Hence, the condition for orthogonality is met. In an orthogonal design, such as a RCBD, all information about the treatment comparisons is contained in comparisons made within blocks. For more complex blocking structures, such as incomplete block designs, this is not the case. We shall see orthogonal blocking again in Chapter \ref{block-factorial}.

\hypertarget{balanced-incomplete-block-designs}{%
\section{Balanced incomplete block designs}\label{balanced-incomplete-block-designs}}

When the blocks sizes are less than the number of treatments, i.e.~\(k_i < t\) for all \(i=1,\ldots, b\), by necessity the design is incomplete, in that not all treatments can be allocated to every block. We will restrict ourselves now to considering binary designs with common block size \(k<t\). In a binary design, each treatment occurs within a block either 0 or 1 times (\(n_{ij}=0\) or \(n_{ij}=1\)).

Example \ref{exm:blocks-tyres} is an example of an incomplete design with \(k=3<t=4\). For incomplete designs, it is often useful to study the \emph{treatment concurrence} matrix, given by \(NN^{\mathrm{T}}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}
  \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
    \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
    \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{,}
    \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{nrow =} \DecValTok{4}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T}
\NormalTok{)}
\NormalTok{N }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(N)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    3    2    2    2
## [2,]    2    3    2    2
## [3,]    2    2    3    2
## [4,]    2    2    2    3
\end{verbatim}

This matrix has the number of treatment replications, \(n_j\), on the diagonal and the off-diagonal elements are equal to the number of blocks within which each pair of treatments occurs together. We will denote as \(\lambda_{ij}\) the number of blocks that contain both treatment \(i\) and treatment \(j\). For Example \ref{exm:blocks-tyres}, \(\lambda_{ij} = 2\) for all \(i,j = 1,\ldots, 4\); that is, each pair of treatments occurs together in two blocks.

\begin{definition}
\protect\hypertarget{def:bibd}{}\label{def:bibd}

A \textbf{balanced incomplete block design} (BIBD) is an incomplete block design with \(k<t\) that meets three requirements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The design is binary.
\item
  Each treatment is applied to a unit in the same number of blocks. It follows that the common number of units applied to each treatment must be \(r = n_j = bk / t\) (\(j=1,\ldots, t\)), where \(n = bk\). (Sometimes referred to as first-order balance).
\item
  Each pair of treatments is applied to two units in the same number of blocks, that is \(\lambda_{ij} = \lambda\). (Sometimes referred to as second-order balance).

  In fact, we can deduce that \(\lambda(t-1) = r(k - 1)\). To see this, focus on treatment 1. This treatment occurs in \(r\) blocks, and in each of these blocks, it occurs together with \(k-1\) other treatments. But also, treatment 1 occurs \(\lambda\) times with each of the other \(t-1\) treatments. Hence \(\lambda(t-1) = r(k - 1)\), or \(\lambda = r(k - 1) / (t-1)\).
\end{enumerate}

\end{definition}

The design in Example \ref{exm:blocks-tyres} is a BIBD with \(b=4\), \(k=3\), \(t = 4\), \(r = 4\times 3 / 4 = 3\), \(\lambda = 3 \times (3-1) / (4-1) = 2\).

\hypertarget{construction-of-bibds}{%
\subsection{Construction of BIBDs}\label{construction-of-bibds}}

BIBDs do not exist for all combinations of values of \(t\), \(k\) and \(b\). In particular, we must ensure

\begin{itemize}
\tightlist
\item
  \(r=bk/t\) is integer, and
\item
  \(\lambda = r(k - 1) / (t-1)\) is integer.
\end{itemize}

In general, we can always construct a BIBD for \(t\) treatments in \(b = {t \choose k}\) blocks of size \(k\), although it may not be the smallest possible BIBD. Such a design will have \(r = {t-1 \choose k-1}\) and \(\lambda = {t-2 \choose k-2}\). The design in Example \ref{exm:blocks-tyres} was constructed this way, with \(b = 4\), \(r = 3\) and \(\lambda = 2\).

\hypertarget{reduced-normal-equations}{%
\subsection{Reduced normal equations}\label{reduced-normal-equations}}

It can be shown that the reduced normal equations \eqref{eq:block-rne} for a BIBD can be written as

\begin{equation}
\left(I_t - \frac{1}{t}J_t\right)\hat{\boldsymbol{\tau}} = \frac{k}{\lambda t}\left(X_2^{\mathrm{T}}\boldsymbol{y}- \frac{1}{k}NX_1^{\mathrm{T}}\right)\boldsymbol{y}\,.
\label{eq:blocks-bibd-rne}
\end{equation}

Equation \eqref{eq:blocks-bibd-rne} defines a series of \(t\) equations of the form

\[
\hat{\tau_j} - \hat{\tau}_w = \frac{k}{\lambda t}\left(\sum_{i = 1}^b n_{ij}y_{ij} - \frac{1}{k}\sum_{i=1}^bn_{ij}\sum_{j=1}^tn_{ij}y_{ij}\right)\,. 
\]
Hence, as with the CRD and RCD we can estimate contrasts in the \(\tau_i\), with estimator

\[
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} = \frac{k}{\lambda t}\sum_{j=1}^tc_jq_j\,,
\]
with \(q_j = \sum_{i = 1}^b n_{ij}y_{ij} - \frac{1}{k}\sum_{i=1}^bn_{ij}\sum_{j=1}^tn_{ij}y_{ij}\).

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Consider the below randomised complete block design for comparing two catalysts, \(A\) and \(B\), for a chemical reaction using six batches of material. The response is the yield (\%) from the reaction.

  \begin{longtable}[]{@{}lcccccc@{}}
  \toprule
  Catalyst & Batch 1 & Batch 2 & Batch 3 & Batch 4 & Batch 5 & Batch 6 \\
  \midrule
  \endhead
  A & 9 & 19 & 28 & 22 & 18 & 8 \\
  B & 10 & 22 & 30 & 21 & 23 & 12 \\
  \bottomrule
  \end{longtable}

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Write down a unit-block-treatment model for this example.
  \item
    Test if there is a significant difference between catalysts at the 5\% level.
  \item
    Fit a unit-treatment model ignoring blocks and test again for a difference between catalysts. Comment on difference between this analysis and the one including blocks.
  \end{enumerate}
\item
  Consider the data below obtained from an agricultural experiment in which six different fertilizers were given to a crop of blackcurrants in a field. The field was divided into four equal areas of land so that the land in each area was fairly homogeneous. Each area of land was further subdivided into six plots and one of the fertilizers, chosen by a random procedure, was applied to each plot. The yields of blackcurrants obtained from each plot were recorded (in lbs) and are given in Table \ref{tab:blackcurrent-data}. In this randomised block design the treatments are the six fertilizers and the blocks are the four areas of land.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{blackcurrent }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{fertilizer =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{), }\DecValTok{4}\NormalTok{),}
               \AttributeTok{block =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{factor}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{)), }
               \AttributeTok{yield =} \FunctionTok{c}\NormalTok{(}\FloatTok{14.5}\NormalTok{, }\FloatTok{13.5}\NormalTok{, }\FloatTok{11.5}\NormalTok{, }\FloatTok{13.0}\NormalTok{, }\FloatTok{15.0}\NormalTok{, }\FloatTok{12.5}\NormalTok{,}
                            \FloatTok{12.0}\NormalTok{, }\FloatTok{10.0}\NormalTok{, }\FloatTok{11.0}\NormalTok{, }\FloatTok{13.0}\NormalTok{, }\FloatTok{12.0}\NormalTok{, }\FloatTok{13.5}\NormalTok{,}
                            \FloatTok{9.0}\NormalTok{, }\FloatTok{9.0}\NormalTok{, }\FloatTok{14.0}\NormalTok{, }\FloatTok{13.5}\NormalTok{, }\FloatTok{8.0}\NormalTok{, }\FloatTok{14.0}\NormalTok{,}
                            \FloatTok{6.5}\NormalTok{, }\FloatTok{8.5}\NormalTok{, }\FloatTok{10.0}\NormalTok{, }\FloatTok{7.5}\NormalTok{, }\FloatTok{7.0}\NormalTok{, }\FloatTok{8.0}\NormalTok{)}
\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}
\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(blackcurrent, }\AttributeTok{names\_from =}\NormalTok{ fertilizer, }\AttributeTok{values\_from =}\NormalTok{ yield),}
 \AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Block"}\NormalTok{, }\FunctionTok{paste}\NormalTok{(}\StringTok{"Ferilizer"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)),}
 \AttributeTok{caption =} \StringTok{"Blackcurrent experiment: yield (lbs) from six different fertilizers."}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{table}

   \caption{\label{tab:blackcurrent-data}Blackcurrent experiment: yield (lbs) from six different fertilizers.}
   \centering
   \begin{tabular}[t]{l|r|r|r|r|r|r}
   \hline
   Block & Ferilizer 1 & Ferilizer 2 & Ferilizer 3 & Ferilizer 4 & Ferilizer 5 & Ferilizer 6\\
   \hline
   1 & 14.5 & 13.5 & 11.5 & 13.0 & 15 & 12.5\\
   \hline
   2 & 12.0 & 10.0 & 11.0 & 13.0 & 12 & 13.5\\
   \hline
   3 & 9.0 & 9.0 & 14.0 & 13.5 & 8 & 14.0\\
   \hline
   4 & 6.5 & 8.5 & 10.0 & 7.5 & 7 & 8.0\\
   \hline
   \end{tabular}
   \end{table}

  Conduct a full analysis of this experiment, including

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    exploratory data analysis;
  \item
    fitting an appropriate linear model, and conducting an F-test to compare a model that explains variation between the six fertilizers to the model only containing blocks;
  \item
    Linear model diagnostics;
  \item
    if appropriate, multiple comparisons of all pairwise differences between treatments.
  \end{enumerate}
\end{enumerate}

\hypertarget{factorial}{%
\chapter{Factorial experiments}\label{factorial}}

\hypertarget{block-factorial}{%
\chapter{Blocking in factorial designs}\label{block-factorial}}

\hypertarget{fractional-factorial}{%
\chapter{Fractional factorial designs}\label{fractional-factorial}}

\hypertarget{response-surface-methodology}{%
\chapter{Response surface methodology}\label{response-surface-methodology}}

\hypertarget{optimal-design-of-experiments}{%
\chapter{Optimal design of experiments}\label{optimal-design-of-experiments}}

  \bibliography{math3014-6027.bib,packages.bib}

\end{document}
