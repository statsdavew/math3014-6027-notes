# Blocking {#blocking}

The completely randomised design (CRD) works well when there is sufficient homogeneous experimental units to perform the whole experiment under the same, or very similar, conditions and there are no restrictions on the randomisation of treatments to units. The only systematic (non-random) differences in the observed responses result from differences between the treatments. While such designs are commonly and successfully used, especially in smaller experiments, their application can often be unrealistic or impractical in many settings.

A common way in which the CRD fails is a lack of sufficiently similar experimental units. If there are systemtic differences between different batches, or **blocks** of units, these differences should be taken into account in both the allocation of treatments to units and the modelling of the resultant data. Otherwise, block-to-block differences may bias treatment comparisons and/or inflate our estimate of the background variability and hence reduce our ability to detect important treatment effects.

::: {.example #blocks-bars}
Steel bar experiment [@Morris2011, ch. 4]

@KSN2005 described an experiment to assess the strength of steel reinforcement bars from $t=4$ coatings^[Thr four coatings were all made from Engineering Thermoplastic Polyurethane (ETPU); coating one was solely made from ETPU, coatings 2-4 had additional glass fibre, carbon fibre or aramid fibre added, respectively.] (treatments). In total $n=32$ different bars (units) were available, but the testing process meant sets of four bars were tested together. To account for potential test-specific features (e.g. environmental or operational), these different test sets were assumed to form $b=8$ blocks of size $k=4$. The data are shown in Table \@ref(tab:bar-expt-data) below.

```{r bar-expt-data, warning = F}
bar <- data.frame(coating = rep(factor(1:4), 8),
                   block = rep(factor(1:8), rep(4, 8)), 
                   strength = c(136, 147, 138, 149, 136, 143, 122, 153, 150, 142, 131, 136,
                                   155, 148, 130, 129, 145, 149, 136, 139, 150, 149, 147, 144,
                                   147, 150, 125, 140, 148, 149, 118, 145)
                     )
knitr::kable(
 tidyr::pivot_wider(bar, names_from = coating, values_from = strength),
 col.names = c("Block", paste("Coating", 1:4)),
 caption = "Steel bar experiment: tensile strength values (kliograms per square inch, ksi) from steel bars with four different coatings."
)
```
Here, each block has size 4, which is equal to the number of treatments in the experiment, and each treatment is applied in each block. This is an example of a **randomised complete block design**.

We can study the data graphically, plotting by treatment and by block.

```{r bar-expt-boxplots, fig.show = 'hold', fig.align = 'center', fig.cap = 'Steel bar experiment: distributions of tensile strength (ksi) from the eight blocks (top) and the four coatings (bottom).'}
boxplot(strength ~ block, data = bar)
boxplot(strength ~ coating, data = bar)
```
The box plots within each plot in Figure \@ref(fig:bar-expt-boxplots) are comparable, as every treatment has occured with every block the same number of times (once). For example, when we compare the box plots for treatments 1 and 3, we know each of then display one observation from each block. Therefore, differences between treatments will not be influenced by large differences between blocks. This **balance** makes our analysis more straighforward. By eye, it appears here there may be differences between both coating 3 and the other three coatings.
:::

::: {.example #blocks-tyres}
Tyre experiment [@WH2009, ch. 3]

@Davies1954, p.200, examined the effect of $t=4$ different rubber compounds (treatments) on the lifetime of a tyre. Each tyre is only large enough to split into $k=3$ segments whilst still containing a representative amount of each compound. When tested, each tyre is subjected to the same road conditions, and hence is treated as a block. A design with $b=4$ blocks was used, as displayed in Table \@ref(tab:tyre-expt-data).

```{r tyre-expt-data, warning = F}
tyre <- data.frame(compound = as.factor(c(1, 2, 3, 1, 2, 4, 1, 3, 4, 2, 3, 4)),
                   block = rep(factor(1:4), rep(3, 4)), 
                   wear = c(238, 238, 279, 196, 213, 308, 254, 334, 367, 312, 421, 412)
                     )
options(knitr.kable.NA = '')
knitr::kable(
 tidyr::pivot_wider(tyre, names_from = compound, values_from = wear),
 col.names = c("Block", paste("Compound", 1:4)),
 caption = "Tyre experiment: relative wear measurements (unitless) from tires made with four different rubber compounds."
)
```
Here, each block has size $k=3$, which is smaller than the number of treatments ($t=4$). Hence, each block cannot contain an application of each treatment. This is an example of an **incomplete block design**.

Graphical exploration of the data is a little more problematic in this example. As each treatment does not occur in each block, box plots such as Figure \@ref(fig:tyre-expt-boxplots) are not as informative. Do compounds three and four have higher average wear because they were the only compounds to both occur in blocks 3 and 4? Or do blocks 3 and 4 have a higher mean because they contain both compounds 3 and 4? The design cannot help us entirely disentangle the impact of blocks and treatments^[This is our first example of (partial) confounding, which we will see again in Chapters \@ref(block-factorial) and \@ref(fractional-factorial)]. In our modelling, we will assume variation should first be described by blocks (which are generally fixed aspects of the experiment) and then treatments (which are more directly under the experimenter's control).


```{r tyre-expt-boxplots, fig.show = 'hold', fig.align = 'center', fig.cap = 'Tyre experiment: distributions of wear from the four blocks (top) and the four compounds (bottom).'}
boxplot(wear ~ block, data = tyre)
boxplot(wear ~ compound, data = tyre)
```
:::

## Unit-block-treatment model

If $n_{ij}$ is the number of times treatment $j$ occurs in block $i$, a common statistical model to describe data from a blocked experiment has the form

\begin{equation}
y_{ijl} = \mu + \beta_i + \tau_i + \varepsilon_{ijl}\,, \qquad i = 1,\ldots, b; j = 1, \ldots, t; l = 1,\ldots,n_{ij}\,,
(\#eq:block-model)
\end{equation}
where $y_{ijl}$ is the response from the $l$th application of the $j$th treatment in the $i$th block, $\mu$ is a constant parameter, $\beta_i$ is the effect of the $i$th block, $\tau_j$ is the effect of treatment $j$, and $\varepsilon_{ijl}\sim N(0, \sigma^2)$ are once again random individual effects from each experimental unit, assumed independent. The total number of runs in the experiment is given by $n = \sum_{i=1}^b\sum_{j=1}^t n_{ij}$.

For Example \@ref(exm:blocks-bars), there are $t=4$ experiments, $b = 8$ blocks and each treatment occurs once in each block, so $n_{ij} = 1$ for all $i, j$. In Example \@ref(exm:blocks-tyres), there are again $t=4$ treatments but now only $b=4$ blocks and not every treatment occurs in every block. In fact, we have $n_{11} = n_{12} = n_{13} = 1$, $n_{14} = 0$, $n_{21} = n_{22} =n_{24} = 1$, $n_{23} = 0$, $n_{31} = n_{33} =n_{34} = 1$, $n_{32} = 0$, $n_{41} = 0$ and $n_{42} = n_{43} =n_{44} = 1$.

Writing model \@ref(eq:block-model) is matrix form as a partitioned linear model, we obtain 

\begin{equation}
\by = \mu\boldsymbol{1}_n + X_1\boldsymbol{\beta} + X_2\boldsymbol{\tau} + \boldsymbol{\varepsilon}\,,
(\#eq:block-plm)
\end{equation}

with $\by$ the $n$-vector of responses, $X_1$ and $X_2$ $n\times b$ and $n\times t$ model matrices for blocks and treatments, respectively, $\boldsymbol{\beta} = (\beta_1,\ldots, \beta_b)^{\mathrm{T}}$, $\boldsymbol{\tau} = (\tau_1,\ldots, \tau_t)^{\mathrm{T}}$ and $\boldsymbol{\varepsilon}$ the $n$-vector of errors.

In equation\@ref(eq:block-plm), assuming without loss of generality that runs of the experiment are ordered by block, the matrix $X_1$ has the form

$$
X_1 = \bigoplus_{i = 1}^b \boldsymbol{1}_{k_i} = \begin{bmatrix}
\boldsymbol{1}_{k_1} & \boldsymbol{0}_{k_1} & \cdots &  \boldsymbol{0}_{k_1} \\
\boldsymbol{0}_{k_2} & \boldsymbol{1}_{k_2} & \cdots &  \boldsymbol{0}_{k_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b} & \boldsymbol{0}_{k_b} & \cdots &  \boldsymbol{1}_{k_b} \\
\end{bmatrix}\,,
$$
where $k_i = \sum_{j=1}^t n_{ij}$, the number of units in the $i$th block. The structure of matrix $X_2$ is harder to describe so distinctly, but each row includes a single non-zero entry, equal to one, indicating which treatment was applied in that run of the experiment. The first $k_1$ rows correspond to block 1, the second $k_2$ to block 2, and so on. We will see special cases later. 

## Normal equations

Writing as a partitioned model $\by = W\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$, with $W = [\boldsymbol{1} | X_1 | X_2]$ and $\boldsymbol{\alpha}^{\mathrm{T}} = [\mu | \boldsymbol{\beta}^{\mathrm{T}} | \boldsymbol{\tau}^{\mathrm{T}}]$, the least squares normal equations

$$
W^{\mathrm{T}}W \hat{\boldsymbol{\alpha}} = W^{\mathrm{T}}\by
$$
can be written as a set of three matrix equations:

\begin{align}
n\hat{\mu} + \boldsymbol{1}_n^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + \boldsymbol{1}_n^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = \boldsymbol{1}_n^{\mathrm{T}}\bY\,, \\
X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_1^{\mathrm{T}}\bY\,, \\
X_2^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_2^{\mathrm{T}}\bY\,. \\
\end{align}

Above, the matrices $X_1^{\mathrm{T}}X_1 = \mathrm{diag}(k_1,\ldots,k_b)$ and $X_2^{\mathrm{T}}X_2 = \mathrm{diag}(n_1,\ldots,n_t)$ have simple forms as diagonal matrices with entries equal to the size of each block and the number of replications of each treatment, respectively.

The $t\times b$ matrix $N = X_2^{\mathcal{T}}X_1$ is particularly important in block designs, and is called the **incidence** matrix. Each of the $i$th row of $N$ indicates in which blocks the $i$th treatment occurs.

We can eliminate the explicit dependence on $\mu$ and $\boldsymbol{\beta}$ to find reduced normal equations for $\boldsymbol{\tau}$ by multiplying the middle equation by $X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}$:

\begin{multline}
X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} \\
 = X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} \\
 = X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\bY \\
\end{multline}

and subtracting from the final equation:

\begin{multline}
X_2^{\mathrm{T}}\left(\boldsymbol{1}_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\right)\hat{\mu} + \left(X_2^{\mathrm{T}}X_1 - X_2^{\mathrm{T}}X_1\right)\boldsymbol{\beta} + X_2^{\mathrm{T}}\left(I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\right)X_2\boldsymbol{\tau}\\
= X_2^{\mathrm{T}}\left(I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\right)\bY\,.
\end{multline}
Clearly, a zero matrix is multiplying the block effects $\boldsymbol{\beta}$. Also,

$$
X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n = \boldsymbol{1}_n\,,
$$
as

$$
X_1(X_1^{\mathrm{T}}X_1)^{-1} = \bigoplus_{i = 1}^b \frac{1}{k_i}\boldsymbol{1}_{k_i} = \begin{bmatrix}
\frac{1}{k_1}\boldsymbol{1}_{k_1} & \boldsymbol{0}_{k_1} & \cdots &  \boldsymbol{0}_{k_1} \\
\boldsymbol{0}_{k_2} & \frac{1}{k_2}\boldsymbol{1}_{k_2} & \cdots &  \boldsymbol{0}_{k_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b} & \boldsymbol{0}_{k_b} & \cdots &  \frac{1}{k_b}\boldsymbol{1}_{k_b} \\
\end{bmatrix}\,,
$$
and hence

$$
X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}} = \bigoplus_{i = 1}^b \frac{1}{k_i}J_{k_i} = \begin{bmatrix}
\frac{1}{k_1}J_{k_1} & \boldsymbol{0}_{k_1\times k_2} & \cdots &  \boldsymbol{0}_{k_1\times k_b} \\
\boldsymbol{0}_{k_2\times k_1} & \frac{1}{k_2}J_{k_2} & \cdots &  \boldsymbol{0}_{k_2\times k_b} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b\times k_1} & \boldsymbol{0}_{k_b\times k_2} & \cdots &  \frac{1}{k_b}J_{k_b} \\
\end{bmatrix}\,.
$$

Writing $H_1 = X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}$, we then get the reduced normal equations for $\boldsymbol{\tau}$:

\begin{equation}
X_2^{\mathrm{T}}\left(I_n - H_1\right)X_2\boldsymbol{\tau}= X_2^{\mathrm{T}}\left(I_n - H_1\right)\bY\,.
(\#eq:block-rne)
\end{equation}

We can demonstrate the form of these matrices through our two examples.

For Example \@ref(exm:blocks-bars):

```{r blocks-bars-matrices, results = 'hold', eval = F}
one <- rep(1, 4)
X1 <- kronecker(diag(1, nrow = 8), one)
X2 <- diag(1, nrow = 4)
X2 <- do.call("rbind", replicate(8, X2, simplify = FALSE))
#incidence matrix
N <- t(X2) %*% X1
X1tX1 <- t(X1) %*% X1 # diagonal
X2tX2 <- t(X2) %*% X2 # diagonal
H1 <- X1 %*% solve(t(X1) %*% X1) %*% t(X1)
ones <- H1 %*% rep(1, 32) # H1 times vector of 1s is also a vector of 1s
A <- t(X2) %*% X2 - t(X2) %*% H1 %*% X2 # X2t(I - H1)X2
qr(A)$rank # rank 3
X2tH1 <- t(X2) %*% H1 # adjustment to y
W <- cbind(ones, X1, X2) # overall model matrix
qr(W)$rank # rank 11 (t+b - 1)
```

For Example \@ref(exm:blocks-tyres):

```{r blocks-tyres-matrices, results = 'hold', eval = F}
one <- rep(1, 3)
X1 <- kronecker(diag(1, nrow = 4), one)
X2 <- matrix(
        c(1, 0, 0, 0,
          0, 1, 0, 0,
          0, 0, 1, 0,
          1, 0, 0, 0,
          0, 1, 0, 0,
          0, 0, 0, 1,
          1, 0, 0, 0,
          0, 0, 1, 0,
          0, 0, 0, 1,
          0, 1, 0, 0,
          0, 0, 1, 0,
          0, 0, 0, 1), nrow = 12, byrow = T
)
#incidence matrix
N <- t(X2) %*% X1
X1tX1 <- t(X1) %*% X1 # diagonal
X2tX2 <- t(X2) %*% X2 # diagonal
H1 <- X1 %*% solve(t(X1) %*% X1) %*% t(X1)
ones <- H1 %*% rep(1, 12) # H1 times vector of 1s is also a vector of 1s
A <- t(X2) %*% X2 - t(X2) %*% H1 %*% X2 # X2t(I - H1)X2
qr(A)$rank # rank 3
X2tH1 <- t(X2) %*% H1 # adjustment to y
W <- cbind(ones, X1, X2) # overall model matrix
qr(W)$rank # rank 7 (t+b - 1)
```

Notice that if we write $X_{2|1} = (I_n - H_1)X2$, then the reduced normal equations become

$$
X_{2|1}^{\mathrm{T}}X_{2|1}\boldsymbol{\tau} = X_{2|1}^{\mathrm{T}}\bY\,,
$$
which have the same form as the CRD in Chapter \@ref(crd) albeit with a different $X_{2|1}$ matrix as we are adjusting for more complex nuisance parameters.

















































































