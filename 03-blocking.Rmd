# Blocking {#blocking}

The completely randomised design (CRD) works well when there is sufficient homogeneous experimental units to perform the whole experiment under the same, or very similar, conditions and there are no restrictions on the randomisation of treatments to units. The only systematic (non-random) differences in the observed responses result from differences between the treatments. While such designs are commonly and successfully used, especially in smaller experiments, their application can often be unrealistic or impractical in many settings.

A common way in which the CRD fails is a lack of sufficiently similar experimental units. If there are systemtic differences between different batches, or **blocks** of units, these differences should be taken into account in both the allocation of treatments to units and the modelling of the resultant data. Otherwise, block-to-block differences may bias treatment comparisons and/or inflate our estimate of the background variability and hence reduce our ability to detect important treatment effects.

::: {.example #blocks-bars}
Steel bar experiment [@Morris2011, ch. 4]

@KSN2005 described an experiment to assess the strength of steel reinforcement bars from $t=4$ coatings^[The four coatings were all made from Engineering Thermoplastic Polyurethane (ETPU); coating one was solely made from ETPU, coatings 2-4 had additional glass fibre, carbon fibre or aramid fibre added, respectively.] (treatments). In total $n=32$ different bars (units) were available, but the testing process meant sets of four bars were tested together. To account for potential test-specific features (e.g. environmental or operational), these different test sets were assumed to form $b=8$ blocks of size $k=4$. The data are shown in Table \@ref(tab:bar-expt-data) below.

```{r bar-expt-data, warning = F}
bar <- data.frame(coating = rep(factor(1:4), 8),
                   block = rep(factor(1:8), rep(4, 8)), 
                   strength = c(136, 147, 138, 149, 136, 143, 122, 153, 150, 142, 131, 136,
                                   155, 148, 130, 129, 145, 149, 136, 139, 150, 149, 147, 144,
                                   147, 150, 125, 140, 148, 149, 118, 145)
                     )
knitr::kable(
 tidyr::pivot_wider(bar, names_from = coating, values_from = strength),
 col.names = c("Block", paste("Coating", 1:4)),
 caption = "Steel bar experiment: tensile strength values (kliograms per square inch, ksi) from steel bars with four different coatings."
)
```
Here, each block has size 4, which is equal to the number of treatments in the experiment, and each treatment is applied in each block. This is an example of a **randomised complete block design**.

We can study the data graphically, plotting by treatment and by block.

```{r bar-expt-boxplots, fig.show = 'hold', fig.align = 'center', fig.cap = 'Steel bar experiment: distributions of tensile strength (ksi) from the eight blocks (top) and the four coatings (bottom).'}
boxplot(strength ~ block, data = bar)
boxplot(strength ~ coating, data = bar)
```
The box plots within each plot in Figure \@ref(fig:bar-expt-boxplots) are comparable, as every treatment has occured with every block the same number of times (once). For example, when we compare the box plots for treatments 1 and 3, we know each of then display one observation from each block. Therefore, differences between treatments will not be influenced by large differences between blocks. This **balance** makes our analysis more straighforward. By eye, it appears here there may be differences between both coating 3 and the other three coatings.
:::

::: {.example #blocks-tyres}
Tyre experiment [@WH2009, ch. 3]

@Davies1954, p.200, examined the effect of $t=4$ different rubber compounds (treatments) on the lifetime of a tyre. Each tyre is only large enough to split into $k=3$ segments whilst still containing a representative amount of each compound. When tested, each tyre is subjected to the same road conditions, and hence is treated as a block. A design with $b=4$ blocks was used, as displayed in Table \@ref(tab:tyre-expt-data).

```{r tyre-expt-data, warning = F}
tyre <- data.frame(compound = as.factor(c(1, 2, 3, 1, 2, 4, 1, 3, 4, 2, 3, 4)),
                   block = rep(factor(1:4), rep(3, 4)), 
                   wear = c(238, 238, 279, 196, 213, 308, 254, 334, 367, 312, 421, 412)
                     )
options(knitr.kable.NA = '')
knitr::kable(
 tidyr::pivot_wider(tyre, names_from = compound, values_from = wear),
 col.names = c("Block", paste("Compound", 1:4)),
 caption = "Tyre experiment: relative wear measurements (unitless) from tires made with four different rubber compounds."
)
```
Here, each block has size $k=3$, which is smaller than the number of treatments ($t=4$). Hence, each block cannot contain an application of each treatment. This is an example of an **incomplete block design**.

Graphical exploration of the data is a little more problematic in this example. As each treatment does not occur in each block, box plots such as Figure \@ref(fig:tyre-expt-boxplots) are not as informative. Do compounds three and four have higher average wear because they were the only compounds to both occur in blocks 3 and 4? Or do blocks 3 and 4 have a higher mean because they contain both compounds 3 and 4? The design cannot help us entirely disentangle the impact of blocks and treatments^[This is our first example of (partial) confounding, which we will see again in Chapters \@ref(block-factorial) and \@ref(fractional-factorial)]. In our modelling, we will assume variation should first be described by blocks (which are generally fixed aspects of the experiment) and then treatments (which are more directly under the experimenter's control).


```{r tyre-expt-boxplots, fig.show = 'hold', fig.align = 'center', fig.cap = 'Tyre experiment: distributions of wear from the four blocks (top) and the four compounds (bottom).'}
boxplot(wear ~ block, data = tyre)
boxplot(wear ~ compound, data = tyre)
```
:::

## Unit-block-treatment model

If $n_{ij}$ is the number of times treatment $j$ occurs in block $i$, a common statistical model to describe data from a blocked experiment has the form

\begin{equation}
y_{ijl} = \mu + \beta_i + \tau_j + \varepsilon_{ijl}\,, \qquad i = 1,\ldots, b; j = 1, \ldots, t; l = 1,\ldots,n_{ij}\,,
(\#eq:block-model)
\end{equation}
where $y_{ijl}$ is the response from the $l$th application of the $j$th treatment in the $i$th block, $\mu$ is a constant parameter, $\beta_i$ is the effect of the $i$th block, $\tau_j$ is the effect of treatment $j$, and $\varepsilon_{ijl}\sim N(0, \sigma^2)$ are once again random individual effects from each experimental unit, assumed independent. The total number of runs in the experiment is given by $n = \sum_{i=1}^b\sum_{j=1}^t n_{ij}$.

For Example \@ref(exm:blocks-bars), there are $t=4$ experiments, $b = 8$ blocks and each treatment occurs once in each block, so $n_{ij} = 1$ for all $i, j$. In Example \@ref(exm:blocks-tyres), there are again $t=4$ treatments but now only $b=4$ blocks and not every treatment occurs in every block. In fact, we have $n_{11} = n_{12} = n_{13} = 1$, $n_{14} = 0$, $n_{21} = n_{22} =n_{24} = 1$, $n_{23} = 0$, $n_{31} = n_{33} =n_{34} = 1$, $n_{32} = 0$, $n_{41} = 0$ and $n_{42} = n_{43} =n_{44} = 1$.

Writing model \@ref(eq:block-model) is matrix form as a partitioned linear model, we obtain 

\begin{equation}
\by = \mu\boldsymbol{1}_n + X_1\boldsymbol{\beta} + X_2\boldsymbol{\tau} + \boldsymbol{\varepsilon}\,,
(\#eq:block-plm)
\end{equation}

with $\by$ the $n$-vector of responses, $X_1$ and $X_2$ $n\times b$ and $n\times t$ model matrices for blocks and treatments, respectively, $\boldsymbol{\beta} = (\beta_1,\ldots, \beta_b)^{\mathrm{T}}$, $\boldsymbol{\tau} = (\tau_1,\ldots, \tau_t)^{\mathrm{T}}$ and $\boldsymbol{\varepsilon}$ the $n$-vector of errors.

In equation\@ref(eq:block-plm), assuming without loss of generality that runs of the experiment are ordered by block, the matrix $X_1$ has the form

$$
X_1 = \bigoplus_{i = 1}^b \boldsymbol{1}_{k_i} = \begin{bmatrix}
\boldsymbol{1}_{k_1} & \boldsymbol{0}_{k_1} & \cdots &  \boldsymbol{0}_{k_1} \\
\boldsymbol{0}_{k_2} & \boldsymbol{1}_{k_2} & \cdots &  \boldsymbol{0}_{k_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b} & \boldsymbol{0}_{k_b} & \cdots &  \boldsymbol{1}_{k_b} \\
\end{bmatrix}\,,
$$
where $k_i = \sum_{j=1}^t n_{ij}$, the number of units in the $i$th block. The structure of matrix $X_2$ is harder to describe so distinctly, but each row includes a single non-zero entry, equal to one, indicating which treatment was applied in that run of the experiment. The first $k_1$ rows correspond to block 1, the second $k_2$ to block 2, and so on. We will see special cases later. 

## Normal equations

Writing as a partitioned model $\by = W\boldsymbol{\alpha} + \boldsymbol{\varepsilon}$, with $W = [\boldsymbol{1} | X_1 | X_2]$ and $\boldsymbol{\alpha}^{\mathrm{T}} = [\mu | \boldsymbol{\beta}^{\mathrm{T}} | \boldsymbol{\tau}^{\mathrm{T}}]$, the least squares normal equations

\begin{equation}
W^{\mathrm{T}}W \hat{\boldsymbol{\alpha}} = W^{\mathrm{T}}\by
(\#eq:bne)
\end{equation}

can be written as a set of three matrix equations:

\begin{align}
n\hat{\mu} + \boldsymbol{1}_n^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + \boldsymbol{1}_n^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = \boldsymbol{1}_n^{\mathrm{T}}\bY\,, (\#eq:blocks-normal-1)\\
X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_1^{\mathrm{T}}\bY\,, (\#eq:blocks-normal-2)\\
X_2^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} & = X_2^{\mathrm{T}}\bY\,. (\#eq:blocks-normal-3)\\
\end{align}

Above, the matrices $X_1^{\mathrm{T}}X_1 = \mathrm{diag}(k_1,\ldots,k_b)$ and $X_2^{\mathrm{T}}X_2 = \mathrm{diag}(n_1,\ldots,n_t)$ have simple forms as diagonal matrices with entries equal to the size of each block and the number of replications of each treatment, respectively.

The $t\times b$ matrix $N = X_2^{\mathcal{T}}X_1$ is particularly important in block designs, and is called the **incidence** matrix. Each of the $i$th row of $N$ indicates in which blocks the $i$th treatment occurs.

We can eliminate the explicit dependence on $\mu$ and $\boldsymbol{\beta}$ to find reduced normal equations for $\boldsymbol{\tau}$ by multiplying the middle equation by $X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}$:

\begin{multline}
X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} \\
 = X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\hat{\mu} + X_2^{\mathrm{T}}X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2\hat{\boldsymbol{\tau}} \\
 = X_2^{\mathrm{T}}X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\bY \\
\end{multline}

and subtracting from the final equation:

\begin{multline}
X_2^{\mathrm{T}}\left(\boldsymbol{1}_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n\right)\hat{\mu} + \left(X_2^{\mathrm{T}}X_1 - X_2^{\mathrm{T}}X_1\right)\boldsymbol{\beta} \\ + X_2^{\mathrm{T}}\left(I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\right)X_2\boldsymbol{\tau}\\
= X_2^{\mathrm{T}}\left(I_n - X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\right)\bY\,.
\end{multline}
Clearly, a zero matrix is multiplying the block effects $\boldsymbol{\beta}$. Also,

$$
X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}\boldsymbol{1}_n = \boldsymbol{1}_n\,,
$$
as

$$
X_1(X_1^{\mathrm{T}}X_1)^{-1} = \bigoplus_{i = 1}^b \frac{1}{k_i}\boldsymbol{1}_{k_i} = \begin{bmatrix}
\frac{1}{k_1}\boldsymbol{1}_{k_1} & \boldsymbol{0}_{k_1} & \cdots &  \boldsymbol{0}_{k_1} \\
\boldsymbol{0}_{k_2} & \frac{1}{k_2}\boldsymbol{1}_{k_2} & \cdots &  \boldsymbol{0}_{k_2} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b} & \boldsymbol{0}_{k_b} & \cdots &  \frac{1}{k_b}\boldsymbol{1}_{k_b} \\
\end{bmatrix}\,,
$$
and hence

$$
X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}} = \bigoplus_{i = 1}^b \frac{1}{k_i}J_{k_i} = \begin{bmatrix}
\frac{1}{k_1}J_{k_1} & \boldsymbol{0}_{k_1\times k_2} & \cdots &  \boldsymbol{0}_{k_1\times k_b} \\
\boldsymbol{0}_{k_2\times k_1} & \frac{1}{k_2}J_{k_2} & \cdots &  \boldsymbol{0}_{k_2\times k_b} \\
\vdots & & \ddots & \vdots \\
\boldsymbol{0}_{k_b\times k_1} & \boldsymbol{0}_{k_b\times k_2} & \cdots &  \frac{1}{k_b}J_{k_b} \\
\end{bmatrix}\,.
$$

Writing $H_1 = X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}$, we then get the reduced normal equations for $\boldsymbol{\tau}$:

\begin{equation}
X_2^{\mathrm{T}}\left(I_n - H_1\right)X_2\boldsymbol{\tau}= X_2^{\mathrm{T}}\left(I_n - H_1\right)\bY\,.
(\#eq:block-rne)
\end{equation}

We can demonstrate the form of these matrices through our two examples.

For Example \@ref(exm:blocks-bars):

```{r blocks-bars-matrices, results = 'hold', eval = F}
one <- rep(1, 4)
X1 <- kronecker(diag(1, nrow = 8), one)
X2 <- diag(1, nrow = 4)
X2 <- do.call("rbind", replicate(8, X2, simplify = FALSE))
#incidence matrix
N <- t(X2) %*% X1
X1tX1 <- t(X1) %*% X1 # diagonal
X2tX2 <- t(X2) %*% X2 # diagonal
H1 <- X1 %*% solve(t(X1) %*% X1) %*% t(X1)
ones <- H1 %*% rep(1, 32) # H1 times vector of 1s is also a vector of 1s
A <- t(X2) %*% X2 - t(X2) %*% H1 %*% X2 # X2t(I - H1)X2
qr(A)$rank # rank 3
X2tH1 <- t(X2) %*% H1 # adjustment to y
W <- cbind(ones, X1, X2) # overall model matrix
qr(W)$rank # rank 11 (t+b - 1)
```

For Example \@ref(exm:blocks-tyres):

```{r blocks-tyres-matrices, results = 'hold', eval = F}
one <- rep(1, 3)
X1 <- kronecker(diag(1, nrow = 4), one)
X2 <- matrix(
        c(1, 0, 0, 0,
          0, 1, 0, 0,
          0, 0, 1, 0,
          1, 0, 0, 0,
          0, 1, 0, 0,
          0, 0, 0, 1,
          1, 0, 0, 0,
          0, 0, 1, 0,
          0, 0, 0, 1,
          0, 1, 0, 0,
          0, 0, 1, 0,
          0, 0, 0, 1), nrow = 12, byrow = T
)
#incidence matrix
N <- t(X2) %*% X1
X1tX1 <- t(X1) %*% X1 # diagonal
X2tX2 <- t(X2) %*% X2 # diagonal
H1 <- X1 %*% solve(t(X1) %*% X1) %*% t(X1)
ones <- H1 %*% rep(1, 12) # H1 times vector of 1s is also a vector of 1s
A <- t(X2) %*% X2 - t(X2) %*% H1 %*% X2 # X2t(I - H1)X2
qr(A)$rank # rank 3
X2tH1 <- t(X2) %*% H1 # adjustment to y
W <- cbind(ones, X1, X2) # overall model matrix
qr(W)$rank # rank 7 (t+b - 1)
```

Notice that if we write $X_{2|1} = (I_n - H_1)X_2$, then the reduced normal equations become

$$
X_{2|1}^{\mathrm{T}}X_{2|1}\boldsymbol{\tau} = X_{2|1}^{\mathrm{T}}\bY\,,
$$
which have the same form as the CRD in Chapter \@ref(crd) albeit with a different $X_{2|1}$ matrix as we are adjusting for more complex nuisance parameters.

In general, the solution of these equations will depend on the exact form of the design. For the randomised complete block design, the solution turns out to be straighforward (see Section \@ref(#rcdb) below). By default, to fit model \@ref(eq:block-plm), the `lm` function in `R` applies the constraint $\tau_t = \beta_b = 0$, and removes the corresponding columns from $X_1$ and $X_2$, to leave a $W$ matrix with full column rank. Clearly, this solution is not unique but, as with CRDs, we will identify uniquely estimatable combinations of the model parameters (and use `emmeans` to extract these estimates from an `lm` object).

## Analysis of variance {#block-anova}

As was the case with the CRD, it can be shown that any solution to the normal equations \@ref(eq:bne) will produce a unique solution to $\widehat{W\alpha}$, and hence a unique analysis of variance decomposition can be obtained.

For a block experiment, the ANOVA table is comparing the full model \@ref(eq:block-plm), the model containing the block effects

\begin{equation}
\bY = \mu\boldsymbol{1} + X_1\boldsymbol{\beta} + \boldsymbol{\varepsilon}
(\#eq:anova-bm)
\end{equation}

and the null model

\begin{equation}
\bY = \mu\boldsymbol{1} + \boldsymbol{\varepsilon}\,,
(\#eq:anova-nm)
\end{equation}

and has the form:

| Source | Degrees of freedom | Sums of squares | Mean square |
| :----- | :----------------: | :-------------- | :---------- |
| Blocks | $b-1$ | RSS \@ref(eq:anova-nm) - RSS \@ref(eq:anova-bm) | |
| Treatments | $t-1$ | RSS \@ref(eq:anova-bm) - RSS \@ref(eq:block-plm) | [RSS \@ref(eq:anova-bm) - RSS \@ref(eq:block-plm)] / $(t-1)$ |
| Residual | $n - b - t + 1$ | RSS \@ref(eq:block-plm) | RSS \@ref(eq:block-plm) / $(n - b - t + 1)$ |
| Total | $n - 1$ | RSS \@ref(eq:anova-nm) | |

We test the hypothesis $H_0: \tau_1 = \cdots = \tau_t = 0$ at the $100\alpha$% significance level by comparing the ratio of treatment and residual mean squares to the $1-\alpha$ quantile of an $F$ distribution with $t-1$ and $n-b-t+1$ degrees of freedom.

For Example \@ref(exm:blocks-bars), we obtain the following ANOVA. 

```{r block-bars-anova}
bar.lm <- lm(strength ~ block + coating, data = bar)
anova(bar.lm)
```
Clearly, the null hypothesis of no treatment effect is rejected. The `anova` function also compares the block mean square to the residual mean square to perform a test of the hypothesis $H_0: \beta_1 = \cdots = \beta_b  = 0$. This is not a hypothesis that should usually be tested. The blocks are a nuisance factor and are generally a feature of the experimental process that has not been subject to randomisation; we are not interested in testing for block-to-block differences.^[`R` and `anova` don't, of course, know that this is a block design or that a blocking factor is being tested.] 

For Example \@ref(exm:blocks-tyres), we get the ANOVA table:

```{r block-tyres-anova}
tyre.lm <- lm(wear ~ block + compound, data = tyre)
anova(tyre.lm)
```

Again, the null hypothesis is rejected, and hence we should investigate which tyre compounds differ in their mean response.

```{r blocks-s2, echo = F}
bar.s2 <- summary(bar.lm)$sigma^2
tyre.s2 <- summary(tyre.lm)$sigma^2
```

The residual mean square for model \@ref(eq:block-plm) also provides an unbiased estimate, $s^2$, of $\sigma^2$, the variability of the $\varepsilon_{ijl}$, *assuming the unit-block-treatment model is correct*. For Example \@ref(exm:blocks-bars), 
$s^2 = `r bar.s2`$ and for Example \@ref(exm:blocks-tyres), $s^2 = `r tyre.s2`$.

## Randomised complete block designs {#rcdb}

A randomised complete block design (RCBD) has each treatment replicated exactly once in each block, that is $n_{ij} = 1$ for $i=1,\ldots, b; j = 1, \ldots, t$. Therefore each block has common size $k_1=\cdots =k_b = t$. The $t$ treatments are randomised to the $t$ units in each block. We can drop the index $l$ from our unit-block-treatment model, as every treatment is replicated just once:

\begin{equation*}
y_{ij} = \mu + \beta_i + \tau_j + \varepsilon_{ij}\,, \qquad i = 1,\ldots, b; j = 1, \ldots, t\,.
\end{equation*}

For an RCBD, the matrix $X_{2|1}$ has the form

\begin{align}
X_{2|1} & = (I_n - H_1)X_2 \nonumber \\
& = X_2 - H_1X_2 \nonumber \\
& = X_2 - \frac{1}{t}J_{n \times t} (\#eq:rcbd-x21)\,,
\end{align}

following from the fact that

\begin{align}
H_1X_2 & = X_1(X_1^{\mathrm{T}}X_1)^{-1}X_1^{\mathrm{T}}X_2 \\
& = \frac{1}{t}X_1X_1^{\mathrm{T}}X_2 \\
& = \frac{1}{t}X_1N^{\mathrm{T}} \\
& = \frac{1}{t}X_1J_{b\times t} \\
& = \frac{1}{t}J_{n\times t}\,,
\end{align}

as for a RCBD $X_1^{\mathrm{T}}X_1 = \mathrm{diag}(k_1,\ldots, k_b) = tI_b$ and $X_2^{\mathrm{T}}X_1 = N = J_{t\times b}$.

Comparing \@ref(eq:rcbd-x21) to the form of $X_{2|1}$ for a CRD, equation \@ref(eq:crd-x21), we see that for the RCBD, $X_{2|1}$ has the same form as a CRD with $b$ replicates of each treatment (that is, $n_i = b$ for $i=1,\ldots, t$). This is a powerful result, as it tell us

- The reduced normal equations for the RCBD take the same form as for the CRD,
    
  $$
    \hat{\tau}_j - \hat{\tau}_w = \bar{y}_{.j} - \bar{y}_{..}\,,
  $$

    with $\hat{\tau}_w = \frac{1}{t}\sum_{j=1}^t\hat{\tau}_j$, $\bar{y}_{.j} = \frac{1}{b}\sum_{i=1}^b y_{ij}$ and $\bar{y}_{..} = \frac{1}{n}\sum_{i=1}^b\sum_{j=1}^t y_{ij}$. Hence, as with a CRD, we can estimate any contrast $\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}$, having $\sum_{j=1}^tc_j = 0$, with estimator 
    
  $$
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} = \sum_{j=1}^tc_j\bar{y}_{.j}\,.
  $$
    
    Hence, the **point estimate** for a contrast $\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}$ is exactly the same as would be obtained by ignoring blocks and treating the experiment as a CRD with $n = bt$ and $n_i = b$, for $i=1,\ldots, t$.
    
- Inference for a contrast takes exactly the same form as for a CRD (Section \@ref(contrast-crd)), with in particular:

  $$
\mathrm{var}\left(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\right) = \frac{\sigma^2}{b}\sum_{j=1}^tc_j^2\,,
  $$

  and

  $$
    \widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} \sim N\left(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}, \frac{\sigma^2}{b}\sum_{j=1}^t c_j^2\right)\,.
  $$

Although these equations have the same form as for a CRD, note that $\sigma^2$ is representing different quantities in each case.

- In a CRD, $\sigma^2$ is the uncontrolled variation in the response *among all experimental units*.

- In a RCBD, $\sigma^2$ is the uncontrolled variation in the response *among all units within a common block*.

Block-to-block differences are modelled via inclusion of the block effects $\beta_i$ in the model, and hence if blocking is effective, we would expect $\sigma^2$ from a RCBD to be substantially smaller than from a corresponding CRD with $n_i = b$.

Example \@ref(exm:blocks-bars) is a RCBD. We can estimate the contrasts 

$$
\tau_{1} - \tau_{2} \\
\tau_{1} - \tau_{3} \\
\tau_{1} - \tau_{4} \\
$$

between coatings^[These contrasts measure the difference between the coating only made from ETPU and the three coatings with added fibres.] using `emmeans`.

```{r blocks-bar-contrasts}
bar.emm <- emmeans::emmeans(bar.lm, ~ coating)
contrastv1.emmc <- function(levs)
  data.frame('t1 v t2' = c(1, -1, 0, 0), 't1 v t3' = c(1, 0, -1, 0), 
  't1 v t4' = c(1, 0, 0, -1))
emmeans::contrast(bar.emm, 'contrastv1')
```

```{r blocks-bar-bonf, echo = F}
temp <- pmin(3 * transform(emmeans::contrast(bar.emm, 'contrastv1'))[, 6], 1)
```

It is important to once again adjust for mulitple comparisons. Here we can use a Bonferroni adjustment, and multiply each p-value by the number of tests (3). We obtain p-values of `r temp[1]` (coating 1 versus 2), `r temp[2]` (1 versus 3) and `r temp[3]` (2 versus 3). Hence, there is a significant difference between coatings 1 and 3, with $H_0: \tau_1 = \tau_3$ rejected at the 1% significant level.

We can demonstrate the equivalence of the contrast point estimates between a RCBD and a CRD by fitting a unit-treatment model that ignores blocks:

```{r blocks-crd}
bar_crd.lm <- lm(strength ~ coating, data = bar)
bar_crd.emm <- emmeans::emmeans(bar_crd.lm, ~ coating)
emmeans::contrast(bar_crd.emm, 'contrastv1')
```

```{r blocks-crd-s2, echo = F}
crd.s2 <- summary(bar_crd.lm)$sigma^2
rcbd.s2 <- summary(bar.lm)$sigma^2
```

As expected the point estimates of the three contrasts are identical. In this case, the standard error of each contrast is actually smaller assuming a CRD without blocks, suggesting block-to-block differences were actually small here (further evidence is provided by the small block sums of squares in the ANOVA table). Here the estimate of $\sigma$ from the RCBD is $s_{RCBD} = `r sqrt(rcbd.s2)`$ and from the CRD is $s_{CRD} = `r sqrt(crd.s2)`$, so for this example the unit-to-unit variation within and between blocks is not so different, and actually estimated to be slightly smaller in the CRD^[Of course, the CRD has seven more degrees of freedom for estimating $\sigma^2$ as block effects do not require estimation.].

## Orthogonal blocking {#blocks-orthogonal}

The equality of the point estimates from the RCBD and the CRD is a consequence of the block and treatment parameters in model \@ref(eq:block-model) being **orthogonal**. That is, the least squares estimators for $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are independent in the sense that the estimators obtained from model \@ref(eq:block-plm) are the same as those obtained from the sub-models

$$
\by = \mu\boldsymbol{1}_n + X_1\boldsymbol{\beta} + \boldsymbol{\varepsilon}\,,
$$

and

$$
\by = \mu\boldsymbol{1}_n + X_2\boldsymbol{\tau} + \boldsymbol{\varepsilon}\,.
$$

That is, the presence or absence of the block parameters does not affect the estimator of the treatment parameters (and vice versa).

A condition for $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ to be estimated orthogonally can be derived from normal equations \@ref(eq:blocks-normal-1) - \@ref(eq:blocks-normal-2). Firstly. we premultiply \@ref(eq:blocks-normal-1) by $\frac{1}{n}X_1^{\mathrm{T}}\boldsymbol{1}_n$ and substract it from \@ref(eq:blocks-normal-2):

\begin{align}
 & \left(X_1^{\mathrm{T}}\boldsymbol{1}_n - X_1^{\mathrm{T}}\boldsymbol{1}_n\right)\hat{\mu} + \left(X_1^{\mathrm{T}}X_1 - \frac{1}{n}X_1^{\mathrm{T}}\boldsymbol{1}_n\boldsymbol{1}_n^{\mathrm{T}}X_1\right)\hat{\boldsymbol{\beta}} + \left(X_1^{\mathrm{T}}X_2 - \frac{1}{n}X_1^{\mathrm{T}}\boldsymbol{1}_n\boldsymbol{1}_n^{\mathrm{T}}X_2\right)\hat{\boldsymbol{\tau}} \nonumber \\
 & = X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1\hat{\boldsymbol{\beta}} + X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_2\hat{\boldsymbol{\tau}} \nonumber \\
 & =  X_1^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right) (\#eq:blocks-orth-ne1)\,.
\end{align}

Secondly, we premultiply \@ref(eq:blocks-normal-1) by $\frac{1}{n}X_2^{\mathrm{T}}\boldsymbol{1}_n$ and substract it from \@ref(eq:blocks-normal-3):

\begin{equation}
X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1\hat{\boldsymbol{\beta}} + X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_2\hat{\boldsymbol{\tau}} = X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)\,.
(\#eq:blocks-orth-ne2)
\end{equation}

For equations \@ref(eq:blocks-orth-ne1) and \@ref(eq:blocks-orth-ne2) to be independent, we require 

$$
X_2^{\mathrm{T}}\left(I_n - \frac{1}{n}J_n\right)X_1 = \boldsymbol{0}_{t\times b}\,.
$$
Hence, we obtain the following condition on the incidence matrix $N = X_2^{\mathrm{T}}X_1$ for a block design to be orthogonal:

\begin{align}
N & = \frac{1}{n}X_2^{\mathrm{T}}J_nX_1 \\
& = \frac{1}{n}\boldsymbol{n}\boldsymbol{k}^{\mathrm{T}}\,,
\end{align}

where $\boldsymbol{n}^{\mathrm{T}} = (n_1,\ldots, n_t)$ is the vector of treatment replications and $\boldsymbol{k}^{\mathrm{T}} = (k_1,\ldots, k_b)$ is the vector of block sizes.

The most common orthogonal block design for unstructured treatments is the RCBD, which has $n = bt$, $\boldsymbol{n} = b\boldsymbol{1}_t$, $\boldsymbol{k} = t\boldsymbol{1}_b$, and 

\begin{align}
N & = J_{t \times b}
& = \frac{1}{bt}\boldsymbol{n}\boldsymbol{k}^{\mathrm{T}}\,.
\end{align}

Hence, the condition for orthogonality is met. In an orthogonal design, such as a RCBD, all information about the treatment comparisons is contained in comparisons made within blocks. For more complex blocking structures, such as incomplete block designs, this is not the case. We shall see orthogonal blocking again in Chapter \@ref(block-factorial).

## Balanced incomplete block designs

When the blocks sizes are less than the number of treatments, i.e. $k_i < t$ for all $i=1,\ldots, b$, by necessity the design is incomplete, in that not all treatments can be allocated to every block. We will restrict ourselves now to considering binary designs with common block size $k<t$. In a binary design, each treatment occurs within a block either 0 or 1 times ($n_{ij}=0$ or $n_{ij}=1$).

Example \@ref(exm:blocks-tyres) is an example of an incomplete design with $k=3<t=4$. For incomplete designs, it is often useful to study the *treatment concurrence* matrix, given by $NN^{\mathrm{T}}$.

```{r block-tyre-concurrence}
N <- matrix(
  c(1, 1, 1, 0,
    1, 1, 0, 1,
    1, 0, 1, 1,
    0, 1, 1, 1),
  nrow = 4, byrow = T
)
N %*% t(N)

```
This matrix has the number of treatment replications, $n_j$, on the diagonal and the off-diagonal elements are equal to the number of blocks within which each pair of treatments occurs together. We will denote as $\lambda_{ij}$ the number of blocks that contain both treatment $i$ and treatment $j$ ($i\ne j$). For Example \@ref(exm:blocks-tyres), $\lambda_{ij} = 2$ for all $i,j = 1,\ldots, 4$; that is, each pair of treatments occurs together in two blocks. 

::: {.definition #bibd}
A **balanced incomplete block design** (BIBD) is an incomplete block design with $k<t$ that meets three requirements:

1. The design is binary.
2. Each treatment is applied to a unit in the same number of blocks. It follows that the common number of units applied to each treatment must be $r = n_j = bk / t$ ($j=1,\ldots, t$), where $n = bk$. (Sometimes referred to as first-order balance).
3. Each pair of treatments is applied to two units in the same number of blocks, that is $\lambda_{ij} = \lambda$. (Sometimes referred to as second-order balance). 

    In fact, we can deduce that $\lambda(t-1) = r(k - 1)$. To see this, focus on treatment 1. This treatment occurs in $r$ blocks, and in each of these blocks, it occurs together with $k-1$ other treatments. But also, treatment 1 occurs $\lambda$ times with each of the other $t-1$ treatments. Hence $\lambda(t-1) = r(k - 1)$, or $\lambda = r(k - 1) / (t-1)$.
:::

The design in Example \@ref(exm:blocks-tyres) is a BIBD with $b=4$, $k=3$, $t = 4$, $r = 4\times 3 / 4 = 3$, $\lambda = 3 \times (3-1) / (4-1) = 2$.

### Construction of BIBDs

BIBDs do not exist for all combinations of values of $t$, $k$ and $b$. In particular, we must ensure

- $r=bk/t$ is integer, and
- $\lambda = r(k - 1) / (t-1)$ is integer.

In general, we can always construct a BIBD for $t$ treatments in $b = {t \choose k}$ blocks of size $k$, although it may not be the smallest possible BIBD. Each of the possible choices of $k$ treatments from the total $t$ forms one block. Such a design will have $r = {t-1 \choose k-1}$ and $\lambda = {t-2 \choose k-2}$. The design in Example \@ref(exm:blocks-tyres) was constructed this way, with $b = 4$, $r = 3$ and $\lambda = 2$.

Sometimes, smaller BIBDs that satisfy the two conditions above can be constructed. Finding these designs is an combinatorial problem, and tables of designs are available in the literature^[See @CochranCox1957 and @FisherYates1963]. A large collection of BIBDs has also been catalogued in the `R` package `ibd`. The function `bibd` generates BIBDs for given values of $t$, $b$, $r$, $k$ and $\lambda$, or returns a message that a design is not available for those values. We can use the function to find the design used in Example \@ref(exm:blocks-tyres).

```{r idb-tyre, warning = FALSE}
tyre.bibd <- ibd::bibd(v = 4, b = 4, r = 3, k = 3, lambda = 2) # note, v is the notation for the number of treatments
tyre.bibd$N # incidence matrix
```
We can also use the package to find a design for bigger experiments, for example $t=8$ treatments in $b=14$ blocks of size $k=4$. Here, $r = 14\times 4 / 8 = `r 14*4/8`$ and $\lambda = `r 14*4/8` \times 3 / 7 = `r (14*4/8) * 3 / 7`$.

```{r idb-larger-ex}
larger.bibd <- ibd::bibd(v = 8, b = 14, r = 7, k = 4, lambda = 3) 
larger.bibd$N
```
Although larger than the examples we have considered before, this design is small compared to the design that would be obtained from the naive construction above with $b = {t \choose k} = {8 \choose 4} = `r choose(8, 4)`$ blocks.  


### Reduced normal equations

It can be shown that the reduced normal equations \@ref(eq:block-rne) for a BIBD can be written as

\begin{equation}
\left(I_t - \frac{1}{t}J_t\right)\hat{\boldsymbol{\tau}} = \frac{k}{\lambda t}\left(X_2^{\mathrm{T}} - \frac{1}{k}NX_1^{\mathrm{T}}\right)\bY\,.
(\#eq:blocks-bibd-rne)
\end{equation}

Equation \@ref(eq:blocks-bibd-rne) defines a series of $t$ equations of the form

\begin{align*}
\hat{\tau_j} - \hat{\tau}_w & = \frac{k}{\lambda t}\left(\sum_{i = 1}^b n_{ij}y_{ij} - \frac{1}{k}\sum_{i=1}^bn_{ij}\sum_{j=1}^tn_{ij}y_{ij}\right) \\
& = \frac{k}{\lambda t} q_j\,,
\end{align*}

with $q_j = \sum_{i = 1}^b n_{ij}y_{ij} - \frac{1}{k}\sum_{i=1}^bn_{ij}\sum_{j=1}^tn_{ij}y_{ij}$.

Notice that unlike for the RCBD, the reduced normal equations for a BIBD do not correspond to the equations for a CRD. Although the first term in $q_i$ is the sum of the responses for the $j$th treatment (mirroring the CRD), the second term is no longer the overall sum (or average) of the responses. In fact, for $q_j$ this second term is an adjusted total, just involving observations from those blocks that contain treatment $j$.

### Estimation and inference

As with the CRD and RCD we can estimate contrasts in the $\tau_i$, with estimator

$$
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} = \frac{k}{\lambda t}\sum_{j=1}^tc_jq_j\,.
$$
Due to the form of the reduced normal equations for the BIBD, the estimator is no longer just a linear combination of the treatment means; $q_j$ includes a term that adjusts for the blocks in which treatment $j$ has occurred.

The simplest way to derive the variance of this estimator is to rewrite it in the form

$$
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} = \frac{k}{\lambda t}\boldsymbol{c}^{\mathrm{T}}\boldsymbol{q}\,, 
$$
with $\boldsymbol{q} = \left(X_2^{\mathrm{T}} - \frac{1}{k}NX_1^{\mathrm{T}}\right)\bY$. Then

$$
\mathrm{var}\left(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\right) = \frac{k^2}{\lambda^2t^2}\boldsymbol{c}^{\mathrm{T}}\mathrm{var}(\boldsymbol{q}) \boldsymbol{c}\,.
$$
Recalling that $NN^{\mathrm{T}}$ is the treatment coincidence matrix, the variance-covariance matrix of $\boldsymbol{q}$ is given by

\begin{align*}
\mathrm{var}(\boldsymbol{q}) & = \left(X_2^{\mathrm{T}} - \frac{1}{k}NX_1^{\mathrm{T}}\right)\left(X_2 - \frac{1}{k}X_1N\right)\sigma^2 \\
& = \sigma^2\left\{rI_t - \frac{1}{k}NN^{\mathrm{T}}\right\} \\
& = \sigma^2\left\{rI_t - \frac{1}{k}\left[(r-\lambda)I_t + \lambda J_t\right]\right\} \\
& = \sigma^2\left\{\left(\frac{r(k-1) + \lambda}{k}\right)I_t - \frac{\lambda}{k}J_t\right\} \\
& = \sigma^2\left\{\left(\frac{\lambda(t-1) + \lambda}{k}\right)I_t - \frac{\lambda}{k}J_t\right\} \\
& = \sigma^2\left\{\frac{\lambda t}{k}I_t - \frac{\lambda}{k}J_t\right\}\,.
\end{align*}

Hence

\begin{align*}
\mathrm{var}\left(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}\right)
& = \frac{k^2}{\lambda^2t^2}\boldsymbol{c}^{\mathrm{T}}\left(\frac{\lambda t}{k}I_t - \frac{\lambda}{k}J_t\right)\boldsymbol{c}\sigma^2 \\
& = \frac{k\sigma^2}{\lambda t}\boldsymbol{c}^{\mathrm{T}}\boldsymbol{c} \\
& = \frac{k\sigma^2}{\lambda t}\sum_{j=1}^tc_i^2\,,
\end{align*}

as $\boldsymbol{c}^{\mathrm{T}}J_t = \boldsymbol{0}$ as $\sum_{j=1}^tc_j = 0$. The estimator is also unbiased ($E(\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}}) = \boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}$), and hence the sampling distribution, upon which inference can be based, is given by

$$
\widehat{\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}} \sim N\left(\boldsymbol{c}^{\mathrm{T}}\boldsymbol{\tau}, \frac{k\sigma^2}{\lambda t}\sum_{j=1}^t c_j^2\right)\,.
$$

Returning to Example \@ref(exm:blocks-tyres), we can use these results to estimate all pairwise differences between the four treatments. Firstly, we directly calculate the $q_j$ from treatment and block sums, using the incidence matrix.

```{r bibd-tyre-q}
trtsum <- aggregate(wear ~ compound, data = tyre, FUN = sum)[, 2]
blocksum <- aggregate(wear ~ block, data = tyre, FUN = sum)[, 2]
q <- trtsum - N %*% blocksum / 3
C <- matrix(
  c(1, -1, 0, 0,
    1, 0, -1, 0,
    1, 0, 0, -1,
    0, 1, -1, 0,
    0, 1, 0, -1,
    0, 0, 1, -1),
  ncol = 4, byrow = T
)
k <- 3; lambda <- 2; t <- 4
pe <- k * C %*% q / (lambda * t) # point estimates
se <- sqrt(2 * k * tyre.s2 / (lambda * t)) # st error (same for each contrast)
t.ratio <- pe / se
p.value <- 1 - ptukey(abs(t.ratio) * sqrt(2), 4, 5)
data.frame(Pair = c('1v2', '1v3', '1v4', '2v3', '2v4', '3v4'),
  Estimate = pe, St.err = se, t.ratio = t.ratio, 
           p.value = p.value, reject = p.value < 0.05)
```

Secondly, we can use `emmeans` to generate the same output from an `lm` object. 

```{r bibd-tyre-contrasts}
tyre.emm <- emmeans::emmeans(tyre.lm, ~ compound)
pairs(tyre.emm)
```
For this experiment, treatments 1 and 3, 1 and 4, 2 and 3, and 2 and 4 are significantly different at an experiment-wise type I error rate of 5%.

## Exercises

1. Consider the below randomised complete block design for comparing two catalysts, $A$ and $B$, for a chemical reaction using six batches of material. The response is the yield (%) from the reaction.


    | Catalyst | Batch 1 | Batch 2 | Batch 3 | Batch 4 | Batch 5 | Batch 6 |
    | :------- | :----: | :-----: | :-----: | :-----: | :-----: | :-----: |
    | A      | 9       | 19      | 28      | 22      | 18      | 8       |
    | B      | 10      | 22      | 30      | 21      | 23      | 12      |

    i. Write down a unit-block-treatment model for this example.
    i. Test if there is a significant difference between catalysts at the 5% level.
    i. Fit a unit-treatment model ignoring blocks and test again for a difference between catalysts. Comment on difference between this analysis and the one including blocks.
    
<details>
<summary><b>Solution</b></summary>
i. The unit-block-treatment model for this RCBD is given by

    \begin{equation}
y_{ij} = \mu + \beta_i + \tau_j + \varepsilon_{ij}\,\qquad i = 1, \ldots, 6; j = \mathrm{A}, \mathrm{B}\,,
(\#eq:exercises-rcbd-model)
    \end{equation}

    where $y_{ij}$ is the yield from the application of catalyst $j$ to block $i$, $\mu$ is a constant parameter, $\beta_i$ is the effect of block $i$ and $\tau_j$ is the effect of treatment $j$. The errors follow a normal distribution $\varepsilon_{ij}\sim N(0, \sigma^2)$ with mean 0 and constant variance, and are assumed independent for different experimental units.

i. To test if there is a difference between catalysts, we compare model \@ref(eq:exercises-rcbd-model) with the model that only includes block effects:

    \begin{equation}
y_{ij} = \mu + \beta_i + \varepsilon_{ij}\,\qquad i = 1, \ldots, 6; j = \mathrm{A}, \mathrm{B}\,.
(\#eq:exercises-rcbd-model-block)
    \end{equation}

    The relative difference in the residual mean squares between these two models follows an F distribution under $H_0: \tau_1 = \tau_2 = 0$, see Section \@ref(block-anova). These test statistic and associated p-value can be calculated in `R` using `anova`.

    ```{r ex1-anova}
reaction <- data.frame(
  catalyst = factor(rep(c('A', 'B'), 6)),
  batch = factor(rep(1:6, rep(2, 6))),
  yield = c(9, 10, 19, 22, 28, 30, 22, 21, 18, 23, 8, 12)
)
reaction.lm <- lm(yield ~ batch + catalyst, data = reaction)
anova(reaction.lm)
    ```

    The p-value is (just) less than 0.05, and so we can reject $H_0$ (no treatment difference) at the 5\% level.

i. The unit-treatment model is given by

    $$
y_{ij} = \mu + \tau_j + \varepsilon_{ij}\,,\qquad i = 1,\ldots, 6; j = 1,2\,,
    $$

    where now **all** the between unit differences, including any block-to-block differences, are modelled by the unit errors $\varepsilon_{ij}$. We can fit this model in `R`.

    ```{r ex1-unit-treatment}
reaction2.lm <- lm(yield ~ catalyst, data = reaction)
anova(reaction2.lm)
    ```

    There is no longer evidence to reject the null hypothesis of no treatment difference. This is because the residual mean square is now so much larger (`r summary(reaction2.lm)$sigma^2` versus `r summary(reaction.lm)$sigma^2`). The residual mean square is also an unbiased estimate of $\sigma^2$, and our estimate of $\sigma^2$ from the unit-treatment model is clearly much larger, as block-to-block variation has also been included.
</details>
    
2. Consider the data below obtained from an agricultural experiment in which six different fertilizers were given to a crop of blackcurrants in a field. The field was divided into four equal areas of land so that the land in each area was fairly homogeneous. Each area of land was further subdivided into six plots and one of the fertilizers, chosen by a random procedure, was applied to each plot. The yields of blackcurrants obtained from each plot were recorded (in lbs) and are given in Table \@ref(tab:blackcurrent-data). In this randomised block design the treatments are the six fertilizers and the blocks are the four areas of land. 

    ```{r blackcurrent-data, warning = F}
blackcurrent <- data.frame(fertilizer = rep(factor(1:6), 4),
                   block = rep(factor(1:4), rep(6, 4)), 
                   yield = c(14.5, 13.5, 11.5, 13.0, 15.0, 12.5,
                                12.0, 10.0, 11.0, 13.0, 12.0, 13.5,
                                9.0, 9.0, 14.0, 13.5, 8.0, 14.0,
                                6.5, 8.5, 10.0, 7.5, 7.0, 8.0)
)
knitr::kable(
 tidyr::pivot_wider(blackcurrent, names_from = fertilizer, values_from = yield),
 col.names = c("Block", paste("Ferilizer", 1:6)),
 caption = "Blackcurrent experiment: yield (lbs) from six different fertilizers."
)
    ```

    Conduct a full analysis of this experiment, including
    
    a. exploratory data analysis;
    b. fitting an appropriate linear model, and conducting an F-test to compare a model that explains variation between the six fertilizers to the model only containing blocks;
    c. Linear model diagnostics;
    d. if appropriate, multiple comparisons of all pairwise differences between treatments.

3. Prove that $2k/\lambda t > 2/b$, and use this result to compare the precision of a pairwise treatment comparison from a BIBD with block size $k$ and an RCBD, both with $t$ treatments in $b$ blocks.

4. Construct a BIBD for $t=5$ treatments in $b=5$ blocks of size $k=4$ units. What are $r$ and $k$ for your design? Compare your design to a RCBD via the efficiency for estimating a pairwise treatment difference.

5. Consider an experiment for testing the abrasion resistance of rubber-coated fabric. There are four types of material, denoted A - D. The response is the loss in weight in 0.1 milligrams (mg) over a standard period of time. The testing machine has four positions, so four samples of material can be tested at a time. Past experience suggests that there may be differences between these positions, and there may be differences between each application of the testing machine (due to changes in set-up). Therefore, we have two blocking variables, "Position" and "Application". For this experiment, we use a **latin square** design, as follows.

    ```{r rc-expt-treatments, warning = F}
fabric <- data.frame(material = factor(c('C', 'D', 'B', 'A', 'A', 'B', 'D', 'C', 
                               'D', 'C', 'A', 'B', 'B', 'A', 'C', 'D')),
                   position = rep(factor(1:4), 4),
                   application = rep(factor(1:4), rep(4, 4)),
                   weight = c(235, 236, 218, 268,
                                251, 241, 227, 229, 
                                234, 273, 274, 226,
                                195, 270, 230, 225)
                     )
knitr::kable(
 tidyr::pivot_wider(fabric, id_cols = application, names_from = position, 
                    values_from = material),
 col.names = c("Application", paste("Position", 1:4)))
    ```


    The blocking variables are represented as the rows and columns of the square; the latin letters represent the different treatments. A latin square of order $k$ is a $k\times k$ square of $k$ latin letters arranged so that each letter appears exactly once in each row and column (Sudoko squares are also examples of latin squares). To perform the experiment, the levels of the blocking factors are randomly assigned to the rows and the columns, and the different treatments to the letters.

    A suitable unit-block-treatment model for a latin square design has the form
    
    $$
    y_{ijk} = \mu + \beta_i + \gamma_j + \tau_k + \varepsilon_{ijk}\,,\qquad i,j,k = 1,\ldots, t\,,
    $$
    
    with $\mu$ a constant parameter, $\beta_i$ row block effects, $\gamma_j$ column block effects and $\tau_k$ the treatment effects. As usual, $\varepsilon_{ijk}\sim N(0, \sigma^2)$, with errors from different units assumed independent.  Note that not all combinations of $i,j,k$ actually occur in the design; at the intersection of the $i$th row and $j$th column, only one of the $t$ treatments is applied.

    a. Write down a set of normal equations for the model parameters. 
    b. It can be shown that the reduced normal equations for the treatment parameters $\tau_1,\ldots, \tau_t$ have the form
    $$
    \hat{\tau}_k - \hat{\tau}_w = \bar{y}_{..j} - \bar{y}_{...}\,,
    $$
    with $\hat{\tau}_w = \frac{1}{t}\sum_{k=1}^t\hat{\tau}_k$, $\bar{y}_{..k} = \frac{1}{t}\sum_{i=1}^t\sum_{j=1}^tn_{ijk}y_{ijk}$ (mean for treatment $k$) and $\bar{y}_{...} = \frac{1}{n}\sum_{i=1}^t\sum_{j=1}^t\sum_{k=1}^tn_{ij}y_{ijk}$ (overall mean) where $n_{ijk} = 1$ if treatment $k$ occurs at the intersection of row $i$ and column $j$ and zero otherwise, and $\sum_{i=1}^t\sum_{j=1}^tn_{ijk} = t$ for all $k=1, \ldots, t$. 
    
        Demonstrate that any contrast can therefore be estimated from this design, and derive the variance of the estimator of $\sum_{k=1}^tc_k\tau_k$.

    The data for this experiment is as follows, where the entries in the table give the response for the corresponding treatment:

    ```{r rc-expt-data, warning = F}
knitr::kable(
 tidyr::pivot_wider(fabric, id_cols = application, names_from = position, 
                    values_from = weight),
 col.names = c("Application", paste("Position", 1:4)))
    ```
   
    c. For this data, test if there is a significant difference between materials. If there is, conduct multiple comparisons of all pairs at an experimentwise type I error rate of 5%.






























































