[["index.html", "MATH3014-6027 Design (and Analysis) of Experiments Preface", " MATH3014-6027 Design (and Analysis) of Experiments Dave Woods 2022-02-13 Preface These are draft lecture notes for the modules MATH3014 and MATH6027 Design (and Analysis) of Experiments at the University of Southampton for academic year 2021-22. They are very much work in progress. Southampton prerequisites for this module are MATH2010 or MATH6174 and STAT6123 (or equivalent modules on linear modelling). "],["intro.html", "Chapter 1 Motivation, introduction and revision 1.1 Motivation 1.2 Aims of experimentation and some examples 1.3 Some definitions 1.4 Principles of experimentation 1.5 Revision on the linear model 1.6 Exercises", " Chapter 1 Motivation, introduction and revision Definition 1.1 An experiment is the process through which data are collected to answer a scientific question (physical science, social science, actuarial science \\(\\dots\\)) by deliberately varying some features of the process under study in order to understand the impact of these changes on measureable responses. In this course we consider only intervention experiments, in which some aspects of the process are under the experimenters’ control. We do not consider surveys or observational studies. Definition 1.2 Design of experiments is the topic in Statistics concerned with the selection of settings of controllable variables or factors in an experiment and their allocation to experimental units in order to maximise the effectiveness of the experiment at achieving its aim. People have been designing experiments for as long as they have been exploring the natural world. Collecting empirical evidence is key for scientific development, as described in terms of clinical trials by xkcd: Some notable milestones in the history of the design of experiments include: prior to the 20th century: Francis Bacon (17th century; pioneer of the experimental methods) James Lind (18th century; experiments to eliminate scurvy) Charles Peirce (19th century; advocated randomised experiments and randomisation-based inference) 1920s: agriculture (particularly at the Rothamsted Agricultural Research Station) 1940s: clinical trials (Austin Bradford-Hill) 1950s: (manufacturing) industry (W. Edwards Deming; Genichi Taguchi) 1960s: psychology and economics (Vernon Smith) 1980s: in-silico (computer experiments) 2000s: online (A/B testing) See Luca and Bazerman (2020) for further history, anecdotes and examples, especially from psychology and technology. Figure 1.1 shows the Broadbalk agricultural field experiment at Rothamsted, one of the longest continuous running experiments in the world, which is testing the impact of different manures and fertilizers on the growth of winter wheat. Figure 1.1: The Broadbalk experiment, Rothamsted (photograph taken 2016) 1.1 Motivation Example 1.1 Consider an experiment to compare two treatments (e.g. drugs, diets, fertilisers, \\(\\dots\\)). We have \\(n\\) subjects (people, mice, plots of land, \\(\\dots\\)), each of which can be assigned one of the two treatments. A response (protein measurement, weight, yield, \\(\\dots\\)) is then measured. Question: How many subjects should be assigned to each treatment to gain the most precise1 inference about the difference in response from the two treatments? Consider a linear statistical model2 for the response (see MATH2010 or MATH6174/STAT6123): \\[\\begin{equation} Y_j=\\beta_{0}+\\beta_{1}x_j+\\varepsilon_j\\,,\\qquad j=1, \\ldots, n\\,, \\tag{1.1} \\end{equation}\\] where \\(\\varepsilon_j\\sim N(0,\\sigma^{2})\\) are independent and identically distributed errors and \\(\\beta_{0}, \\beta_{1}\\) are unknown constants (parameters). Let \\[\\begin{equation} x_{j}=\\left\\{\\begin{array}{ll} -1&amp;\\textrm{if treatment 1 is applied to the $j$th subject}\\\\ +1&amp;\\textrm{if treatment 2 is applied to the $j$th subject}\\nonumber , \\end{array} \\right. \\end{equation}\\] for \\(j=1,\\dots,n\\).3 The difference in expected response from treatments 1 and 2 is \\[\\begin{equation} \\begin{split} \\textrm{E}[Y_j\\, |\\, x_j = +1] - \\textrm{E}[Y_j\\, |\\, x_j = -1] &amp; = \\beta_{0}+\\beta_{1}-\\beta_{0}+\\beta_{1} \\\\ &amp; = 2\\beta_{1}\\,. \\end{split} \\tag{1.2} \\end{equation}\\] Therefore, we require the the most precise estimator of \\(\\beta_{1}\\) possible. That is, we wish to make the variance of our estimator of \\(\\beta_1\\) as small as possible. Parameters \\(\\beta_{0}\\) and \\(\\beta_{1}\\) can be estimated using least squares (see MATH2010 or MATH6174/STAT6123). For \\(Y_1,\\dots,Y_n\\), we can write the model down in matrix form: \\[\\begin{equation*} \\left[ \\begin{array}{c} Y_1\\\\ \\vdots\\\\ Y_n\\end{array}\\right] =\\left[ \\begin{array}{cc} 1&amp;x_{1}\\\\ \\vdots&amp;\\vdots\\\\ 1&amp;x_{n}\\end{array}\\right] \\left[ \\begin{array}{c} \\beta_{0}\\\\ \\beta_{1}\\end{array}\\right] +\\left[ \\begin{array}{c} \\varepsilon_{1}\\\\ \\vdots\\\\ \\varepsilon_{n}\\end{array}\\right]\\,. \\end{equation*}\\] Or, by defining some notation: \\[\\begin{equation} \\boldsymbol{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\, \\tag{1.3} \\end{equation}\\] where \\(\\boldsymbol{Y}\\) - \\(n\\times 1\\) vector of responses; \\(X\\) - \\(n\\times p\\) model matrix; \\(\\boldsymbol{\\beta}\\) - \\(p\\times 1\\) vector of parameters; \\(\\boldsymbol{\\varepsilon}\\) - \\(n\\times 1\\) vector of errors. The least squares estimators, \\(\\hat{\\boldsymbol{\\beta}}\\), are chosen such that the quadratic form \\[\\begin{equation*} (\\boldsymbol{Y}-X\\boldsymbol{\\beta})^{\\textrm{T}}(\\boldsymbol{Y}-X\\boldsymbol{\\beta}) \\end{equation*}\\] is minimised (recall that \\(\\textrm{E}(\\textbf{Y})=X\\boldsymbol{\\beta}\\)). Therefore \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\textrm{argmin}_{\\boldsymbol{\\beta}}(\\boldsymbol{Y}^{\\textrm{T}}\\boldsymbol{Y}+\\boldsymbol{\\beta}^{\\textrm{T}}X^{\\textrm{T}}X\\boldsymbol{\\beta} -2\\boldsymbol{\\beta}^{\\textrm{T}}X^{\\textrm{T}}\\boldsymbol{Y})\\,. \\end{equation*}\\] If we differentiate with respect to \\(\\boldsymbol{\\beta}\\)4, \\[\\begin{equation*} \\frac{\\partial}{\\partial\\boldsymbol{\\beta}}=2X^{\\textrm{T}}X\\boldsymbol{\\beta}-2X^{\\textrm{T}}\\boldsymbol{Y}\\,,\\nonumber \\end{equation*}\\] and equate to 0, we get the estimators \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}=(X^{\\textrm{T}}X)^{-1}X^{\\textrm{T}}\\boldsymbol{Y}\\,. \\tag{1.4} \\end{equation}\\] These are the least squares estimators. For Example 1.1, \\[ X=\\left[\\begin{array}{cc} 1&amp;x_{1}\\\\ \\vdots&amp;\\vdots\\\\ 1&amp;x_{n}\\end{array}\\right]\\,, \\qquad X^{\\textrm{T}}X=\\left[\\begin{array}{cc} n&amp;\\sum x_j\\\\ \\sum x_j&amp;\\sum x_j^{2}\\end{array}\\right]\\,, \\] \\[ (X^{\\textrm{T}}X)^{-1}=\\frac{1}{n\\sum x_j^{2}-(\\sum x_j)^{2}}\\left[\\begin{array}{cc} \\sum x_j^{2}&amp;-\\sum x_j\\\\ -\\sum x_j&amp;n\\end{array}\\right]\\,, \\qquad X^{\\textrm{T}}\\boldsymbol{Y}=\\left[\\begin{array}{c} \\sum Y_j\\\\ \\sum x_jY_j\\end{array}\\right]\\,. \\] Then, \\[\\begin{align} \\hat{\\boldsymbol{\\beta}}=\\left[\\begin{array}{c} \\hat{\\beta}_{0}\\\\ \\hat{\\beta}_{1}\\end{array}\\right] &amp; =\\frac{1}{n\\sum x_j^{2}-(\\sum x_j)^{2}} \\left[\\begin{array}{cc} \\sum x_j^{2}&amp;-\\sum x_j\\\\ -\\sum x_j&amp;n\\end{array}\\right] \\left[\\begin{array}{c} \\sum Y_j\\\\ \\sum x_jY_j\\end{array}\\right]\\nonumber \\\\ &amp;= \\frac{1}{n\\sum x_j^{2}-(\\sum x_j)^{2}}\\left[\\begin{array}{c} \\sum Y_j\\sum x_j^{2}-\\sum x_j\\sum x_jY_j\\\\ n\\sum x_jY_j-\\sum x_j\\sum Y_j\\end{array}\\right]\\,. \\end{align}\\] We don’t usually work through the algebra in such detail; the matrix form is often sufficient for theoretical and numerical calculations and software, e.g. R, can be used. The precision of \\(\\hat{\\boldsymbol{\\beta}}\\) is measured via the variance-covariance matrix, given by \\[\\begin{align} \\textrm{Var}(\\hat{\\boldsymbol{\\beta}}) &amp; = \\textrm{Var}\\{(X^{\\textrm{T}}X)^{-1}X^{\\textrm{T}}\\boldsymbol{Y}\\}\\\\ &amp; =(X^{\\textrm{T}}X)^{-1}X^{\\textrm{T}}\\textrm{Var}(\\boldsymbol{Y})X(X^{\\textrm{T}}X)^{-1}\\\\ &amp; = (X^{\\textrm{T}}X)^{-1}\\sigma^{2}\\,, \\end{align}\\] where \\(\\boldsymbol{Y}\\sim N(X\\boldsymbol{\\beta},I_n\\sigma^{2})\\), where \\(I_n\\) is an \\(n\\times n\\) identity matrix. Hence, in our example, \\[\\begin{align*} \\textrm{Var}(\\hat{\\boldsymbol{\\beta}}) &amp; = \\frac{1}{n\\sum x_j^{2}-(\\sum x_j)^{2}}\\left[\\begin{array}{cc} \\sum x_j^{2}&amp;-\\sum x_j\\\\ -\\sum x_j&amp;n\\end{array}\\right]\\sigma^{2}\\\\ &amp; = \\left[\\begin{array}{cc} \\textrm{Var}(\\hat\\beta_{0})&amp;\\textrm{Cov}(\\hat\\beta_{0},\\hat\\beta_{1})\\\\ \\textrm{Cov}(\\hat\\beta_{0},\\hat\\beta_{1})&amp;\\textrm{Var}(\\hat\\beta_{1})\\end{array}\\right]\\,. \\end{align*}\\] For estimating the difference between treatments, we are interested in \\[\\begin{align*} \\textrm{Var}(\\hat{\\beta}_{1})&amp; = \\frac{n}{n\\sum x_j^{2}-(\\sum x_j)^{2}}\\sigma^{2}\\\\ &amp; = \\frac{n}{n^2 - (\\sum x_j)^2}\\sigma^{2}\\,, \\end{align*}\\] as \\(x_j=\\pm 1\\), therefore \\(x_j^2=1\\) for all \\(j=1,\\ldots,n\\), and hence \\(\\sum x_j^2=n\\). To achieve the most precise estimator, we need to minimise \\(\\textrm{Var}(\\hat{\\beta}_{1})\\) or equivalently minimise \\(|\\sum x_j|\\). This goal can achieve this through the choice of \\(x_{1},\\dots,x_{n}\\): as each \\(x_j\\) can only take one of two values, -1 or +1, this is equivalent to choosing the numbers of subjects assigned to treatment 1 and treatment 2; call these \\(n_{1}\\) and \\(n_{2}\\) respectively, with \\(n_{1}+n_{2}=n\\) It is obvious that \\(\\sum x_j = 0\\) if and only if \\(n_1=n_2\\). Therefore, assuming \\(n\\) is even, the optimal design has \\(n_{1}=\\frac{n}{2}\\) subjects assigned to treatment 1 and \\(n_{2}=\\frac{n}{2}\\) subjects assigned to treatment 2. For \\(n\\) odd, we choose \\(n_{1}=\\frac{n+1}{2}\\), \\(n_{2}=\\frac{n-1}{2}\\), or vice versa. Definition 1.3 We can assess different designs using their efficiency: \\[\\begin{equation} \\textrm{Eff}=\\frac{\\textrm{Var}(\\hat{\\beta}_{1}\\, |\\, d^{*})}{\\textrm{Var}(\\hat{\\beta}_{1}\\, |\\, d_{1})} \\tag{1.5} \\end{equation}\\] where \\(d_{1}\\) is a design we want to assess and \\(d^{*}\\) is the optimal design with smallest variance. Note that \\(0\\leq\\textrm{Eff}\\leq 1\\). In Figure 1.2 below, we plot this efficiency for Example 1.1, using different choices of \\(n_1\\). The total number of runs is fixed at \\(n = 100\\), and the function eff calculates the efficiency from Definition 1.3 for a design with \\(n_1\\) subjects assigned to treatment 1. Clearly, efficiency of 1 is achieved when \\(n_1 = n_2\\) (equal allocation of treatments 1 and 2). If \\(n_1=0\\) or \\(n_1 = 1\\), the efficiency is zero; we cannot estimate the difference between two treatments if we only allocate subjects to one of them. n &lt;- 100 eff &lt;- function(n1) 1 - ((2 * n1 - n) / n)^2 curve(eff, from = 0, to = n, ylab = &quot;Eff&quot;, xlab = expression(n[1])) Figure 1.2: Efficiencies for designs for Example 1.1 with different numbers, \\(n_1\\), of subjects assigned to treatment 1 when the total number of subjects is \\(n=100\\). 1.2 Aims of experimentation and some examples Some reasons experiments are performed: Treatment comparison (Chapters 2 and 3) compare several treatments (and choose the best) e.g. clinical trial, agricultural field trial Factor screening (Chapters 4, 5 and 6) many complex systems may involve a large number of (discrete) factors (controllable features) which of these factors have a substantive impact? (relatively) small experiments e.g. industrial experiments on manufacturing processes Response surface exploration (Chapter 7) detailed description of relationship between important (continuous) variables and response typically second order polynomial regression models larger experiments, often built up sequentially e.g. alcohol yields in a pharmaceutical experiments Optimisation (Chapter 7) finding settings of variables that lead to maximum or minimum response typically use response surface methods and sequential ``hill climbing’’ strategy 1.3 Some definitions Definition 1.4 The response \\(Y\\) is the outcome measured in an experiment; e.g. yield from a chemical process. The response from the \\(n\\) observations are denoted \\(Y_{1},\\dots,Y_{n}\\). Definition 1.5 Factors (discrete) or variables (continuous) are features which can be set or controlled in an experiment; \\(m\\) denotes the number of factors or variables under investigation. For discrete factors, we call the possible settings of the factor its levels. We denote by \\(x_{ij}\\) the value taken by factor or variable \\(i\\) in the \\(j\\)th run of the experiment (\\(i = 1, \\ldots, m\\); \\(j = 1, \\ldots, n\\)). Definition 1.6 The treatments or support points are the distinct combinations of factor or variable values in the experiment. Definition 1.7 An experimental unit is the basic element (material, animal, person, time unit, ) to which a treatment can be applied to produce a response. In Example 1.1 (comparing two treatments): Response \\(Y\\): Measured outcome, e.g. protein level or pain score in clinical trial, yield in an agricultural field trial. Factor \\(x\\): “treatment” applied Levels \\[ \\begin{array}{ll} \\textrm{treatment 1}&amp;x =-1\\\\ \\textrm{treatment 2}&amp;x =+1 \\end{array} \\] Treatment or support point: Two treatments or support points Experimental unit: Subject (person, animal, plot of land, ). 1.4 Principles of experimentation Three fundamental principles that need to be considered when designing an experiment are: replication randomisation stratification (blocking) 1.4.1 Replication Each treatment is applied to a number of experimental units, with the \\(j\\)th treatment replicated \\(r_{j}\\) times. This enables the estimation of the variances of treatment effect estimators; increasing the number of replications, or replicates, decreases the variance of estimators of treatment effects. (Note: proper replication involves independent application of the treatment to different experimental units, not just taking several measurements from the same unit). 1.4.2 Randomisation Randomisation should be applied to the allocation of treatments to units. Randomisation protects against bias; the effect of variables that are unknown and potentially uncontrolled or subjectivity in applying treatments. It also provides a formal basis for inference and statistical testing. For example, in a clinical trial to compare a new drug and a control random allocation protects against “unmeasured and uncontrollable” features (e.g. age, sex, health) bias resulting from the clinician giving new drug to patients who are sicker. Clinical trials are usually also double-blinded, i.e. neither the healthcare professional nor the patient knows which treatment the patient is receiving. 1.4.3 Stratification (or blocking) We would like to use a wide variety of experimental units (e.g. people or plots of land) to ensure coverage of our results, i.e. validity of our conclusions across the population of interest. However, if the sample of units from the population is too heterogenous, then this will induce too much random variability, i.e. increase \\(\\sigma^{2}\\) in \\(\\varepsilon_{j}\\sim N(0,\\sigma^{2})\\), and hence increase the variance of our parameter estimators. We can reduce this extraneous variation by splitting our units into homogenous sets, or blocks, and including a blocking term in the model. The simplest blocked experiment is a randomised complete block design, where each block contains enough units for all treatments to be applied. Comparisons can then be made within each block. Basic principle: block what you can, randomise what you cannot. Later we will look at blocking in more detail, and the principle of incomplete blocks. 1.5 Revision on the linear model Recall: \\(\\boldsymbol{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\), with \\(\\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0},I_n\\sigma^{2})\\). Let the \\(j\\)th row of \\(X\\) be denoted \\(\\boldsymbol{x}^\\textrm{T}_j\\), which holds the values of the predictors, or explanatory variables, for the \\(j\\)th observation. Then \\[\\begin{equation*} Y_j=\\boldsymbol{x}_j^{\\textrm{T}}\\boldsymbol{\\beta}+\\varepsilon_j\\,,\\quad j=1,\\ldots,n\\,. \\end{equation*}\\] For example, quite commonly, for continuous variables \\[ \\boldsymbol{x}_j=(1,x_{1j},x_{2j},\\dots,x_{mj})^{\\textrm{T}}\\,, \\] and so \\[ \\boldsymbol{x}_j^{\\textrm{T}}\\boldsymbol{\\beta}=\\beta_{0}+\\beta_{1}x_{1j}+\\dots+\\beta_{m}x_{mj}\\,. \\] The laest squares estimators are given by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}=(X^{\\textrm{T}}X)^{-1}X^{\\textrm{T}}\\boldsymbol{Y}\\,,\\nonumber \\end{equation}\\] with \\[\\begin{equation} \\textrm{Var}(\\hat{\\boldsymbol{\\beta}})=(X^{\\textrm{T}}X)^{-1}\\sigma^{2}\\,.\\nonumber \\end{equation}\\] 1.5.1 Variance of a Prediction/Fitted Value A prediction of the mean response at point \\(\\boldsymbol{x}_0\\) (which may or may not be in the design) is \\[ \\hat{Y}_0 = \\boldsymbol{x}_0^{\\textrm{T}}\\hat{\\boldsymbol{\\beta}}\\,, \\] with \\[\\begin{align*} \\textrm{Var}(\\hat{Y}_0) &amp; = \\textrm{Var}\\left(\\boldsymbol{x}_0^{\\textrm{T}}\\hat{\\boldsymbol{\\beta}}\\right) \\\\ &amp; = \\boldsymbol{x}_0^{\\textrm{T}}\\textrm{Var}(\\hat{\\boldsymbol{\\beta}})\\boldsymbol{x}_0 \\\\ &amp; = \\boldsymbol{x}_0^{\\textrm{T}}(X^{\\textrm{T}}X)^{-1}\\boldsymbol{x}_0\\sigma^{2}\\,. \\end{align*}\\] For a linear model, this variance depends only on the assumed regression model and the design (through \\(X\\)), the point at which prediction is to be made (\\(\\boldsymbol{x}_0\\)) and the value of \\(\\sigma^2\\); it does not depend on data \\(\\boldsymbol{Y}\\) or parameters \\(\\boldsymbol{\\beta}\\). Similarly, we can find the variance-covariance matrix of the fitted values: \\[ \\textrm{Var}(\\hat{Y})=\\textrm{Var}(X\\hat{\\boldsymbol{\\beta}})=X(X^{\\textrm{T}}X)^{-1}X^{\\textrm{T}}\\sigma^{2}\\,. \\] 1.5.2 Analysis of Variance and R\\(^{2}\\) as Model Comparison To assess the goodness-of-fit of a model, we can use the residual sum of squares \\[\\begin{align*} \\textrm{RSS} &amp; = (\\boldsymbol{Y} - X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}} (\\boldsymbol{Y} - X\\hat{\\boldsymbol{\\beta}})\\\\ &amp; = \\sum^{n}_{j=1}\\left\\{Y_{j}-\\boldsymbol{x}_{j}^{\\textrm{T}}\\hat{\\boldsymbol{\\beta}}\\right\\}^{2}\\\\ &amp; = \\sum^{n}_{j=1}r_{j}^{2}\\,, \\end{align*}\\] where \\[ r_{j}=Y_{j}-\\boldsymbol{x}_{j}^{\\textrm{T}}\\hat{\\boldsymbol{\\beta}}\\,. \\] Often, a comparison is made to the null model \\[ Y_{j}=\\beta_{0}+\\varepsilon_{j}\\,, \\] i.e. \\(Y_{i}\\sim N(\\beta_{0},\\sigma^{2})\\). The residual sum of squares for the null model is given by \\[ \\textrm{RSS}(\\textrm{null}) = \\boldsymbol{Y}^{\\textrm{T}}\\boldsymbol{Y} - m\\bar{Y}^{2}\\,, \\] as \\[ \\hat{\\beta}_{0} = \\bar{Y} = \\frac{1}{n}\\sum_{j=1}^n Y_{j}\\,. \\] How do we compare these models? Ratio of residual sum of squares: \\[\\begin{align*} R^{2} &amp; = 1 - \\frac{\\textrm{RSS}}{\\textrm{RSS}(\\textrm{null})} \\\\ &amp; = 1 - \\frac{(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}}(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})}{\\boldsymbol{Y}^{\\textrm{T}}\\boldsymbol{Y}-n\\bar{Y}^{2}}\\,. \\end{align*}\\] The quantity \\(0\\leq R^{2}\\leq 1\\) is sometimes called the coefficient of multiple determination: high \\(R^{2}\\) implies that the model describes much of the variation in the data; but note that \\(R^{2}\\) will always increase as \\(p\\) (the number of explanatory variables) increases, with \\(R^{2}=1\\) when \\(p=n\\); some software packages will report the adjusted \\(R^{2}\\). \\[\\begin{align*} R^{2}_{a} &amp; = 1-\\frac{\\textrm{RSS}/(n-p)}{\\textrm{RSS}(\\textrm{null})/(n-1)}\\\\ &amp; = 1 - \\frac{(\\boldsymbol{Y} - X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}} (\\boldsymbol{Y} - X\\hat{\\boldsymbol{\\beta}})/(n-p)}{(\\boldsymbol{Y}^{\\textrm{T}}\\boldsymbol{Y} - n\\bar{Y}^{2})/(n-1)}; \\end{align*}\\] \\(R_a^2\\) does not necessarily increase with \\(p\\) (as we divide by degrees of freedom to adjust for complexity of the model). Analysis of variance (ANOVA): An ANOVA table is compact way of presenting the results of (sequential) comparisons of nested models. You should be familiar with an ANOVA table of the following form. Table 1.1: A standard ANOVA table. Source Degress of Freedom (Sequential) Sum of Squares Mean Square Regression \\(p-1\\) By subtraction; see (1.6) Reg SS/\\((p-1)\\) Residual \\(n-p\\) \\((\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}}(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})\\)5 RSS/\\((n-p)\\) Total \\(n-1\\) \\(\\boldsymbol{Y}^{\\textrm{T}}\\boldsymbol{Y}-n\\bar{Y}^{2}\\)6 In row 1 of Table 1.1 above, \\[\\begin{align} \\textrm{Regression SS = Total SS $-$ RSS} &amp; = \\boldsymbol{Y}^{\\textrm{T}}\\boldsymbol{Y} - n\\bar{Y}^{2} - (\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}}(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})\\\\ &amp; = -n\\bar{Y}^{2}-\\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}(X^{\\textrm{T}}X)\\hat{\\boldsymbol{\\beta}}+2\\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}X^{\\textrm{T}}\\boldsymbol{Y} \\\\ &amp; = \\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}(X^{\\textrm{T}}X)\\hat{\\boldsymbol{\\beta}}-n\\bar{Y}^{2}\\,, \\tag{1.6} \\end{align}\\] with the last line following from \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}X^{\\textrm{T}}\\boldsymbol{Y} &amp; = \\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}(X^{\\textrm{T}}X)(X^{\\textrm{T}}X)^{-1}X^{\\textrm{T}}\\boldsymbol{Y} \\\\ &amp; = \\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}(X^{\\textrm{T}}X)\\hat{\\boldsymbol{\\beta}} \\end{align*}\\] This idea can be generalised to the comparison of a sequence of nested models - see Problem Sheet 1. Hypothesis testing is performed using the mean square: \\[\\begin{equation} \\frac{\\textrm{Regression SS}}{p-1}=\\frac{\\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}(X^{\\textrm{T}}X)\\hat{\\boldsymbol{\\beta}}-n\\bar{Y}^{2}}{p-1}\\,.\\nonumber \\end{equation}\\] Under \\(\\textrm{H}_{0}: \\beta_{1}=\\dots=\\beta_{p-1}=0\\) \\[\\begin{align*} \\frac{\\textrm{Regression SS}/(p-1)}{\\textrm{RSS}/(n-p)} &amp; = \\frac{(\\hat{\\boldsymbol{\\beta}}^{\\textrm{T}}(X^{\\textrm{T}}X)\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^{2})/(p-1)}{(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}}(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})/(n-p)}\\nonumber\\\\ &amp; \\sim F_{p-1,n-p}\\,, \\end{align*}\\] an \\(F\\) distribution with \\(p-1\\) and \\(n-p\\) degrees of freedom; defined via the ratio of two independent \\(\\chi^{2}\\) distributions. Also, \\[\\begin{equation*} \\frac{\\textrm{RSS}}{n-p}=\\frac{(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})^{\\textrm{T}}(\\boldsymbol{Y}-X\\hat{\\boldsymbol{\\beta}})}{n-p}=\\hat{\\sigma}^{2} \\end{equation*}\\] is an unbiased estimator for \\(\\sigma^{2}\\), and \\[\\begin{equation*} \\frac{(n-p)}{\\sigma^{2}}\\hat{\\sigma}^{2}\\sim\\chi^{2}_{n-p}\\,. \\end{equation*}\\] This is a Chi-squared distribution with \\(n-p\\) degrees of freedom (see MATH2010 or MATH6174 notes). 1.6 Exercises (Adapted from Morris, 2011) A classic and famous example of a simple hypothetical experiment was described by Fisher (1935): A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was added first to the cup. We will consider the problem of designing an experiment by means of which this assertion can be tested. For this purpose let us first lay down a simple form of experiment with a view to studying its limitations and its characteristics, both those that same essential to the experimental method, when well developed, and those that are not essential but auxiliary. Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgement in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is an order not determined arbitrarily by human choice, but by the actual manipulation of the physical appartatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling numbers purporting to give the actual results of such manipulation7. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received. Define the treatments in this experiment. Identify the units in this experiment. How might a “physical appartatus” from a “game of chance” be used to perform the randomisation. Explain one example. Suppose eight tea cups are available for this experiment but they are not identical. Instead they come from two sets. Foru are made from heavy, thick porcelain; four from much lighter china. If each cup can only be used once, how might this fact be incorporated into the design of the experiment? Solution There are two treatments in the experiment - the two ingredients “milk first” and “tea first”. The experimental units are the “cups of tea”, made up from the tea and milk used and also the cup itself. The simplest method here might be to select four black playing cards and four red playing cards, assign one treatment to each colour, shuffle the cards, and then draw them in order. The colour drawn indicates the treatment that should be used to make the next cup of tea. This operation would give one possible randomisation. We could of course also use R. sample(rep(c(&quot;Milk first&quot;, &quot;Tea first&quot;), c(4, 4)), size = 8, replace = F) ## [1] &quot;Tea first&quot; &quot;Tea first&quot; &quot;Tea first&quot; &quot;Milk first&quot; &quot;Tea first&quot; ## [6] &quot;Milk first&quot; &quot;Milk first&quot; &quot;Milk first&quot; Type of cup could be considered as a blocking factor. One way of incorporating it would be to split the experiment into two (blocks), each with four cups (two milk first, two tea first). We would still wish to randomise allocation of treatments to units within blocks. # block 1 sample(rep(c(&quot;Milk first&quot;, &quot;Tea first&quot;), c(2, 2)), size = 4, replace = F) ## [1] &quot;Tea first&quot; &quot;Tea first&quot; &quot;Milk first&quot; &quot;Milk first&quot; # block 2 sample(rep(c(&quot;Milk first&quot;, &quot;Tea first&quot;), c(2, 2)), size = 4, replace = F) ## [1] &quot;Milk first&quot; &quot;Milk first&quot; &quot;Tea first&quot; &quot;Tea first&quot; Consider the linear model \\[\\boldsymbol{y}= X\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon}\\,,\\] with \\(\\boldsymbol{y}\\) an \\(n\\times 1\\) vector of responses, \\(X\\) a \\(n\\times p\\) model matrix and \\(\\boldsymbol{\\varepsilon}\\) a \\(n\\times 1\\) vector of independent and identically distributed random variables with constant variance \\(\\sigma^2\\). Derive the least squares estimator \\(\\hat{\\boldsymbol{\\beta}}\\) for this multiple linear regression model, and show that this estimator is unbiased. Using the definition of (co)variance, show that \\[\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) = \\left(X^{\\mathrm{T}}X\\right)^{-1}\\sigma^2\\,.\\] If \\(\\boldsymbol{\\varepsilon}\\sim N (\\boldsymbol{0},I_n\\sigma^2)\\), with \\(I_n\\) being the \\(n\\times n\\) identity matrix, show that the maximum likelihood estimators for \\(\\boldsymbol{\\beta}\\) coincide with the least squares estimators. Solution The method of least squares minimises the sum of squared differences between the responses and the expected values, that is, minimises the expression \\[ (\\boldsymbol{y}-X\\boldsymbol{\\beta})^{\\mathrm{T}}(\\boldsymbol{y}-X\\boldsymbol{\\beta}) = \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- 2\\boldsymbol{\\beta}^{\\mathrm{T}}X^{\\mathrm{T}}\\boldsymbol{y}+ \\boldsymbol{\\beta}^{\\mathrm{T}}X^{\\mathrm{T}}X\\boldsymbol{\\beta}\\,. \\] Differentiating with respect to the vector \\(\\boldsymbol{\\beta}\\), we obtain \\[ \\frac{\\partial}{\\partial\\boldsymbol{\\beta}} = -2X^{\\mathrm{T}}\\boldsymbol{y}+ 2X^{\\mathrm{T}}X\\boldsymbol{\\beta}\\,. \\] Set equal to \\(\\boldsymbol{0}\\) and solve: \\[ X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} = X^{\\mathrm{T}}\\boldsymbol{y}\\Rightarrow \\hat{\\boldsymbol{\\beta}} = \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}\\boldsymbol{y}\\,. \\] The estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is unbiased: \\[ E(\\hat{\\boldsymbol{\\beta}}) = \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}E(\\boldsymbol{y}) = \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}X\\boldsymbol{\\beta}= \\boldsymbol{\\beta}\\,, \\] and has variance: \\[\\begin{align*} \\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) &amp; =E\\left\\{ \\left[\\hat{\\boldsymbol{\\beta}} - E(\\hat{\\boldsymbol{\\beta}})\\right] \\left[\\hat{\\boldsymbol{\\beta}} - E(\\hat{\\boldsymbol{\\beta}})\\right]^{\\mathrm{T}} \\right\\}\\\\ &amp; = E\\left\\{ \\left[\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}\\right] \\left[\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}\\right]^{\\mathrm{T}} \\right\\}\\\\ &amp; = E\\left\\{ \\hat{\\boldsymbol{\\beta}}\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}} - 2\\boldsymbol{\\beta}\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}} + \\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}} \\right\\}\\\\ &amp; = E\\left\\{ \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}\\boldsymbol{y}\\boldsymbol{y}^{\\mathrm{T}}X\\left(X^{\\mathrm{T}}X\\right)^{-1} - 2\\boldsymbol{\\beta}\\boldsymbol{y}^{\\mathrm{T}}X\\left(X^{\\mathrm{T}}X\\right)^{-1} + \\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}}\\right\\}\\\\ &amp; = \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}E(\\boldsymbol{y}\\boldsymbol{y}^{\\mathrm{T}})X\\left(X^{\\mathrm{T}}X\\right)^{-1} - 2\\boldsymbol{\\beta}E(\\boldsymbol{y}^{\\mathrm{T}})X\\left(X^{\\mathrm{T}}X\\right)^{-1} + \\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}}\\\\ &amp; = \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}\\left[\\mbox{Var}(\\boldsymbol{y}) + E(\\boldsymbol{y})E(\\boldsymbol{y}^{\\mathrm{T}})\\right]X\\left(X^{\\mathrm{T}}X\\right)^{-1} - 2\\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}}X^{\\mathrm{T}}X\\left(X^{\\mathrm{T}}X\\right)^{-1} + \\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}}\\\\ &amp; = \\left(X^{\\mathrm{T}}X\\right)^{-1}X^{\\mathrm{T}}\\left[I_N\\sigma^2 + X\\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}}X^{\\mathrm{T}}\\right]X\\left(X^{\\mathrm{T}}X\\right)^{-1} - \\boldsymbol{\\beta}\\boldsymbol{\\beta}^{\\mathrm{T}}\\\\ &amp; = \\left(X^{\\mathrm{T}}X\\right)^{-1}\\sigma^2\\,. \\end{align*}\\] As \\(\\boldsymbol{y}\\sim N\\left(X\\boldsymbol{\\beta}, I_N\\sigma^2\\right)\\), the likelihood is given by \\[ L(\\boldsymbol{\\beta}\\,; \\boldsymbol{y}) = \\left(2\\pi\\sigma^2\\right)^{-N/2}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(\\boldsymbol{y}- X\\boldsymbol{\\beta})^{\\mathrm{T}}(\\boldsymbol{y}- X\\boldsymbol{\\beta})\\right\\}\\,. \\] The log-likelihood is given by \\[ l(\\boldsymbol{\\beta}\\,;\\boldsymbol{y}) = -\\frac{1}{2\\sigma^2}(\\boldsymbol{y}- X\\boldsymbol{\\beta})^{\\mathrm{T}}(\\boldsymbol{y}- X\\boldsymbol{\\beta}) + \\mbox{constant}\\,. \\] Up to a constant, this expression is \\(-1\\times\\) the least squares equations; hence maximising the log-likelihood is equivalent to minimising the least squares equation. Consider the two nested linear models \\(Y_j = \\beta_0 + \\beta_1x_{1j} + \\beta_2x_{2j} + \\ldots + \\beta_{p_1}x_{p_1j} + \\varepsilon_j\\), or \\(\\boldsymbol{y}= X_1\\boldsymbol{\\beta}_1 + \\boldsymbol{\\varepsilon}\\), \\(Y_j = \\beta_0 + \\beta_1x_{1j} + \\beta_2x_{2j} + \\ldots + \\beta_{p_1}x_{p_1j} + \\beta_{p_1+1}x_{(p_1+1)j} + \\ldots + \\beta_{p_2}x_{p_2j} + \\varepsilon_j\\), or \\(\\boldsymbol{y}= X_1\\boldsymbol{\\beta}_1 + X_2\\boldsymbol{\\beta}_2+ \\boldsymbol{\\varepsilon}\\) with \\(\\varepsilon_j\\sim N(0, \\sigma^2)\\), and \\(\\varepsilon_{j}\\), \\(\\varepsilon_{k}\\) independent \\((\\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0},I_n\\sigma^2))\\). Construct an ANOVA table to compare model (ii) with the null model \\(Y_j=\\beta_0 + \\varepsilon_j\\). Extend this ANOVA table to compare models (i) and (ii) by further decomposing the regression sum of squares for model (ii). Hint: which residual sum of squares are you interested in to compare models (i) and (ii)? You should end up with an ANOVA table of the form Source Degrees of freedom Sums of squares Mean square Model (i) \\(p_1\\) ? ? Model (ii) \\(p_2\\) ? ? Residual \\(n-p_1-p_2-1\\) ? ? Total \\(n-1\\) \\(\\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- n\\bar{Y}^2\\) The second row of the table gives the extra sums of squares for the additional terms in fitting model (ii), over and above those in model (i). Calculate the extra sum of squares for fitting the terms in model (i), over and above those terms only in model (ii), i.e. those held in \\(X_2\\boldsymbol{\\beta}_2\\). Construct an ANOVA table containing both the extra sum of squares for the terms only in model (i) and the extra sum of squares for the terms only in model (ii). Comment on the table. Solution From lectures Source Degrees of freedom Sums of squares Mean square Regression \\(p_1+p_2\\) \\(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\right)/(p_1+p_2)\\) Residual \\(n-p_1-p_2-1\\) \\((\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})^{\\mathrm{T}}(\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})\\) RSS\\(/(n-p_1-p_2-1)\\) Total \\(n-1\\) \\(\\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- n\\bar{Y}^2\\) where \\[\\begin{align*} \\mbox{RSS(null) - RSS(ii)} &amp; = \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- n\\bar{Y}^2 - (\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})^{\\mathrm{T}}(\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})\\\\ &amp; = \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- n\\bar{Y}^2 - \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}+ 2\\boldsymbol{y}^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}}\\\\ &amp; = 2\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\\\ &amp; = \\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\,. \\end{align*}\\] To compare model (i) with the null model, we compute \\[\\begin{align*} \\mbox{RSS(null) - RSS(i)} &amp; = \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- N\\bar{Y}^2 - (\\boldsymbol{y}- X_1\\hat{\\boldsymbol{\\beta}}_1)^{\\mathrm{T}}(\\boldsymbol{y}- X_1\\hat{\\boldsymbol{\\beta}}_1)\\\\ &amp; = \\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1 - n\\bar{Y}^2\\,. \\end{align*}\\] To compare models (i) and (ii), we compare them both to the null model, and look at the difference between these comparisons: \\[\\begin{align*} \\mbox{[RSS(null) - RSS(ii)] - [RSS(null) - RSS(i)]} &amp; = \\mbox{RSS(i) - RSS(ii)}\\\\ &amp; = (\\boldsymbol{y}- X_1\\hat{\\boldsymbol{\\beta}}_1)^{\\mathrm{T}}(\\boldsymbol{y}- X_1\\hat{\\boldsymbol{\\beta}}_1) - (\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})^{\\mathrm{T}}(\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})\\\\ &amp; = \\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1\\,. \\end{align*}\\] Source Degrees of freedom Sums of squares Mean square Regression \\(p_1+p_2\\) \\(\\hat{\\boldsymbol{\\beta}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\right)/(p_1+p_2)\\) Model (i) \\(p_1\\) \\(\\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1 - n\\bar{Y}^2\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1 - n\\bar{Y}^2\\right)/p_1\\) Extra due to Model (ii) \\(p_2\\) \\(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1\\right)/p_2\\) Residual \\(n-p_1-p_2-1\\) \\((\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})^{\\mathrm{T}}(\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})\\) RSS\\(/(n-p_1-p_2-1)\\) Total \\(n-1\\) \\(\\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- n\\bar{Y}^2\\) By definition, the Model (i) SS and the Extra SS for Model (ii) sum to the Regression SS. The extra sum of squares for the terms in model (i) over and above those in model (ii) are obtained through comparison of the models ia. \\(\\boldsymbol{y}= X_2\\boldsymbol{\\beta}_2 + \\boldsymbol{\\varepsilon}\\), iia. \\(\\boldsymbol{y}= X_1\\boldsymbol{\\beta}_1 + X_2\\boldsymbol{\\beta}_2+ \\boldsymbol{\\varepsilon}= X\\boldsymbol{\\beta}+ \\varepsilon\\) Extra sum of squares for model (iia): \\[\\begin{align*} \\mbox{[RSS(null) - RSS(iia)] - [RSS(null) - RSS(ia)]} &amp; = \\mbox{RSS(ia) - RSS(iia)}\\\\ &amp; = (\\boldsymbol{y}- X_2\\hat{\\boldsymbol{\\beta}}_2)^{\\mathrm{T}}(\\boldsymbol{y}- X_2\\hat{\\boldsymbol{\\beta}}_2) - (\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})^{\\mathrm{T}}(\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})\\\\ &amp; = \\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_2^{\\mathrm{T}}X_2^{\\mathrm{T}}X_2\\hat{\\boldsymbol{\\beta}}_2\\,. \\end{align*}\\] Hence, an ANOVA table for the extra sums of squares is given by Source Degrees of freedom Sums of squares Mean square Regression \\(p_1+p_2\\) \\(\\hat{\\boldsymbol{\\beta}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - n\\bar{Y}^2\\right)/(p_1+p_2)\\) Extra Model (i) \\(p_1\\) \\(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_2^{\\mathrm{T}}X_2^{\\mathrm{T}}X_2\\hat{\\boldsymbol{\\beta}}_2\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_2^{\\mathrm{T}}X_2^{\\mathrm{T}}X_2\\hat{\\boldsymbol{\\beta}}_2\\right)/p_1\\) Extra Model (ii) \\(p_2\\) \\(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1\\) \\(\\left(\\hat{\\boldsymbol{\\beta}}^{\\mathrm{T}}X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_1^{\\mathrm{T}}X_1^{\\mathrm{T}}X_1\\hat{\\boldsymbol{\\beta}}_1\\right)/p_2\\) Residual \\(n-p_1-p_2-1\\) \\((\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})^{\\mathrm{T}}(\\boldsymbol{y}- X\\hat{\\boldsymbol{\\beta}})\\) RSS\\(/(n-p_1-p_2-1)\\) Total \\(n-1\\) \\(\\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}- n\\bar{Y}^2\\) Note that for these adjusted sums of squares, in general the extra sum of squares for model (i) and (ii) do not sum to the regression sum of squares. This will only be the case if the columns of \\(X_1\\) and \\(X_2\\) are mutually orthogonal, i.e. \\(X_1^{\\mathrm{T}}X_2 = \\boldsymbol{0}\\). References "],["crd.html", "Chapter 2 Completely randomised designs 2.1 A unit-treatment linear model 2.2 The partitioned linear model 2.3 Reduced normal equations for the CRD 2.4 Contrasts 2.5 Treatment contrast estimators in the CRD 2.6 Analysing CRDs in R 2.7 Multiple comparisons 2.8 Exercises", " Chapter 2 Completely randomised designs The simplest form of experiment we will consider compares \\(t\\) different unstructured treatments. By unstructured, we mean the treatments form a discrete collection, not related through the settings of other experimental features (compare with factorial experiments in Chapter 4). We also make the assumption that there are no restrictions in the randomisation of treatments to experimental units (compare with Chapter 3 on blocking). A designs for such an experiment is therefore called a completely randomised design (CRD). Example 2.1 Pulp experiment (Wu and Hamada, 2009, ch. 2) In a paper pulping mill, an experiment was run to examine differences between the reflectance (brightness; ratio of amount of light leaving a target to the amount of light striking the target) of sheets of pulp made by \\(t=4\\) operators. The data are given in Table 2.1 below. pulp &lt;- data.frame(operator = rep(factor(1:4), 5), repetition = rep(1:5, rep(4, 5)), reflectance = c(59.8, 59.8, 60.7, 61.0, 60.0, 60.2, 60.7, 60.8, 60.8, 60.4, 60.5, 60.6, 60.8, 59.9, 60.9, 60.5, 59.8, 60.0, 60.3, 60.5) ) knitr::kable( tidyr::pivot_wider(pulp, names_from = operator, values_from = reflectance)[, -1], col.names = paste(&quot;Operator&quot;, 1:4), caption = &quot;Pulp experiment: reflectance values (unitless) from four different operators.&quot; ) Table 2.1: Pulp experiment: reflectance values (unitless) from four different operators. Operator 1 Operator 2 Operator 3 Operator 4 59.8 59.8 60.7 61.0 60.0 60.2 60.7 60.8 60.8 60.4 60.5 60.6 60.8 59.9 60.9 60.5 59.8 60.0 60.3 60.5 The experiment has one factor (operator) with four levels (sometimes called a one-way layout). The CRD employed has equal replication of each treatment (operator). We can informally compare the responses from these four treatments graphically. boxplot(reflectance ~ operator, data = pulp) Figure 2.1: Pulp experiments: distributions of reflectance from the four operators. Figure 2.1 shows that, relative to the variation, there may be a difference in the mean response between treatments 1 and 2, and 3 and 4. In this chapter, we will see how to make this comparison formally using linear models, and to assess how the choice of design impacts our results. Throughout this chapter we will assume the \\(i\\)th treatment is applied to \\(n_i\\) experimental unit, with total number of runs \\(n = \\sum_{i=1}^t n_i\\) in the experiment. 2.1 A unit-treatment linear model An appropriate, and common, model to describe data from such experiments when the response is continuous is given by \\[\\begin{equation} y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}\\,, \\quad i = 1, \\ldots, t; j = 1, \\ldots, n_i\\,, \\tag{2.1} \\end{equation}\\] where \\(y_{ij}\\) is the response from the \\(j\\)th application of treatment \\(i\\), \\(\\mu\\) is a constant parameter, \\(\\tau_i\\) is the effect of the \\(i\\)th treatment, and \\(\\varepsilon_{ij}\\) is the random individual effect from each experimental unit with \\(E(\\varepsilon_{ij})\\) and \\(\\mathrm{Var}(\\varepsilon_{ij}) = \\sigma^2\\). All random errors are assumed independent and here we also assume \\(\\varepsilon_{ij} \\sim N(0, \\sigma^2)\\). Model (2.1) assumes that each treatment can be randomly allocated to one of the \\(n\\) experimental units, and that this allocation is independent of the allocation of all the other treatments. Why is this model appropriate and commonly used? The expected response from the application of the \\(i\\)th treatment is \\[ E(y_{ij}) = \\mu + \\tau_i\\,. \\] The parameter \\(\\mu\\) can be thought of as representing the impact of many different features particular to this experiment, and \\(\\tau_i\\) is the deviation due to applying treatment \\(i\\). From the applicable of two different hypothetical experiments, A and B, the expected response from treatment \\(i\\) may be different due to a different overall mean. From experiment A: \\[ E(y_{ij}) = \\mu_{\\mathrm{A}} + \\tau_i\\,. \\] From experiment B: \\[ E(y_{ij}) = \\mu_{\\mathrm{B}} + \\tau_i\\,. \\] But the difference between treatments \\(k\\) and \\(l\\) (\\(k, l = 1,\\ldots, t\\)) \\[\\begin{align*} E(y_{kj}) - E(y_{lj}) &amp; = \\mu_A + \\tau_k - \\mu_A - \\tau_l \\\\ &amp; = \\tau_k - \\tau_l\\,, \\end{align*}\\] is constant across different experiments. This concept of comparison underpins most design of experiments, and will be applied throughout this module. 2.2 The partitioned linear model In matrix form, we can write model (2.1) as \\[ \\boldsymbol{y}= X_1\\mu + X_2\\boldsymbol{\\tau}+ \\boldsymbol{\\varepsilon}\\,, \\] where \\(X_1 = \\boldsymbol{1}_n\\), the \\(n\\)-vector with every entry equal to one, \\[ X_2 = \\begin{bmatrix} \\boldsymbol{1}_{n_1} &amp; \\boldsymbol{0}_{n_1} &amp; \\cdots &amp; \\boldsymbol{0}_{n_1} \\\\ \\boldsymbol{0}_{n_2} &amp; \\boldsymbol{1}_{n_2} &amp; \\cdots &amp; \\boldsymbol{0}_{n_2} \\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots \\\\ \\boldsymbol{0}_{n_t} &amp; \\boldsymbol{0}_{n_t} &amp; \\cdots &amp; \\boldsymbol{1}_{n_t} \\\\ \\end{bmatrix}\\,, \\] with \\(\\boldsymbol{0}_{n_i}\\) is the \\(n_i\\)-vector with every entry equal to zero, \\(\\boldsymbol{\\tau}= [\\tau_1, \\ldots, \\tau_t]^{\\mathrm{T}}\\) and \\(\\boldsymbol{\\varepsilon}= [\\varepsilon_{11}, \\ldots, \\varepsilon_{tn_t}]^{\\mathrm{T}}\\). Why are we partitioning the model? Going back to our discussion of the role of \\(\\mu\\) and \\(\\tau_i\\), it is clear that we not interested in estimating \\(\\mu\\), which represents an experiment-specific contribution to the expected mean. Our only interest is in estimating the (differences between the) \\(\\tau_i\\). Hence, we can treat \\(\\mu\\) as a nuisance parameter. If we define \\(X = [X_1\\, \\vert\\, X_2]\\) and \\(\\boldsymbol{\\beta}^{\\mathrm{T}} = [\\mu \\vert \\boldsymbol{\\tau}^{\\mathrm{T}}]\\), we can write the usual least squares equations \\[\\begin{equation} X^{\\mathrm{T}}X\\hat{\\boldsymbol{\\beta}} = X^{\\mathrm{T}}\\boldsymbol{y} \\tag{2.2} \\end{equation}\\] as a system of two matrix equations \\[\\begin{align*} X_1^{\\mathrm{T}}X_1\\hat{\\mu} + X_1^{\\mathrm{T}}X_2\\hat{\\boldsymbol{\\tau}} &amp; = X_1^{\\mathrm{T}}\\boldsymbol{y}\\\\ X_2^{\\mathrm{T}}X_1\\hat{\\mu} + X_2^{\\mathrm{T}}X_2\\hat{\\boldsymbol{\\tau}} &amp; = X_2^{\\mathrm{T}}\\boldsymbol{y}\\,. \\\\ \\end{align*}\\] Assuming \\((X_1^{\\mathrm{T}}X_1)^{-1}\\) exists, which it does in this case, we can pre-multiply the first of these equations by \\(X_2^{\\mathrm{T}}X_1(X_1^{\\mathrm{T}}X_1)^{-1}\\) and subtract it from the second equation to obtain \\[\\begin{align*} X_2^{\\mathrm{T}}[I_n - X_1(X_1^{\\mathrm{T}}X_1)^{-1}X_1^{\\mathrm{T}}]X_1\\hat{\\mu} &amp; + X_2^{\\mathrm{T}}[I_n - X_1(X_1^{\\mathrm{T}}X_1)^{-1}X_1^{\\mathrm{T}}]X_2\\hat{\\boldsymbol{\\tau}} \\\\ &amp; = X_2^{\\mathrm{T}}[I_n - X_1(X_1^{\\mathrm{T}}X_1)^{-1}X_1^{\\mathrm{T}}]\\boldsymbol{y}\\,. \\end{align*}\\] Writing \\(H_1 = X_1(X_1^{\\mathrm{T}}X_1)^{-1}X_1^{\\mathrm{T}}\\), we obtain \\[\\begin{equation} X_2^{\\mathrm{T}}[I_n - H_1]X_1\\hat{\\mu} + X_2^{\\mathrm{T}}[I_n - H_1]X_2\\hat{\\boldsymbol{\\tau}} = X_2^{\\mathrm{T}}[I_n - H_1]\\boldsymbol{y}\\,. \\tag{2.3} \\end{equation}\\] The matrix \\(H_1\\) is a “hat” matrix for a linear model containing only the term \\(\\mu\\), and hence \\(H_1X_1 = X_1\\) (see MATH2010 or STAT6123). Hence the first term in (2.3) is zero, and we obtain the reduced normal equations for \\(\\boldsymbol{\\tau}\\): \\[\\begin{equation} X_2^{\\mathrm{T}}[I_n - H_1]X_2\\hat{\\boldsymbol{\\tau}} = X_2^{\\mathrm{T}}[I_n - H_1]\\boldsymbol{y}\\,. \\tag{2.4} \\end{equation}\\] Note that the solutions from (2.4) are not different from the solution to \\(\\hat{\\boldsymbol{\\tau}}\\) that would be obtained from solving (2.2); equation (2.4) is simply a re-expression, where we have eliminated the nuisance parameter \\(\\mu\\). This fact means that we rarely need to solve (2.4) explicitly. Recalling that for a hat matrix, \\(I_n - H_1\\) is idempotent and symmetric (see MATH2010 or MATH6174), if we define \\[ X_{2|1} = (I_n - H_1)X_2\\,, \\] then we can rewrite equation (2.4) as \\[\\begin{equation} X_{2|1}^{\\mathrm{T}}X_{2|1}\\hat{\\boldsymbol{\\tau}} = X_{2|1}^{\\mathrm{T}}\\boldsymbol{y}\\,, \\tag{2.5} \\end{equation}\\] which are the normal equations for a linear model with expectation \\(E(\\boldsymbol{y}) = X_{2|1}\\boldsymbol{\\tau}\\). 2.3 Reduced normal equations for the CRD For the CRD discussed in this chapter, \\(X_1^{\\mathrm{T}}X_1 = n\\), the total number of runs in the experiment8. Hence \\((X_1^{\\mathrm{T}}X_1)^{-1} = 1/n\\) and \\(H_1 = \\frac{1}{n}J_n\\), with \\(J_n\\) the \\(n\\times n\\) matrix with all entries equal to 1. The adjusted model matrix then has the form \\[\\begin{align*} X_{2|1} &amp; = (I_n - H_1)X_2 \\\\ &amp; = X_2 - \\frac{1}{n}J_nX_2 \\\\ &amp; = X_2 - \\frac{1}{n}[n_1\\boldsymbol{1}_n \\vert \\cdots \\vert n_t\\boldsymbol{1}_n]\\,. \\end{align*}\\] That is, every column of \\(X_2\\) has been adjusted by the subtraction of the column mean from each entry9. Also notice that each row of \\(X_{2|1}\\) has a row-sum equal to zero (\\(= 1 - \\sum_{i=1}^tn_t/n\\)). Hence, \\(X_{2|1}\\) is not of full column rank, and so the reduced normal equations do not have a unique solution10. In MATH2010 and STAT6123 we fitted models with categorical variables by defining a set of dummy variables and estimating a reduced model. Here, we will take a slightly different approach and study which combinations of parameters from (2.1) are estimable, and in particular which linear combinations of the treatment parameters \\(\\tau_i\\) we can estimate. Let’s study equation (2.5) in more detail. We have \\[\\begin{align*} X^{\\mathrm{T}}_{2|1}X_{2|1} &amp; = X_2^{\\mathrm{T}}(I_n - H_1)X_2 \\\\ &amp; = X_2^{\\mathrm{T}}X_2 - X_2^{\\mathrm{T}}H_1X_2 \\\\ &amp; = \\mathrm{diag}(\\boldsymbol{n}) - \\frac{1}{n}X_2^{\\mathrm{T}}J_nX_2 \\\\ &amp; = \\mathrm{diag}(\\boldsymbol{n}) - \\frac{1}{n}\\boldsymbol{n}\\boldsymbol{n}^{\\mathrm{T}}\\,, \\end{align*}\\] where \\(\\boldsymbol{n}^{\\mathrm{T}} = (n_1, \\ldots, n_t)\\). Hence, the reduced normal equations become \\[\\begin{align} \\left[\\mathrm{diag}(\\boldsymbol{n}) - \\frac{1}{n}\\boldsymbol{n}^{\\mathrm{T}}\\boldsymbol{n}\\right]\\hat{\\boldsymbol{\\tau}} &amp; = X_2^{\\mathrm{T}}\\boldsymbol{y}- \\frac{1}{n}X_2^{\\mathrm{T}}J_n\\boldsymbol{y}\\\\ &amp; = X_2^{\\mathrm{T}}\\boldsymbol{y}- \\boldsymbol{n}\\bar{y}_{..}\\,, \\tag{2.6} \\end{align}\\] where \\(\\bar{y}_{..} = \\frac{1}{n}\\sum_{i = 1}^t\\sum_{j = 1}^{n_i} y_{ij}\\), i.e. the overall average of the observations from the experiment. From (2.6) we obtain a system of \\(t\\) equations, each having the form \\[\\begin{equation} \\hat{\\tau}_i - \\hat{\\tau}_w = \\bar{y}_{i.} - \\bar{y}_{..}\\,, \\tag{2.7} \\end{equation}\\] where \\(\\hat{\\tau}_w = \\frac{1}{n}\\sum_{i=1}^tn_i\\hat{\\tau}_i\\) and \\(\\bar{y}_{i.} = \\frac{1}{n_i}\\sum_{j = 1}^{n_i}y_{ij}\\) \\((i = 1, \\ldots, t)\\). These \\(t\\) equations are not independent; when multiplied by the \\(n_i\\), they sum to zero due to the linear dependency between the columns of \\(X_{2|1}\\). Hence, there is no unique solution to \\(\\hat{\\boldsymbol{\\tau}}\\) from equation (2.6). However, we can estimate certain linear combinations of the \\(\\tau_i\\), called contrasts. 2.4 Contrasts Definition 2.1 A treatment contrast is a linear combination \\(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}\\) with coefficient vector \\(\\boldsymbol{c}^{\\mathrm{T}} = (c_1,\\ldots, c_t)\\) such that \\(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{1} = 0\\); that is, \\(\\sum_{i = 1}^t c_i = 0\\). For example, assume we have \\(t = 3\\) treatments, then the following vectors \\(\\boldsymbol{c}\\) all define contrasts: \\(\\boldsymbol{c}^{\\mathrm{T}} = (1, -1, 0)\\), \\(\\boldsymbol{c}^{\\mathrm{T}} = (1, 0, -1)\\), \\(\\boldsymbol{c}^{\\mathrm{T}} = (0, 1, -1)\\). In fact, they define all \\({3\\choose 2} = 3\\) pairwise comparisons between treatments. The following are also contrasts: \\(\\boldsymbol{c}^{\\mathrm{T}} = (2, -1, -1)\\), \\(\\boldsymbol{c}^{\\mathrm{T}} = (0.5, -1, 0.5)\\), each comparing the sum, or average, of expected responses from two treatments to the expected response from the remaining treatment. The following are not contrasts, as \\(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{1} \\ne 0\\): \\(\\boldsymbol{c}^{\\mathrm{T}} = (2, -1, 0)\\), \\(\\boldsymbol{c}^{\\mathrm{T}} = (1, 0, 0)\\), with the final example once again demonstrating that we cannot estimate the individual \\(\\tau_i\\). 2.5 Treatment contrast estimators in the CRD We estimate a treatment contrast \\(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}\\) in the CRD via linear combinations of equations (2.7): \\[\\begin{align*} &amp; \\sum_{i=1}^t c_i\\hat{\\tau}_i - \\sum_{i=1}^tc_i\\hat{\\tau}_w = \\sum_{i=1}^tc_i\\bar{y}_{i.} - \\sum_{i=1}^tc_i\\bar{y}_{..} \\\\ \\Rightarrow &amp; \\sum_{i=1}^t c_i\\hat{\\tau}_i = \\sum_{i=1}^tc_i\\bar{y}_{i.}\\,, \\end{align*}\\] as \\(\\sum_{i=1}^tc_i\\hat{\\tau}_w = \\sum_{i=1}^tc_i\\bar{y}_{..} = 0\\), as \\(\\sum_{i=1}^tc_i = 0\\). Hence, the unique estimator of the contrast \\(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}\\) has the form \\[ \\widehat{\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}} = \\sum_{i=1}^tc_i\\bar{y}_{i.}\\,. \\] That is, we estimate the contrast in the treatment effects by the corresponding contrast in the treatment means. The variance of this estimator is straightforward to obtain: \\[\\begin{align*} \\mathrm{var}\\left(\\widehat{\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}}\\right) &amp; = \\sum_{i=1}^tc_i^2\\mathrm{var}(\\bar{y}_{i.}) \\\\ &amp; = \\sigma^2\\sum_{i=1}^tc_i^2/n_i\\,, \\end{align*}\\] as, under our model assumptions, each \\(\\bar{y}_{i.}\\) is an average of independent observations with variance \\(\\sigma^2\\). Similarly, from model (2.1) we can derive the distribution of \\(\\widehat{\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}}\\) as \\[ \\widehat{\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}} \\sim N\\left(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}, \\sigma^2\\sum_{i=1}^tc_i^2/n_i\\right)\\,. \\] Confidence intervals and hypothesis tests for \\(\\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau}\\) can be constructed/conducted using this distribution, e.g. a \\(100(1-\\frac{\\alpha}{2})\\)% confidence interval: \\[ \\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau} \\in \\sum_{i=1}^tc_i\\bar{y}_{i.} \\pm t_{n-t, 1-\\frac{\\alpha}{2}}s\\sqrt{\\sum_{i=1}^tc_i^2/n_i}\\,, \\] where \\(t_{n-t, 1-\\frac{\\alpha}{2}}\\) is the \\(1-\\frac{\\alpha}{2}\\) quantile of a \\(t\\)-distribution with \\(n-t\\) degrees of freedom and \\(s^2 = \\frac{1}{n-t}\\sum_{i=1}^t\\sum_{j=1}^{n_i}(y_{ij} - \\bar{y}_{i.})^2\\) is the estimate of \\(\\sigma^2\\). the hypothesis \\(H_0: \\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau} = 0\\) against the two-sided alternative \\(H_0: \\boldsymbol{c}^{\\mathrm{T}}\\boldsymbol{\\tau} \\ne 0\\) is rejected using a test of with confidence level \\(1-\\alpha/2\\) if \\[ \\frac{|\\sum_{i=1}^tc_i\\bar{y}_{i.}|}{s\\sqrt{\\sum_{i=1}^tc_i^2/n_i}} &gt; t_{n-t, 1-\\frac{\\alpha}{2}}\\,. \\] 2.6 Analysing CRDs in R Let’s return to Example 2.1. knitr::kable( tidyr::pivot_wider(pulp, names_from = operator, values_from = reflectance)[, -1], col.names = paste(&quot;Operator&quot;, 1:4), caption = &quot;Pulp experiment: reflectance values (unitless) from four different operators.&quot; ) Table 2.2: Pulp experiment: reflectance values (unitless) from four different operators. Operator 1 Operator 2 Operator 3 Operator 4 59.8 59.8 60.7 61.0 60.0 60.2 60.7 60.8 60.8 60.4 60.5 60.6 60.8 59.9 60.9 60.5 59.8 60.0 60.3 60.5 Clearly, we could directly calculate, and then compare, mean responses for each operator. However, there are (at least) two other ways we can proceed which use the fact we are fitting a linear model. These will be useful when we consider more complex models. Using pairwise.t.test. with(pulp, pairwise.t.test(reflectance, operator, p.adjust.method = &#39;none&#39;)) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: reflectance and operator ## ## 1 2 3 ## 2 0.396 - - ## 3 0.084 0.015 - ## 4 0.049 0.008 0.775 ## ## P value adjustment method: none This function performs hypothesis tests for all pairwise treatment comparisons (with a default confidence level of 0.95). Here we can see that operators 1 and 4, 2 and 3, and 2 and 4 have statistically significant differences. Using lm and the emmeans package. pulp.lm &lt;- lm(reflectance ~ operator, data = pulp) anova(pulp.lm) ## Analysis of Variance Table ## ## Response: reflectance ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## operator 3 1.34 0.447 4.2 0.023 * ## Residuals 16 1.70 0.106 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 pulp.emm &lt;- emmeans::emmeans(pulp.lm, ~ operator) pairs(pulp.emm, adjust = &#39;none&#39;) ## contrast estimate SE df t.ratio p.value ## 1 - 2 0.18 0.206 16 0.873 0.3960 ## 1 - 3 -0.38 0.206 16 -1.843 0.0840 ## 1 - 4 -0.44 0.206 16 -2.134 0.0490 ## 2 - 3 -0.56 0.206 16 -2.716 0.0150 ## 2 - 4 -0.62 0.206 16 -3.007 0.0080 ## 3 - 4 -0.06 0.206 16 -0.291 0.7750 Here, we have first fitted the linear model object. The lm function, by default, will have set up dummy variables with the first treatment (operator) as a baseline (see MATH2010 or STAT6123). We then take the intermediate step of calculating the ANOVA table for this experiment, and use an F-test to compare the model accounting for operator differences to the null model; there are differences between operators at the 5% significance level, The choice of dummy variables in the linear model is unimportant; any set could be used11, as in the next line we use the emmeans function (from the package of the same name) to specify that we are interested in estimating contrasts in the factor operator (which specifies our treatments in this experiment). Finally, the pairs command performs hypothesis tests for all pairwise comparisons between the four treatments. The results are the same as those obtained from using pairwise.t.test. Our preferred approach is using method 2 (lm and emmeans), for four main reasons: The function contrasts in the emmeans package can be used to estimate arbitrary treatment contrasts (see help(\"contrast-methods\")). # same as `pairs` above emmeans::contrast(pulp.emm, &quot;pairwise&quot;, adjust = &quot;none&quot;) ## contrast estimate SE df t.ratio p.value ## 1 - 2 0.18 0.206 16 0.873 0.3960 ## 1 - 3 -0.38 0.206 16 -1.843 0.0840 ## 1 - 4 -0.44 0.206 16 -2.134 0.0490 ## 2 - 3 -0.56 0.206 16 -2.716 0.0150 ## 2 - 4 -0.62 0.206 16 -3.007 0.0080 ## 3 - 4 -0.06 0.206 16 -0.291 0.7750 # estimating single contrast c = (1, -.5, -.5) # comparing operator 1 with operators 2 and 3 contrast1v23.emmc &lt;- function(levs) data.frame(&#39;t1 v avg t2 t3&#39; = c(1, -.5, -.5, 0)) emmeans::contrast(pulp.emm, &#39;contrast1v23&#39;) ## contrast estimate SE df t.ratio p.value ## t1.v.avg.t2.t3 -0.1 0.178 16 -0.560 0.5830 It more easily generalises to the more complicated models we will see in Chapter 3. It explicitly acknowledges that we have fitted a linear model, and so encourages us to check the model assumptions (see Exercise 3). It is straightfoward to apply adjustments for multiple comparisons. 2.7 Multiple comparisons When we perform hypothesis testing, we choose the critical region (i.e. the rule that decides if we reject \\(H_0\\)) to control the probability of a type I error; that is, we control the probability of incorrectly rejecting \\(H_0\\). If we need to test multiple hypotheses, e.g. to test all pairwise differences, we need to consider the overall probability of incorrectly rejecting one or more null hypothesis. This is called the experiment-wise or family-wise error rate. For Example 2.1, there are \\({4 \\choose 2} = 6\\) pairwise comparisons. Under the assumption that all tests are independent12, assuming each individual test has type I error 0.05, the experiment-wise type I error rate is given by: alpha &lt;- 0.05 1 - (1 - alpha)^6 ## [1] 0.265 An experiment-wise error rate of 0.265 is substantially greater than 0.05. Hence, we would expect to make many more type I errors than may be desirable. xkcd has a fun example: alpha &lt;- 0.05 1 - (1 - alpha)^20 ## [1] 0.642 Therefore it is usually desirable to maintain some control of the experiment-wise type I error rate. We will consider two methods. The Bonferroni method. An upper bound on the experiment-wise type I error rate for testing \\(k\\) hypotheses can be shown to be \\[\\begin{align*} P(\\mbox{wrongly reject at least one of } H_{0}^1, \\ldots, H_{0}^k) = &amp; P\\left(\\bigcup_{i=1}^{k}\\{\\mbox{wrongly reject } H_{0}^i\\}\\right) \\\\ &amp; \\leq \\sum_{i=1}^{k}\\underbrace{P(\\mbox{wrongly reject } H_{0}^i)}_{\\leq \\alpha} \\\\ &amp; \\leq k\\alpha\\,. \\end{align*}\\] Hence a conservative13 adjustment for multiple comparisons is to test each hypothesis at size \\(\\alpha / k\\), i.e. for the CRD compare to the quantile \\(t_{n-t, 1-\\frac{\\alpha}{2k}}\\) (or multiply each individual p-value by \\(k\\)). For Example 2.1, we can test all pairwise comparisons, each at size \\(\\alpha/k\\) using the adjustment argument in pairs. pairs(pulp.emm, adjust = &#39;bonferroni&#39;) ## contrast estimate SE df t.ratio p.value ## 1 - 2 0.18 0.206 16 0.873 1.0000 ## 1 - 3 -0.38 0.206 16 -1.843 0.5030 ## 1 - 4 -0.44 0.206 16 -2.134 0.2920 ## 2 - 3 -0.56 0.206 16 -2.716 0.0920 ## 2 - 4 -0.62 0.206 16 -3.007 0.0500 ## 3 - 4 -0.06 0.206 16 -0.291 1.0000 ## ## P value adjustment: bonferroni method for 6 tests Now, only one comparison is significant at an experiment-wise type I error rate of \\(\\alpha = 0.05\\) (operators 2 and 4). Tukey’s method. An alternative approach that gives an exact experiment-wise error rate of \\(100\\alpha\\)% compares the \\(t\\) statistic to a critical value from the studentised range distribution14, given by \\(\\frac{1}{\\sqrt{2}}q_{t, n-t, 1-\\frac{\\alpha}{2}}\\) with \\(q_{t, n-t, 1-\\frac{\\alpha}{2}}\\) the \\(1-\\frac{\\alpha}{2}\\) quantile from the studentised range distribution (available in R as qtukey). For Example 2.1: pairs(pulp.emm) ## contrast estimate SE df t.ratio p.value ## 1 - 2 0.18 0.206 16 0.873 0.8190 ## 1 - 3 -0.38 0.206 16 -1.843 0.2900 ## 1 - 4 -0.44 0.206 16 -2.134 0.1840 ## 2 - 3 -0.56 0.206 16 -2.716 0.0660 ## 2 - 4 -0.62 0.206 16 -3.007 0.0380 ## 3 - 4 -0.06 0.206 16 -0.291 0.9910 ## ## P value adjustment: tukey method for comparing a family of 4 estimates The default adjustment in the pairs function is the Tukey method. Comparing the p-values for each comparison using the Boneferroni and Tukey methods: pairs.b &lt;- pairs(pulp.emm, adjust = &#39;bonferroni&#39;) pairs.t &lt;- pairs(pulp.emm) data.frame(transform(pairs.b)[, 1:5], Bonf.p.value = transform(pairs.b)[, 6], Tukey.p.value = transform(pairs.t)[, 6]) ## contrast estimate SE df t.ratio Bonf.p.value Tukey.p.value ## 1 1 - 2 0.18 0.206 16 0.873 1.0000 0.8185 ## 2 1 - 3 -0.38 0.206 16 -1.843 0.5034 0.2903 ## 3 1 - 4 -0.44 0.206 16 -2.134 0.2918 0.1845 ## 4 2 - 3 -0.56 0.206 16 -2.716 0.0915 0.0658 ## 5 2 - 4 -0.62 0.206 16 -3.007 0.0501 0.0377 ## 6 3 - 4 -0.06 0.206 16 -0.291 1.0000 0.9911 Although the decision on which hypotheses to reject (comparson 2 - 4) is the same here for both methods, the p-values from the Bonferroni method are all larger, reflecting its more conservative nature. 2.8 Exercises For Example 2.1, calculate the mean response for each operator and show that the treatment differences and results from hypothesis tests using the results in Section 2.5 are the same as those found in Section 2.6 using pairwise.t.test, and emmeans. Also check the results in Section 2.7 by (i) adjusting individual p-values (for Bonferroni) and (ii) using the qtukey command.  (Adapted from Wu and Hamada, 2009) The bioactivity of four different drugs \\(A\\), \\(B\\), \\(C\\) and \\(D\\) for treating a particular illness was compared in a study and the following ANOVA table was given for the data: Source Degrees of freedom Sums of squares Mean square Treatment 3 64.42 21.47 Residual 26 62.12 2.39 Total 29 126.54 What considerations should be made when assigning drugs to patients, and why? Use an \\(F\\)-test to test at the 0.01 level the null hypothesis that the four drugs have the same bioactivity. The average response from each treatment is as follows: \\(\\bar{Y}_A=66.10\\) (\\(r_A=7\\) patients), \\(\\bar{Y}_B=65.75\\) (\\(r_B=8\\)), \\(\\bar{Y}_C = 62.63\\) (\\(r_C=9\\)), and \\(\\bar{Y}_D=63.85\\) (\\(r_D=6\\)). Conduct hypothesis tests for all pairwise comparisons using the Bonferroni and Tukey methods for an experiment-wise error rate of 0.05. In fact, \\(A\\) and \\(B\\) are brand-name drugs and \\(C\\) and \\(D\\) are generic drugs. Test the null hypothesis that brand-name and generic drugs have the same bioactivity. The below table gives data from a completely randomised design to compare six different batches of hydrochloric acid on the yield of a dye (naphthalene black 12B). napblack &lt;- data.frame(batch = rep(factor(1:6), rep(5, 6)), repetition = rep(1:5, 6), yield = c(145, 40, 40, 120, 180, 140, 155, 90, 160, 95, 195, 150, 205, 110, 160, 45, 40, 195, 65, 145, 195, 230, 115, 235, 225, 120, 55, 50, 80, 45) ) knitr::kable( tidyr::pivot_wider(napblack, names_from = batch, values_from = yield)[, -1], col.names = paste(&quot;Batch&quot;, 1:6), caption = &quot;Naphthalene black experiment: yields (grams of standard colour) from six different batches of hydrochloric acid.&quot; ) Table 2.3: Naphthalene black experiment: yields (grams of standard colour) from six different batches of hydrochloric acid. Batch 1 Batch 2 Batch 3 Batch 4 Batch 5 Batch 6 145 140 195 45 195 120 40 155 150 40 230 55 40 90 205 195 115 50 120 160 110 65 235 80 180 95 160 145 225 45 Conduct a full analysis of this experiment, including a. exploratory data analysis; a. fitting a linear model, and conducting an F-test to compare to a model that explains variation using the six batches to the null model; a. usual linear model diagnostics b. multiple comparisons of all pairwise differences between treatments. References "],["blocking.html", "Chapter 3 Blocking", " Chapter 3 Blocking "],["factorial.html", "Chapter 4 Factorial experiments", " Chapter 4 Factorial experiments "],["blocking-in-factorial-designs.html", "Chapter 5 Blocking in factorial designs", " Chapter 5 Blocking in factorial designs "],["fractional-factorial-designs.html", "Chapter 6 Fractional factorial designs", " Chapter 6 Fractional factorial designs "],["response-surface-methodology.html", "Chapter 7 Response surface methodology", " Chapter 7 Response surface methodology "],["optimal-design-of-experiments.html", "Chapter 8 Optimal design of experiments", " Chapter 8 Optimal design of experiments "],["references.html", "References", " References "]]
