<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Motivation, introduction and revision | MATH3014-6027 Design (and Analysis) of Experiments</title>
  <meta name="description" content="Lecture notes for the modules MATH3014 and MATH6027 at the University of Southampton" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Motivation, introduction and revision | MATH3014-6027 Design (and Analysis) of Experiments" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecture notes for the modules MATH3014 and MATH6027 at the University of Southampton" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Motivation, introduction and revision | MATH3014-6027 Design (and Analysis) of Experiments" />
  
  <meta name="twitter:description" content="Lecture notes for the modules MATH3014 and MATH6027 at the University of Southampton" />
  

<meta name="author" content="Dave Woods" />


<meta name="date" content="2022-01-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="simple-comparative-experiments.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH3014-6027</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Motivation, introduction and revision</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#motivation"><i class="fa fa-check"></i><b>1.1</b> Motivation</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#aims-of-experimentation-and-some-examples"><i class="fa fa-check"></i><b>1.2</b> Aims of experimentation and some examples</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#some-definitions"><i class="fa fa-check"></i><b>1.3</b> Some definitions</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#principles"><i class="fa fa-check"></i><b>1.4</b> Principles of experimentation</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#replication"><i class="fa fa-check"></i><b>1.4.1</b> Replication</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro.html"><a href="intro.html#randomisation"><i class="fa fa-check"></i><b>1.4.2</b> Randomisation</a></li>
<li class="chapter" data-level="1.4.3" data-path="intro.html"><a href="intro.html#blocking"><i class="fa fa-check"></i><b>1.4.3</b> Stratification (or blocking)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#lin-model-rev"><i class="fa fa-check"></i><b>1.5</b> Revision on the linear model</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#variance-of-a-predictionfitted-value"><i class="fa fa-check"></i><b>1.5.1</b> Variance of a Prediction/Fitted Value</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#analysis-of-variance-and-r2-as-model-comparison"><i class="fa fa-check"></i><b>1.5.2</b> Analysis of Variance and R<span class="math inline">\(^{2}\)</span> as Model Comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simple-comparative-experiments.html"><a href="simple-comparative-experiments.html"><i class="fa fa-check"></i><b>2</b> Simple comparative experiments</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3014-6027 Design (and Analysis) of Experiments</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Motivation, introduction and revision</h1>
<div class="definition">
<p><span id="def:exp" class="definition"><strong>Definition 1.1  </strong></span>An <strong>experiment</strong> is the process through which data are collected to answer a scientific question (physical science, social science, actuarial science <span class="math inline">\(\dots\)</span>) by <strong>deliberately</strong> varying some features of the process under study in order to understand the impact of these changes on measureable responses.</p>
<p>In this course we consider only <em>intervention</em> experiments, in which some aspects of the process are under the experimenters’ control. We do not consider <em>surveys</em> or <em>observational</em> studies.</p>
</div>
<div class="definition">
<p><span id="def:design" class="definition"><strong>Definition 1.2  </strong></span><strong>Design of experiments</strong> is the topic in Statistics concerned with the selection of settings of controllable variables or factors in an experiment and their allocation to experimental units in order to maximise the effectiveness of the experiment at achieving its aim.</p>
</div>
<p>People have been designing experiments for as long as they have been exploring the natural world. Some notable milestones in the history of the design of experiments include:</p>
<ul>
<li>prior to the 20th century:
<ul>
<li><a href="https://en.wikipedia.org/wiki/Baconian_method">Francis Bacon</a> (17th century; pioneer of the experimental methods)</li>
<li><a href="https://en.wikipedia.org/wiki/James_Lind">James Lind</a> (18th century; experiments to eliminate scurvy)</li>
<li><a href="https://en.wikipedia.org/wiki/Charles_Sanders_Peirce#Probability_and_statistics">Charles Peirce</a> (19th century; advocated randomised experiments and randomisation-based inference)</li>
</ul></li>
<li>1920s: agriculture (particularly at the <a href="https://www.rothamsted.ac.uk/history-and-heritage">Rothamsted Agricultural Research Station</a>)</li>
<li>1940s: clinical trials (<a href="https://en.wikipedia.org/wiki/Austin_Bradford_Hill">Austin Bradford-Hill</a>)</li>
<li>1950s: (manufacturing) industry (<a href="https://en.wikipedia.org/wiki/W._Edwards_Deming">W. Edwards Deming</a>; <a href="https://en.wikipedia.org/wiki/Genichi_Taguchi">Genichi Taguchi</a>)</li>
<li>1960s: psychology and economics (<a href="https://en.wikipedia.org/wiki/Vernon_L._Smith">Vernon Smith</a>)</li>
<li>1980s: in-silico (<a href="https://en.wikipedia.org/wiki/Computer_experiment">computer experiments</a>)</li>
<li>2000s: online (<a href="https://en.wikipedia.org/wiki/A/B_testing">A/B testing</a>)</li>
</ul>
<p>See <span class="citation"><a href="#ref-LB2020" role="doc-biblioref">Luca and Bazerman</a> (<a href="#ref-LB2020" role="doc-biblioref">2020</a>)</span> for further history, annecdotes and examples, especially from psychology and technology.</p>
<p>Figure <a href="intro.html#fig:broadbalk">1.1</a> shows the <a href="http://www.era.rothamsted.ac.uk/Broadbalk">Broadbalk</a> agricultural field experiment at Rothamsted, one of the longest continuous running experiments in the world, which is testing the impact of different manures and fertilizers on the growth of winter wheat.</p>
<div class="figure" style="text-align: center"><span id="fig:broadbalk"></span>
<img src="figures/broadbalk.JPG" alt="The Broadbalk experiment, Rothamsted (photograph taken 2016)" width="75%" />
<p class="caption">
Figure 1.1: The Broadbalk experiment, Rothamsted (photograph taken 2016)
</p>
</div>
<!--
Before we can design an experiment, we need to know:

- what is being measured;
- what features (variables or factors) can be varied and controlled, and what values can they be set to;
- what is the aim of the experiment.
-->
<div id="motivation" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Motivation</h2>
<div class="example">
<p><span id="exm:motivation" class="example"><strong>Example 1.1  </strong></span>Consider an experiment to compare two treatments (e.g. drugs, diets, fertilisers, <span class="math inline">\(\dots\)</span>). We have <span class="math inline">\(N\)</span> subjects (people, mice, plots of land, <span class="math inline">\(\dots\)</span>), each of which can be assigned one of the two treatments. A response (protein measurement, weight, yield, <span class="math inline">\(\dots\)</span>) is then measured.</p>
</div>
<p><strong>Question:</strong> How many subjects should be assigned to each treatment to gain the most precise<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> inference about the difference in response from the two treatments?</p>
<p>Consider a linear statistical model<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> for the response (see MATH2010):</p>
<p><span class="math display" id="eq:slr">\[\begin{equation}
Y_j=\beta_{0}+\beta_{1}x_j+\varepsilon_j\,,\qquad j=1, \ldots, n\,,
\tag{1.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_j\sim N(0,\sigma^{2})\)</span> are independent and identically distributed errors and <span class="math inline">\(\beta_{0}, \beta_{1}\)</span> are unknown constants (parameters).</p>
<p>Let
<span class="math display">\[\begin{equation}
x_{j}=\left\{\begin{array}{ll}
-1&amp;\textrm{if treatment 1 is applied to the $j$th subject}\\
+1&amp;\textrm{if treatment 2 is applied to the $j$th subject}\nonumber ,
\end{array}
\right.
\end{equation}\]</span>
for <span class="math inline">\(j=1,\dots,n\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>The difference in expected response from treatments 1 and 2 is</p>
<p><span class="math display" id="eq:ex-ex-response">\[\begin{equation}
\begin{split}
\textrm{E}[Y_j\, |\, x_j = +1] - \textrm{E}[Y_j\, |\, x_j = -1] &amp; = \beta_{0}+\beta_{1}-\beta_{0}+\beta_{1} \\
&amp; = 2\beta_{1}\,.
\end{split}
\tag{1.2}
\end{equation}\]</span></p>
<p>Therefore, we require the the most precise estimator of <span class="math inline">\(\beta_{1}\)</span> possible. That is, we wish to make the variance of our estimator of <span class="math inline">\(\beta_1\)</span> as small as possible.</p>
<p>Parameters <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> can be estimated using least squares (see MATH2010). For <span class="math inline">\(Y_1,\dots,Y_n\)</span>, we can write the model down in matrix form:</p>
<p><span class="math display">\[\begin{equation*}
\left[ \begin{array}{c}
Y_1\\
\vdots\\
Y_n\end{array}\right]
=\left[ \begin{array}{cc}
1&amp;x_{1}\\
\vdots&amp;\vdots\\
1&amp;x_{n}\end{array}\right]
\left[ \begin{array}{c}
\beta_{0}\\
\beta_{1}\end{array}\right]
+\left[ \begin{array}{c}
\varepsilon_{1}\\
\vdots\\
\varepsilon_{n}\end{array}\right]\,.
\end{equation*}\]</span></p>
<p>Or, by defining some notation:</p>
<p><span class="math display" id="eq:matrix-model">\[\begin{equation}
\boldsymbol{Y}=X\boldsymbol{\beta}+\boldsymbol{\varepsilon}\,
\tag{1.3}
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\boldsymbol{Y}\)</span> - <span class="math inline">\(n\times 1\)</span> vector of responses;</li>
<li><span class="math inline">\(X\)</span> - <span class="math inline">\(n\times p\)</span> model matrix;</li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> - <span class="math inline">\(p\times 1\)</span> vector of parameters;</li>
<li><span class="math inline">\(\boldsymbol{\varepsilon}\)</span> - <span class="math inline">\(n\times 1\)</span> vector of errors.</li>
</ul>
<p>The <strong>least squares estimators</strong>, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, are chosen such that the quadratic form</p>
<p><span class="math display">\[\begin{equation*}
(\boldsymbol{Y}-X\boldsymbol{\beta})^{\textrm{T}}(\boldsymbol{Y}-X\boldsymbol{\beta})
\end{equation*}\]</span></p>
<p>is minimised (recall that <span class="math inline">\(\textrm{E}(\textbf{Y})=X\boldsymbol{\beta}\)</span>). Therefore</p>
<p><span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}} = \textrm{argmin}_{\boldsymbol{\beta}}(\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y}+\boldsymbol{\beta}^{\textrm{T}}X^{\textrm{T}}X\boldsymbol{\beta}
-2\boldsymbol{\beta}^{\textrm{T}}X^{\textrm{T}}\boldsymbol{Y})\,.
\end{equation*}\]</span></p>
<p>If we differentiate with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>,</p>
<p><span class="math display">\[\begin{equation*}
\frac{\partial}{\partial\boldsymbol{\beta}}=2X^{\textrm{T}}X\boldsymbol{\beta}-2X^{\textrm{T}}\boldsymbol{Y}\,,\nonumber
\end{equation*}\]</span></p>
<p>and equate to 0, we get the estimators</p>
<p><span class="math display" id="eq:lsestimators">\[\begin{equation}
\hat{\boldsymbol{\beta}}=(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y}\,.
\tag{1.4}
\end{equation}\]</span></p>
<p>These are the least squares estimators.</p>
<p>For Example <a href="intro.html#exm:motivation">1.1</a>,</p>
<p><span class="math display">\[
X=\left[\begin{array}{cc}
1&amp;x_{1}\\
\vdots&amp;\vdots\\
1&amp;x_{n}\end{array}\right]\,,
\qquad
X^{\textrm{T}}X=\left[\begin{array}{cc}
n&amp;\sum x_j\\
\sum x_j&amp;\sum x_j^{2}\end{array}\right]\,,
\]</span></p>
<p><span class="math display">\[
(X^{\textrm{T}}X)^{-1}=\frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}\left[\begin{array}{cc}
\sum x_j^{2}&amp;-\sum x_j\\
-\sum x_j&amp;n\end{array}\right]\,,
\qquad
X^{\textrm{T}}\boldsymbol{Y}=\left[\begin{array}{c}
\sum Y_j\\
\sum x_jY_j\end{array}\right]\,.
\]</span>
Then,
<span class="math display">\[\begin{align}
\hat{\boldsymbol{\beta}}=\left[\begin{array}{c}
\hat{\beta}_{0}\\
\hat{\beta}_{1}\end{array}\right]
&amp; =\frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}
\left[\begin{array}{cc}
\sum x_j^{2}&amp;-\sum x_j\\
-\sum x_j&amp;n\end{array}\right]
\left[\begin{array}{c}
\sum Y_j\\
\sum x_jY_j\end{array}\right]\nonumber \\
&amp;= \frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}\left[\begin{array}{c}
\sum Y_j\sum x_j^{2}-\sum x_j\sum x_jY_j\\
n\sum x_jY_j-\sum x_j\sum Y_j\end{array}\right]\,.
\end{align}\]</span></p>
<p>We don’t usually work through the algebra in such detail; the matrix form is often sufficient for theoretical and numerical calculations and software, e.g. <code>R</code>, can be used.</p>
<p>The precision of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is measured via the variance-covariance matrix, given by
<span class="math display">\[\begin{align}
\textrm{Var}(\hat{\boldsymbol{\beta}}) &amp; = \textrm{Var}\{(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y}\}\\
&amp; =(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\textrm{Var}(\boldsymbol{Y})X(X^{\textrm{T}}X)^{-1}\\
&amp; = (X^{\textrm{T}}X)^{-1}\sigma^{2}\,,
\end{align}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{Y}\sim N(X\boldsymbol{\beta},I_n\sigma^{2})\)</span>, where <span class="math inline">\(I_n\)</span> is an <span class="math inline">\(n\times n\)</span> identity matrix.</p>
<p>Hence, in our example,
<span class="math display">\[\begin{align*}
\textrm{Var}(\hat{\boldsymbol{\beta}}) &amp; = \frac{1}{n\sum x_j^{2}-(\sum x_j)^{2}}\left[\begin{array}{cc}
\sum x_j^{2}&amp;-\sum x_j\\
-\sum x_j&amp;n\end{array}\right]\sigma^{2}\\
&amp; = \left[\begin{array}{cc}
\textrm{Var}(\hat\beta_{0})&amp;\textrm{Cov}(\hat\beta_{0},\hat\beta_{1})\\
\textrm{Cov}(\hat\beta_{0},\hat\beta_{1})&amp;\textrm{Var}(\hat\beta_{1})\end{array}\right]\,.
\end{align*}\]</span></p>
<p>For estimating the difference between treatments, we are interested in</p>
<p><span class="math display">\[\begin{align*}
\textrm{Var}(\hat{\beta}_{1})&amp; = \frac{n}{n\sum x_j^{2}-(\sum x_j)^{2}}\sigma^{2}\\
 &amp; = \frac{n}{n^2 - (\sum x_j)^2}\sigma^{2}\,,
\end{align*}\]</span>
as <span class="math inline">\(x_j=\pm 1\)</span>, therefore <span class="math inline">\(x_j^2=1\)</span> for all <span class="math inline">\(j=1,\ldots,n\)</span>, and hence <span class="math inline">\(\sum x_j^2=n\)</span>.</p>
<p>To achieve the most precise estimator, we need to minimise <span class="math inline">\(\textrm{Var}(\hat{\beta}_{1})\)</span> or equivalently minimise <span class="math inline">\(|\sum x_j|\)</span>. This goal can achieve this through the choice of <span class="math inline">\(x_{1},\dots,x_{N}\)</span>:</p>
<ul>
<li>as each <span class="math inline">\(x_j\)</span> can only take one of two values, -1 or +1, this is equivalent to choosing the numbers of subjects assigned to treatment 1 and treatment 2;</li>
<li>call these <span class="math inline">\(n_{1}\)</span> and <span class="math inline">\(n_{2}\)</span> respectively, with <span class="math inline">\(n_{1}+n_{2}=N\)</span></li>
</ul>
<p>It is obvious that <span class="math inline">\(\sum x_j = 0\)</span> if and only if <span class="math inline">\(n_1=n_2\)</span>. Therefore, assuming <span class="math inline">\(N\)</span> is even, the <strong>optimal design</strong> has</p>
<ul>
<li><span class="math inline">\(n_{1}=\frac{n}{2}\)</span> subjects assigned to treatment 1 and</li>
<li><span class="math inline">\(n_{2}=\frac{n}{2}\)</span> subjects assigned to treatment 2.</li>
</ul>
<p>For <span class="math inline">\(N\)</span> odd, we choose <span class="math inline">\(n_{1}=\frac{n+1}{2}\)</span>, <span class="math inline">\(n_{2}=\frac{n-1}{2}\)</span>, or vice versa.</p>
<div class="definition">
<p><span id="def:simple-efficiency" class="definition"><strong>Definition 1.3  </strong></span>We can assess different designs using their <strong>efficiency</strong>:
<span class="math display" id="eq:simple-efficiency">\[\begin{equation}
\textrm{Eff}=\frac{\textrm{Var}(\hat{\beta}_{1}\, |\, d^{*})}{\textrm{Var}(\hat{\beta}_{1}\, |\, d_{1})}
\tag{1.5}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(d_{1}\)</span> is a design we want to assess and <span class="math inline">\(d^{*}\)</span> is the optimal design with smallest variance. Note that <span class="math inline">\(0\leq\textrm{Eff}\leq 1\)</span>.</p>
</div>
<p>In Figure <a href="intro.html#fig:simple-efficiency">1.2</a> below, we plot this efficiency for Example <a href="intro.html#exm:motivation">1.1</a>, using different choices of <span class="math inline">\(n_1\)</span>. The total number of runs is fixed at <span class="math inline">\(n = 100\)</span>, and the function <code>eff</code> calculates the efficiency from Definition <a href="intro.html#def:simple-efficiency">1.3</a> for a design with <span class="math inline">\(n_1\)</span> subjects assigned to treatment 1. Clearly, efficiency of 1 is achieved when <span class="math inline">\(n_1 = n_2\)</span> (equal allocation of treatments 1 and 2). If <span class="math inline">\(n_1=0\)</span> or <span class="math inline">\(n_1 = 1\)</span>, the efficiency is zero; we cannot estimate the difference between two treatments if we only allocate subjects to one of them.</p>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro.html#cb1-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> </span>
<span id="cb1-2"><a href="intro.html#cb1-2" aria-hidden="true" tabindex="-1"></a>eff <span class="ot">&lt;-</span> <span class="cf">function</span>(n1) <span class="dv">1</span> <span class="sc">-</span> ((<span class="dv">2</span> <span class="sc">*</span> n1 <span class="sc">-</span> n) <span class="sc">/</span> n)<span class="sc">^</span><span class="dv">2</span> </span>
<span id="cb1-3"><a href="intro.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(eff, <span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> n, <span class="at">ylab =</span> <span class="st">&quot;Eff&quot;</span>, <span class="at">xlab =</span> <span class="fu">expression</span>(n[<span class="dv">1</span>]))</span></code></pre></div>
<div class="figure"><span id="fig:simple-efficiency"></span>
<img src="bookdown_math3014-6027_files/figure-html/simple-efficiency-1.png" alt="Efficiencies for designs for Example 1.1 with different numbers, \(n_1\), of subjects assigned to treatment 1 when the total number of subjects is \(n=100\)." width="672" />
<p class="caption">
Figure 1.2: Efficiencies for designs for Example <a href="intro.html#exm:motivation">1.1</a> with different numbers, <span class="math inline">\(n_1\)</span>, of subjects assigned to treatment 1 when the total number of subjects is <span class="math inline">\(n=100\)</span>.
</p>
</div>
</div>
<div id="aims-of-experimentation-and-some-examples" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Aims of experimentation and some examples</h2>
<p>Some reasons experiments are performed:</p>
<ol style="list-style-type: decimal">
<li>Treatment comparison (Chapters 2 and 3)</li>
</ol>
<ul>
<li>compare several treatments (and choose the best)</li>
<li>e.g. clinical trial, agricultural field trial</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Factor screening (Chapters 4, 5 and 6)</li>
</ol>
<ul>
<li>many complex systems may involve a large number of (discrete) factors (controllable features)</li>
<li>which of these factors have a substantive impact?</li>
<li>(relatively) small experiments</li>
<li>e.g. industrial experiments on manufacturing processes</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Response surface exploration (Chapter 7)</li>
</ol>
<ul>
<li>detailed description of relationship between important (continuous) variables and response</li>
<li>typically second order polynomial regression models</li>
<li>larger experiments, often built up sequentially</li>
<li>e.g. alcohol yields in a pharmaceutical experiments</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Optimisation (Chapter 7)</li>
</ol>
<ul>
<li>finding settings of variables that lead to maximum or minimum response</li>
<li>typically use response surface methods and sequential ``hill climbing’’ strategy</li>
</ul>
</div>
<div id="some-definitions" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Some definitions</h2>
<div class="definition">
<p><span id="def:response" class="definition"><strong>Definition 1.4  </strong></span>The <strong>response</strong> <span class="math inline">\(Y\)</span> is the outcome measured in an experiment; e.g. yield from a chemical process. The response from the <span class="math inline">\(n\)</span> observations are denoted <span class="math inline">\(Y_{1},\dots,Y_{n}\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:factor-variable" class="definition"><strong>Definition 1.5  </strong></span><strong>Factors</strong> (discrete) or <strong>variables</strong> (continuous) are features which can be set or controlled in an experiment; <span class="math inline">\(m\)</span> denotes the number of factors or variables under investigation. For discrete factors, we call the possible settings of the factor its <strong>levels</strong>. We denote by <span class="math inline">\(x_{ij}\)</span> the value taken by factor or variable <span class="math inline">\(i\)</span> in the <span class="math inline">\(j\)</span>th run of the experiment (<span class="math inline">\(i = 1, \ldots, m\)</span>; <span class="math inline">\(j = 1, \ldots, n\)</span>).</p>
</div>
<!--
::: {.definition #designpoint}
A **design point** is an $m$-vector $\boldsymbol{x}_{j} = (x_{j1, \ldots, x_{jm}})^\top$ giving the values taken by the $m$ factors or variables in $j$th run ($j=1,\dots,N$). 
:::
-->
<div class="definition">
<p><span id="def:treatment" class="definition"><strong>Definition 1.6  </strong></span>The <strong>treatments</strong> or <strong>support points</strong> are the <em>distinct</em> combinations of factor or variable values in the experiment.</p>
</div>
<div class="definition">
<p><span id="def:unit" class="definition"><strong>Definition 1.7  </strong></span>An experimental <strong>unit</strong> is the basic element (material, animal, person, time unit, ) to which a treatment can be applied to produce a response.</p>
</div>
<p>In Example <a href="intro.html#exm:motivation">1.1</a> (comparing two treatments):</p>
<ul>
<li>Response <span class="math inline">\(Y\)</span>: Measured outcome, e.g. protein level or pain score in clinical trial, yield in an agricultural field trial.</li>
<li>Factor <span class="math inline">\(x\)</span>: ``treatment’’ applied</li>
<li>Levels
<span class="math display">\[
\begin{array}{ll}
\textrm{treatment 1}&amp;x =-1\\
\textrm{treatment 2}&amp;x =+1
\end{array}
\]</span></li>
<li>Design point: factor level applied to <span class="math inline">\(j\)</span>th subject; <span class="math inline">\(x_{j}=\pm 1\)</span></li>
<li>Treatment or support point: Two treatments or support points</li>
<li>Experimental unit: Subject (person, animal, plot of land, ).</li>
</ul>
</div>
<div id="principles" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Principles of experimentation</h2>
<p>Three fundamental principles that need to be considered when designing an experiment are:</p>
<ul>
<li>replication</li>
<li>randomisation</li>
<li>stratification (blocking)</li>
</ul>
<div id="replication" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Replication</h3>
<p>Each treatment is applied to a number of experimental units, with the <span class="math inline">\(j\)</span>th treatment replicated <span class="math inline">\(r_{j}\)</span> times. This enables the estimation of the variances of treatment effect estimators; increasing the number of replications, or replicates, decreases the variance of estimators of treatment effects.
(Note: proper replication involves independent application of the treatment to different experimental units, not just taking several measurements from the same unit).</p>
</div>
<div id="randomisation" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Randomisation</h3>
<p>Randomisation should be applied to the allocation of treatments to units. Randomisation protects against <strong>bias</strong>; the effect of
variables that are unknown and potentially uncontrolled or
subjectivity in applying treatments. It also provides a formal basis
for inference and statistical testing.</p>
<p>For example, in a clinical trial to compare a new drug and a control random allocation protects against</p>
<ul>
<li>“unmeasured and uncontrollable” features (e.g. age, sex, health)</li>
<li>bias resulting from the clinician giving new drug to patients who are sicker.</li>
</ul>
<p>Clinical trials are usually also <em>double-blinded</em>, i.e. neither the healthcare professional nor the patient knows which treatment the patient is receiving.</p>
</div>
<div id="blocking" class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Stratification (or blocking)</h3>
<p>We would like to use a wide variety of experimental units (e.g. people or plots of land) to ensure <strong>coverage</strong> of our results, i.e. validity of our conclusions across the population of interest. However, if the sample of units from the population is too heterogenous, then this will induce too much random variability, i.e. increase <span class="math inline">\(\sigma^{2}\)</span> in <span class="math inline">\(\varepsilon_{j}\sim N(0,\sigma^{2})\)</span>, and hence increase the variance of our parameter estimators.</p>
<p>We can reduce this extraneous variation by splitting our units into homogenous sets, or <strong>blocks</strong>, and including a blocking term in the model. The simplest blocked experiment is a <strong>randomised complete block design</strong>, where each block contains enough units for all treatments to be applied. Comparisons can then be made <em>within</em> each block.</p>
<p>Basic principle: block what you can, randomise what you cannot.</p>
<p>Later we will look at blocking in more detail, and the principle of <strong>incomplete blocks</strong>.</p>
</div>
</div>
<div id="lin-model-rev" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Revision on the linear model</h2>
<p>Recall: <span class="math inline">\(\boldsymbol{Y}=X\boldsymbol{\beta}+\boldsymbol{\varepsilon}\)</span>, with <span class="math inline">\(\boldsymbol{\varepsilon}\sim N(\boldsymbol{0},I_n\sigma^{2})\)</span>. Let the <span class="math inline">\(j\)</span>th row of <span class="math inline">\(X\)</span> be denoted <span class="math inline">\(\boldsymbol{x}^\textrm{T}_j\)</span>, which holds the values of the predictors, or explanatory variables, for the <span class="math inline">\(j\)</span>th observation. Then</p>
<p><span class="math display">\[\begin{equation*}
Y_j=\boldsymbol{x}_j^{\textrm{T}}\boldsymbol{\beta}+\varepsilon_j\,,\quad j=1,\ldots,n\,.
\end{equation*}\]</span></p>
<p>For example, quite commonly, for continuous variables</p>
<p><span class="math display">\[
\boldsymbol{x}_j=(1,x_{1j},x_{2j},\dots,x_{mj})^{\textrm{T}}\,,
\]</span></p>
<p>and so
<span class="math display">\[
\boldsymbol{x}_j^{\textrm{T}}\boldsymbol{\beta}=\beta_{0}+\beta_{1}x_{1j}+\dots+\beta_{m}x_{mj}\,.
\]</span></p>
<p>The laest squares estimators are given by</p>
<p><span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}=(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y}\,,\nonumber
\end{equation}\]</span></p>
<p>with</p>
<p><span class="math display">\[\begin{equation}
\textrm{Var}(\hat{\boldsymbol{\beta}})=(X^{\textrm{T}}X)^{-1}\sigma^{2}\,.\nonumber
\end{equation}\]</span></p>
<div id="variance-of-a-predictionfitted-value" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Variance of a Prediction/Fitted Value</h3>
<p>A prediction of the mean response at point <span class="math inline">\(\boldsymbol{x}_0\)</span> (which may or may not be in the design) is</p>
<p><span class="math display">\[
\hat{Y}_0 = \boldsymbol{x}_0^{\textrm{T}}\hat{\boldsymbol{\beta}}\,,
\]</span></p>
<p>with</p>
<p><span class="math display">\[\begin{align*}
\textrm{Var}(\hat{Y}_0) &amp; = \textrm{Var}\left(\boldsymbol{x}_0^{\textrm{T}}\hat{\boldsymbol{\beta}}\right) \\
&amp; = \boldsymbol{x}_0^{\textrm{T}}\textrm{Var}(\hat{\boldsymbol{\beta}})\boldsymbol{x}_0 \\
&amp; = \boldsymbol{x}_0^{\textrm{T}}(X^{\textrm{T}}X)^{-1}\boldsymbol{x}_0\sigma^{2}\,.
\end{align*}\]</span></p>
<p>For a linear model, this variance depends only on the assumed regression model and the design (through <span class="math inline">\(X\)</span>), the point at which prediction is to be made (<span class="math inline">\(\boldsymbol{x}_0\)</span>) and the value of <span class="math inline">\(\sigma^2\)</span>; it does not depend on data <span class="math inline">\(\boldsymbol{Y}\)</span> or parameters <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>Similarly, we can find the variance-covariance matrix of the fitted values:
<span class="math display">\[
\textrm{Var}(\hat{Y})=\textrm{Var}(X\hat{\boldsymbol{\beta}})=X(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\sigma^{2}\,.
\]</span></p>
</div>
<div id="analysis-of-variance-and-r2-as-model-comparison" class="section level3" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> Analysis of Variance and R<span class="math inline">\(^{2}\)</span> as Model Comparison</h3>
<p>To assess the goodness-of-fit of a model, we can use the residual sum of squares</p>
<p><span class="math display">\[\begin{align*}
\textrm{RSS} &amp; = (\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})^{\textrm{T}} (\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})\\
&amp; = \sum^{n}_{j=1}\left\{Y_{j}-\boldsymbol{x}_{j}^{\textrm{T}}\hat{\boldsymbol{\beta}}\right\}^{2}\\
&amp; = \sum^{n}_{j=1}r_{j}^{2}\,,
\end{align*}\]</span></p>
<p>where</p>
<p><span class="math display">\[
r_{j}=Y_{j}-\boldsymbol{x}_{j}^{\textrm{T}}\hat{\boldsymbol{\beta}}\,.
\]</span></p>
<p>Often, a comparison is made to the null model</p>
<p><span class="math display">\[
Y_{j}=\beta_{0}+\varepsilon_{j}\,,
\]</span></p>
<p>i.e. <span class="math inline">\(Y_{i}\sim N(\beta_{0},\sigma^{2})\)</span>. The residual sum of squares for the null model is given by</p>
<p><span class="math display">\[
\textrm{RSS}(\textrm{null}) = \boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y} - m\bar{Y}^{2}\,,
\]</span>
as</p>
<p><span class="math display">\[
\hat{\beta}_{0} = \bar{Y} = \frac{1}{n}\sum_{j=1}^n Y_{j}\,.
\]</span></p>
<p>How do we compare these models?</p>
<ol style="list-style-type: decimal">
<li>Ratio of residual sum of squares:
<span class="math display">\[\begin{align*}
R^{2} &amp; = 1 - \frac{\textrm{RSS}}{\textrm{RSS}(\textrm{null})} \\
&amp; = 1 - \frac{(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})}{\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y}-n\bar{Y}^{2}}\,.
\end{align*}\]</span></li>
</ol>
<p>The quantity <span class="math inline">\(0\leq R^{2}\leq 1\)</span> is sometimes called the <strong>coefficient of multiple determination</strong>:</p>
<ul>
<li>high <span class="math inline">\(R^{2}\)</span> implies that the model describes much of the variation in the data;</li>
<li><strong>but</strong> note that <span class="math inline">\(R^{2}\)</span> will always increase as <span class="math inline">\(p\)</span> (the number of explanatory variables) increases, with <span class="math inline">\(R^{2}=1\)</span> when <span class="math inline">\(p=n\)</span>;</li>
<li>some software packages will report the adjusted <span class="math inline">\(R^{2}\)</span>.</li>
</ul>
<p><span class="math display">\[\begin{align*}
R^{2}_{a} &amp; = 1-\frac{\textrm{RSS}/(n-p)}{\textrm{RSS}(\textrm{null})/(n-1)}\\
&amp; = 1 - \frac{(\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})^{\textrm{T}} (\boldsymbol{Y} - X\hat{\boldsymbol{\beta}})/(n-p)}{(\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y} - n\bar{Y}^{2})/(n-1)};
\end{align*}\]</span></p>
<ul>
<li><span class="math inline">\(R_a^2\)</span> does not necessarily increase with <span class="math inline">\(p\)</span> (as we divide by degrees of freedom to adjust for complexity of the model).</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Analysis of variance (ANOVA): An ANOVA table is compact way of presenting the results of (sequential) comparisons of nested models. You should be familiar with an ANOVA table of the following form.</li>
</ol>
<table>
<caption><span id="tab:anova">Table 1.1: </span> A standard ANOVA table.</caption>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="left">Degress of Freedom</th>
<th align="left">(Sequential) Sum of Squares</th>
<th align="left">Mean Square</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left"><span class="math inline">\(p-1\)</span></td>
<td align="left">By subtraction; see <a href="intro.html#eq:SSS">(1.6)</a></td>
<td align="left">Reg SS/<span class="math inline">\((p-1)\)</span></td>
</tr>
<tr class="even">
<td align="left">Residual</td>
<td align="left"><span class="math inline">\(N-p\)</span></td>
<td align="left"><span class="math inline">\((\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})\)</span><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></td>
<td align="left">RSS/<span class="math inline">\((N-p)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(N-1\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y}-N\bar{Y}^{2}\)</span><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>In row 1 of Table <a href="intro.html#tab:anova">1.1</a> above,
<span class="math display" id="eq:SSS">\[\begin{align}
\textrm{Regression SS = Total SS $-$ RSS} &amp; = \boldsymbol{Y}^{\textrm{T}}\boldsymbol{Y} - n\bar{Y}^{2} - (\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})\\
&amp; = -n\bar{Y}^{2}-\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}+2\hat{\boldsymbol{\beta}}^{\textrm{T}}X^{\textrm{T}}\boldsymbol{Y} \\
&amp; = \hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}-n\bar{Y}^{2}\,,
\tag{1.6}
\end{align}\]</span></p>
<p>with the last line following from
<span class="math display">\[\begin{align*}
\hat{\boldsymbol{\beta}}^{\textrm{T}}X^{\textrm{T}}\boldsymbol{Y} &amp; =
\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)(X^{\textrm{T}}X)^{-1}X^{\textrm{T}}\boldsymbol{Y} \\
&amp; = \hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}
\end{align*}\]</span></p>
<p>This idea can be generalised to the comparison of a <em>sequence</em> of nested models - see Problem Sheet 1.</p>
<p>Hypothesis testing is performed using the mean square:</p>
<p><span class="math display">\[\begin{equation}
\frac{\textrm{Regression SS}}{p-1}=\frac{\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}}-n\bar{Y}^{2}}{p-1}\,.\nonumber
\end{equation}\]</span></p>
<p>Under <span class="math inline">\(\textrm{H}_{0}: \beta_{1}=\dots=\beta_{p-1}=0\)</span></p>
<p><span class="math display">\[\begin{align*}
\frac{\textrm{Regression SS}/(p-1)}{\textrm{RSS}/(N-p)} &amp; = \frac{(\hat{\boldsymbol{\beta}}^{\textrm{T}}(X^{\textrm{T}}X)\hat{\boldsymbol{\beta}} - n\bar{Y}^{2})/(p-1)}{(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})/(n-p)}\nonumber\\
&amp; \sim F_{p-1,n-p}\,,
\end{align*}\]</span></p>
<p>an <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(p-1\)</span> and <span class="math inline">\(n-p\)</span> degrees of freedom; defined via the ratio of two independent <span class="math inline">\(\chi^{2}\)</span> distributions.</p>
<p>Also,</p>
<p><span class="math display">\[\begin{equation*}
\frac{\textrm{RSS}}{n-p}=\frac{(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})^{\textrm{T}}(\boldsymbol{Y}-X\hat{\boldsymbol{\beta}})}{n-p}=\hat{\sigma}^{2}
\end{equation*}\]</span></p>
<p>is an unbiased estimator for <span class="math inline">\(\sigma^{2}\)</span>, and</p>
<p><span class="math display">\[\begin{equation*}
\frac{(n-p)}{\sigma^{2}}\hat{\sigma}^{2}\sim\chi^{2}_{n-p}\,.
\end{equation*}\]</span></p>
<p>This is a Chi-squared distribution with <span class="math inline">\(N-p\)</span> degrees of freedom (see MATH2010 notes).</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-LB2020" class="csl-entry">
Luca, M., and Bazerman, M. H. (2020), <em>The power of experiments: Decision making in a data-driven world</em>, Cambridge, MA.: MIT Press.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Smallest variance.<a href="intro.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>In this course, we will almost always start with a statistical model which we wish to use to answer our scientific question.<a href="intro.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>We will discuss the choice of <em>coding</em> -1, +1 later.<a href="intro.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Check the <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix Cookbook</a> for matrix calculus, amongst much else.<a href="intro.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Residual sum of squares for the full regression model.<a href="intro.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>Residual sum of squares for the null model.<a href="intro.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple-comparative-experiments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown_math3014-6027.pdf", "bookdown_math3014-6027.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
